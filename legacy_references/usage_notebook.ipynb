{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.50.3)\n",
      "Requirement already satisfied: torch in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.1.4)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (4.13.0)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: unsloth in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2025.3.19)\n",
      "Requirement already satisfied: accelerate in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: bitsandbytes in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.45.4)\n",
      "Requirement already satisfied: unsloth_zoo>=2025.3.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth) (2025.3.17)\n",
      "Requirement already satisfied: torch>=2.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth) (2.6.0)\n",
      "Requirement already satisfied: xformers>=0.0.27.post2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth) (0.0.29.post3)\n",
      "Requirement already satisfied: triton>=3.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth) (3.2.0)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth) (24.2)\n",
      "Requirement already satisfied: tyro in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth) (0.9.18)\n",
      "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth) (4.50.3)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth) (3.5.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth) (4.67.1)\n",
      "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth) (7.0.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth) (0.45.1)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth) (1.26.4)\n",
      "Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth) (0.15.2)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth) (0.15.1)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth) (3.20.3)\n",
      "Requirement already satisfied: huggingface_hub in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth) (0.30.1)\n",
      "Requirement already satisfied: hf_transfer in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth) (0.1.9)\n",
      "Requirement already satisfied: diffusers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth) (0.32.2)\n",
      "Requirement already satisfied: torchvision in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth) (0.21.0)\n",
      "Requirement already satisfied: pyyaml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth) (0.3.8)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth) (3.11.14)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub->unsloth) (4.13.0)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.1)\n",
      "Requirement already satisfied: rich in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (13.9.4)\n",
      "Requirement already satisfied: cut_cross_entropy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth_zoo>=2025.3.17->unsloth) (25.1.1)\n",
      "Requirement already satisfied: pillow in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth_zoo>=2025.3.17->unsloth) (11.1.0)\n",
      "Requirement already satisfied: importlib-metadata in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from diffusers->unsloth) (8.6.1)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tyro->unsloth) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tyro->unsloth) (1.7.1)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tyro->unsloth) (4.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.19.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\n",
      "Requirement already satisfied: flash-attn in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.7.4.post1)\n",
      "Requirement already satisfied: torch in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from flash-attn) (2.6.0)\n",
      "Requirement already satisfied: einops in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from flash-attn) (0.8.1)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->flash-attn) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->flash-attn) (4.13.0)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->flash-attn) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->flash-attn) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->flash-attn) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->flash-attn) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->flash-attn) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->flash-attn) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->flash-attn) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->flash-attn) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->flash-attn) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->flash-attn) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->flash-attn) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->flash-attn) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->flash-attn) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->flash-attn) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->flash-attn) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->flash-attn) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->flash-attn) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->flash-attn) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (3.0.2)\n",
      "Requirement already satisfied: datasets in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.5.0)\n",
      "Requirement already satisfied: pyyaml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (6.0.2)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.30.1)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install core dependencies\n",
    "!pip install transformers torch pandas\n",
    "\n",
    "# For faster inference (important)\n",
    "!pip install unsloth accelerate bitsandbytes\n",
    "\n",
    "# Flash Attention (highly recommended for speed)\n",
    "!pip install flash-attn --no-build-isolation\n",
    "\n",
    "# For dataset handling and YAML parsing\n",
    "!pip install datasets pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented out anything related to flash-att and HF_TOKEN if you have some trouble with them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_from_disk\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import Wandb for experiment tracking\n",
    "import wandb\n",
    "\n",
    "# Import Unsloth\n",
    "import unsloth\n",
    "# Import HuggingFace libraries\n",
    "\n",
    "# Try to import HF token from environment\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)\n",
    "\n",
    "# Disable HuggingFace tokenizers parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of PromptCreator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptCreator:\n",
    "    \"\"\"\n",
    "    Creates and formats prompts for multiple choice questions\n",
    "    Supports different prompt styles for training and inference\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt types\n",
    "    BASIC = \"basic\"  # Simple answer-only format\n",
    "    YAML_REASONING = \"yaml\"  # YAML formatted reasoning\n",
    "    TEACHER_REASONED = \"teacher\"  # Same YAML format as YAML_REASONING but using teacher completions for training\n",
    "\n",
    "    def __init__(self, prompt_type=BASIC):\n",
    "        \"\"\"\n",
    "        Initialize prompt creator with the specified type\n",
    "\n",
    "        Args:\n",
    "            prompt_type: Type of prompts to generate - \"basic\", \"yaml\", or \"teacher\"\n",
    "                         Note: \"teacher\" uses same prompt format as \"yaml\" but with teacher completions\n",
    "        \"\"\"\n",
    "        # For prompt formatting, teacher_reasoned is equivalent to yaml_reasoning\n",
    "        # The difference only matters during training when using teacher completions\n",
    "        if prompt_type == self.TEACHER_REASONED:\n",
    "            prompt_type = self.YAML_REASONING\n",
    "\n",
    "        self.prompt_type = prompt_type\n",
    "        # Store the original prompt type to track if we're using teacher mode\n",
    "        self.original_type = prompt_type\n",
    "\n",
    "    def format_choices(self, choices):\n",
    "        \"\"\"Format choices as a lettered list\"\"\"\n",
    "        return \"\\n\".join(\n",
    "            [f\"{chr(65 + i)}. {choice}\" for i, choice in enumerate(choices)]\n",
    "        )\n",
    "\n",
    "    def get_max_letter(self, choices):\n",
    "        \"\"\"Get the maximum letter based on number of choices\"\"\"\n",
    "        return chr(65 + len(choices) - 1)\n",
    "\n",
    "    def create_inference_prompt(self, question, choices):\n",
    "        \"\"\"\n",
    "        Create a prompt for inference based on current prompt type\n",
    "\n",
    "        Args:\n",
    "            question: The question text\n",
    "            choices: List of choices\n",
    "\n",
    "        Returns:\n",
    "            Formatted prompt string\n",
    "        \"\"\"\n",
    "        formatted_choices = self.format_choices(choices)\n",
    "        max_letter = self.get_max_letter(choices)\n",
    "\n",
    "        if self.prompt_type == self.YAML_REASONING:\n",
    "            return self._create_yaml_prompt(question, formatted_choices, max_letter)\n",
    "        else:\n",
    "            return self._create_basic_prompt(question, formatted_choices, max_letter)\n",
    "\n",
    "    def _create_basic_prompt(self, question, formatted_choices, max_letter):\n",
    "        \"\"\"Create a basic prompt asking for just the answer letter\"\"\"\n",
    "        return f\"\"\"\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CHOICES:\n",
    "{formatted_choices}\n",
    "\n",
    "Answer with a single letter from A through {max_letter} without any additional explanation or commentary.\n",
    "\"\"\"\n",
    "\n",
    "    def _create_yaml_prompt(self, question, formatted_choices, max_letter):\n",
    "        \"\"\"Create a prompt requesting YAML-formatted reasoning\"\"\"\n",
    "        return f\"\"\"\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CHOICES:\n",
    "{formatted_choices}\n",
    "\n",
    "Analyze this question step-by-step and provide a detailed explanation.\n",
    "Your response MUST be in YAML format as follows:\n",
    "\n",
    "understanding: |\n",
    "  <your understanding of what the question is asking>\n",
    "analysis: |\n",
    "  <your analysis of each option>\n",
    "reasoning: |\n",
    "  <your step-by-step reasoning process>\n",
    "conclusion: |\n",
    "  <your final conclusion>\n",
    "answer: <single letter A through {max_letter}>\n",
    "\n",
    "The answer field MUST contain ONLY a single character letter.\n",
    "\"\"\"\n",
    "\n",
    "    def create_training_prompt(self, question, choices):\n",
    "        \"\"\"\n",
    "        Create a prompt for training with the current prompt type\n",
    "\n",
    "        Args:\n",
    "            question: The question text\n",
    "            choices: List of choices\n",
    "\n",
    "        Returns:\n",
    "            Formatted prompt string for training\n",
    "        \"\"\"\n",
    "        formatted_choices = self.format_choices(choices)\n",
    "        max_letter = self.get_max_letter(choices)\n",
    "\n",
    "        if self.prompt_type == self.YAML_REASONING:\n",
    "            return self._create_yaml_training_prompt(\n",
    "                question, formatted_choices, max_letter\n",
    "            )\n",
    "        else:\n",
    "            return self._create_basic_training_prompt(\n",
    "                question, formatted_choices, max_letter\n",
    "            )\n",
    "\n",
    "    def _create_basic_training_prompt(self, question, formatted_choices, max_letter):\n",
    "        \"\"\"Create a basic training prompt\"\"\"\n",
    "        return f\"\"\"\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CHOICES:\n",
    "{formatted_choices}\n",
    "\n",
    "The answer is a single letter (A, B, C, etc.). Only provide ONE character as your answer:\n",
    "\"\"\"\n",
    "\n",
    "    def _create_yaml_training_prompt(self, question, formatted_choices, max_letter):\n",
    "        \"\"\"Create a YAML-formatted training prompt\"\"\"\n",
    "        return f\"\"\"\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CHOICES:\n",
    "{formatted_choices}\n",
    "\n",
    "Analyze this question step-by-step and provide a detailed explanation.\n",
    "Follow the YAML format in your response:\n",
    "\n",
    "understanding: |\n",
    "  <your understanding of the question>\n",
    "analysis: |\n",
    "  <your analysis of each option>\n",
    "reasoning: |\n",
    "  <your reasoning about the correct answer>\n",
    "conclusion: |\n",
    "  <your final conclusion>\n",
    "answer: <single letter A through {max_letter}>\n",
    "\"\"\"\n",
    "\n",
    "    def set_prompt_type(self, prompt_type):\n",
    "        \"\"\"Set the prompt type\"\"\"\n",
    "        # For prompt formatting, teacher_reasoned is equivalent to yaml_reasoning\n",
    "        self.original_type = prompt_type  # Store the original type\n",
    "        \n",
    "        if prompt_type == self.TEACHER_REASONED:\n",
    "            # prompt_type = self.YAML_REASONING\n",
    "            pass\n",
    "\n",
    "        self.prompt_type = prompt_type\n",
    "        return self\n",
    "        \n",
    "    def is_teacher_mode(self):\n",
    "        \"\"\"Check if we're using teacher mode (for training with teacher completions)\"\"\"\n",
    "        return self.original_type == self.TEACHER_REASONED\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of QwenModelHandler class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QwenModelHandler:\n",
    "    \"\"\"Handler for Qwen models with inference and saving capabilities using Unsloth\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"unsloth/Qwen2.5-7B\", max_seq_length=768, \n",
    "                 quantization=None, device_map=\"auto\", cache_dir=None):\n",
    "        \"\"\"\n",
    "        Initialize model and tokenizer using Unsloth\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name or path of the model (preferably an unsloth model)\n",
    "            max_seq_length: Maximum sequence length for the model\n",
    "            quantization: Quantization type (None, '4bit', '8bit') - for compatibility\n",
    "            device_map: Device mapping strategy\n",
    "            cache_dir: Cache directory for models\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.device_map = device_map\n",
    "        self.quantization = quantization\n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "        # Convert quantization parameter to load_in_4bit parameter for Unsloth\n",
    "        self.load_in_4bit = quantization == \"4bit\"\n",
    "        \n",
    "        # Load tokenizer and model\n",
    "        self.tokenizer, self.model = self._load_model()\n",
    "        self.response_parser = ResponseParser()\n",
    "        \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load model and tokenizer with Unsloth for optimization\"\"\"\n",
    "        from unsloth import FastLanguageModel\n",
    "        import torch\n",
    "        \n",
    "        print(f\"Loading {self.model_name} with Unsloth, max_seq_length={self.max_seq_length}\")\n",
    "        \n",
    "        # Set dtype based on hardware\n",
    "        dtype = None  # None for auto detection\n",
    "        \n",
    "        # Load model and tokenizer with Unsloth\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=self.model_name,\n",
    "            max_seq_length=self.max_seq_length,\n",
    "            dtype=dtype,\n",
    "            load_in_4bit=self.load_in_4bit,\n",
    "            cache_dir=self.cache_dir,\n",
    "        )\n",
    "        \n",
    "        return tokenizer, model\n",
    "      \n",
    "    def generate_with_streaming(self, prompt, temperature=0.7, max_tokens=1024, stream=True, use_cache=True):\n",
    "        \"\"\"\n",
    "        Generate completion with optional streaming using Unsloth's optimized inference\n",
    "        \"\"\"\n",
    "        # Enable faster inference\n",
    "        from unsloth import FastLanguageModel\n",
    "        FastLanguageModel.for_inference(self.model)\n",
    "        \n",
    "        # Format as chat\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        chat_text = self.tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Tokenize input\n",
    "        model_inputs = self.tokenizer([chat_text], return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        # Generate with streaming if requested\n",
    "        if stream:\n",
    "            from transformers import TextIteratorStreamer\n",
    "            import threading\n",
    "            \n",
    "            # Set up streamer\n",
    "            streamer = TextIteratorStreamer(\n",
    "                self.tokenizer,\n",
    "                skip_prompt=True,\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            \n",
    "            # Start generation in a thread\n",
    "            generation_kwargs = {\n",
    "                \"input_ids\": model_inputs.input_ids,\n",
    "                \"attention_mask\": model_inputs.attention_mask,\n",
    "                \"temperature\": temperature,\n",
    "                \"max_new_tokens\": max_tokens,\n",
    "                \"streamer\": streamer,\n",
    "                \"do_sample\": temperature > 0.0,\n",
    "                \"use_cache\": use_cache,  # Important for Unsloth performance\n",
    "                \"min_p\": 0.1 if temperature > 0.0 else None, # Optional: Unsloth recommends this for better quality\n",
    "            }\n",
    "            \n",
    "            thread = threading.Thread(target=self.model.generate, kwargs=generation_kwargs)\n",
    "            thread.start()\n",
    "            \n",
    "            # Return the streamer that yields text chunks\n",
    "            return streamer\n",
    "        else:\n",
    "            # Generate without streaming\n",
    "            generated_ids = self.model.generate(\n",
    "                input_ids=model_inputs.input_ids,\n",
    "                attention_mask=model_inputs.attention_mask,\n",
    "                temperature=temperature,\n",
    "                max_new_tokens=max_tokens,\n",
    "                do_sample=temperature > 0.0,\n",
    "                use_cache=use_cache,  # Important for Unsloth performance\n",
    "                min_p=0.1 if temperature > 0.0 else None, # Optional: Unsloth recommends this\n",
    "            )\n",
    "            \n",
    "            # Decode the generated text\n",
    "            generated_text = self.tokenizer.decode(\n",
    "                generated_ids[0][model_inputs.input_ids.shape[1]:],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            \n",
    "            return generated_text\n",
    "            \n",
    "    def calculate_perplexity(self, prompt, answer, temperature=0.0):\n",
    "        \"\"\"\n",
    "        Calculate perplexity for a prompt and answer pair\n",
    "        \n",
    "        Args:\n",
    "            prompt: The input prompt\n",
    "            answer: The expected answer\n",
    "            temperature: Sampling temperature\n",
    "            \n",
    "        Returns:\n",
    "            Perplexity score\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        \n",
    "        # Format chat for perplexity calculation\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"assistant\", \"content\": answer}\n",
    "        ]\n",
    "        chat_text = self.tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False\n",
    "        )\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encodings = self.tokenizer(chat_text, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        # Calculate loss\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**encodings, labels=encodings.input_ids)\n",
    "            \n",
    "        # Get loss and calculate perplexity\n",
    "        neg_log_likelihood = outputs.loss.item()\n",
    "        perplexity = torch.exp(torch.tensor(neg_log_likelihood)).item()\n",
    "        \n",
    "        return perplexity\n",
    "  \n",
    "    def save_model(self, output_dir, save_method=\"lora\"):\n",
    "        \"\"\"\n",
    "        Save model to disk using Unsloth's optimized methods\n",
    "        \n",
    "        Args:\n",
    "            output_dir: Directory to save the model\n",
    "            save_method: Method to use for saving (\"lora\", \"merged_16bit\", \"merged_4bit\", \"gguf\")\n",
    "        \"\"\"\n",
    "        import os\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Use Unsloth's saving methods\n",
    "        if save_method == \"lora\":\n",
    "            # Save LoRA weights\n",
    "            self.model.save_pretrained(output_dir)\n",
    "            self.tokenizer.save_pretrained(output_dir)\n",
    "        elif save_method == \"merged_16bit\":\n",
    "            # Save merged model in float16\n",
    "            self.model.save_pretrained_merged(output_dir, self.tokenizer, save_method=\"merged_16bit\")\n",
    "        elif save_method == \"merged_4bit\":\n",
    "            # Save merged model in 4bit\n",
    "            self.model.save_pretrained_merged(output_dir, self.tokenizer, save_method=\"merged_4bit\")\n",
    "        elif save_method == \"gguf\":\n",
    "            # Save in GGUF format for llama.cpp\n",
    "            self.model.save_pretrained_gguf(output_dir, self.tokenizer, quantization_method=\"q4_k_m\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown save method: {save_method}\")\n",
    "            \n",
    "        print(f\"Model saved to {output_dir} using method {save_method}\")\n",
    "        return output_dir\n",
    "        \n",
    "    def push_to_hub(self, repo_id, token=None, save_method=\"lora\", private=False):\n",
    "        \"\"\"\n",
    "        Push model to Hugging Face Hub using Unsloth's optimized methods\n",
    "        \"\"\"\n",
    "        # Use Unsloth's hub methods directly\n",
    "        if save_method == \"lora\":\n",
    "            self.model.push_to_hub_merged(repo_id, self.tokenizer, save_method=\"lora\", token=token)\n",
    "        elif save_method == \"merged_16bit\":\n",
    "            self.model.push_to_hub_merged(repo_id, self.tokenizer, save_method=\"merged_16bit\", token=token)\n",
    "        elif save_method == \"merged_4bit\":\n",
    "            self.model.push_to_hub_merged(repo_id, self.tokenizer, save_method=\"merged_4bit\", token=token)\n",
    "        elif save_method == \"gguf\":\n",
    "            # Push multiple GGUF variants\n",
    "            self.model.push_to_hub_gguf(\n",
    "                repo_id, \n",
    "                self.tokenizer, \n",
    "                quantization_method=[\"q4_k_m\", \"q5_k_m\"], \n",
    "                token=token\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown save method: {save_method}\")\n",
    "        \n",
    "        print(f\"Model successfully pushed to: https://huggingface.co/{repo_id}\")\n",
    "        return f\"https://huggingface.co/{repo_id}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of ResponseParser class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseParser:\n",
    "    \"\"\"\n",
    "    Parser for model responses with support for different formats\n",
    "    Extracts answers and reasoning from model outputs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parser modes\n",
    "    BASIC = \"basic\"        # Extract single letter answer\n",
    "    YAML = \"yaml\"          # Parse YAML formatted response with reasoning\n",
    "    \n",
    "    def __init__(self, parser_mode=BASIC):\n",
    "        \"\"\"\n",
    "        Initialize with specified parser mode\n",
    "        \n",
    "        Args:\n",
    "            parser_mode: Mode to use for parsing - \"basic\" or \"yaml\"\n",
    "        \"\"\"\n",
    "        self.parser_mode = parser_mode\n",
    "    \n",
    "    def parse(self, response_text):\n",
    "        \"\"\"\n",
    "        Parse the model's response according to the current mode\n",
    "        \n",
    "        Args:\n",
    "            response_text: Raw response text from the model\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (answer, reasoning)\n",
    "        \"\"\"\n",
    "        if self.parser_mode == self.YAML:\n",
    "            return self._parse_yaml_response(response_text)\n",
    "        else:\n",
    "            return self._parse_basic_response(response_text)\n",
    "    \n",
    "    def _parse_basic_response(self, response_text):\n",
    "        \"\"\"\n",
    "        Parse basic response looking for a letter answer\n",
    "        \n",
    "        For basic mode, we look for a single letter (A-Z) with minimal reasoning\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Try to extract a single letter answer (A-Z)\n",
    "        answer_match = re.search(r\"(?:^|\\s)([A-Z])(?:\\s|$|\\.)\", response_text)\n",
    "        if answer_match:\n",
    "            answer = answer_match.group(1)\n",
    "        else:\n",
    "            # Take first character if it's a letter\n",
    "            if response_text and response_text[0].isalpha():\n",
    "                answer = response_text[0].upper()\n",
    "            else:\n",
    "                answer = None\n",
    "        \n",
    "        # For basic mode, we don't extract detailed reasoning\n",
    "        reasoning = \"\"\n",
    "        \n",
    "        return answer, reasoning\n",
    "    \n",
    "    def _parse_yaml_response(self, response_text):\n",
    "        \"\"\"\n",
    "        Parse YAML formatted response extracting answer and reasoning\n",
    "        \n",
    "        For YAML mode, we try to extract both the answer and structured reasoning\n",
    "        \"\"\"\n",
    "        import re\n",
    "        import yaml\n",
    "        \n",
    "        # First try to find answer in YAML format\n",
    "        yaml_match = re.search(r\"answer:\\s*([A-Z])\", response_text)\n",
    "        if yaml_match:\n",
    "            answer = yaml_match.group(1)\n",
    "        else:\n",
    "            # Fall back to basic extraction if YAML parsing fails\n",
    "            answer_match = re.search(r\"(?:^|\\s)([A-Z])(?:\\s|$|\\.)\", response_text)\n",
    "            if answer_match:\n",
    "                answer = answer_match.group(1)\n",
    "            elif response_text and response_text[0].isalpha():\n",
    "                answer = response_text[0].upper()\n",
    "            else:\n",
    "                answer = None\n",
    "        \n",
    "        # Try to parse reasoning from YAML format\n",
    "        reasoning = \"\"\n",
    "        if \"reasoning:\" in response_text:\n",
    "            yaml_content = yaml.safe_load(\"---\\n\" + response_text)\n",
    "            if isinstance(yaml_content, dict) and \"reasoning\" in yaml_content:\n",
    "                reasoning = yaml_content[\"reasoning\"]\n",
    "                \n",
    "                # Add other YAML fields if available\n",
    "                if \"understanding\" in yaml_content:\n",
    "                    reasoning = f\"Understanding: {yaml_content['understanding']}\\n\\n{reasoning}\"\n",
    "                if \"conclusion\" in yaml_content:\n",
    "                    reasoning = f\"{reasoning}\\n\\nConclusion: {yaml_content['conclusion']}\"\n",
    "        else:\n",
    "            # Use the full response as reasoning if not in YAML format\n",
    "            reasoning = response_text\n",
    "        \n",
    "        return answer, reasoning\n",
    "    \n",
    "    def set_parser_mode(self, parser_mode):\n",
    "        \"\"\"Set the parser mode\"\"\"\n",
    "        self.parser_mode = parser_mode\n",
    "        return self\n",
    "    \n",
    "    @classmethod\n",
    "    def from_prompt_type(cls, prompt_type):\n",
    "        \"\"\"\n",
    "        Create a parser instance with mode matching the prompt type\n",
    "        \n",
    "        Args:\n",
    "            prompt_type: Prompt type from PromptCreator\n",
    "            \n",
    "        Returns:\n",
    "            ResponseParser instance with appropriate mode\n",
    "        \"\"\"\n",
    "        if prompt_type == PromptCreator.YAML_REASONING or prompt_type == PromptCreator.TEACHER_REASONED:\n",
    "            return cls(parser_mode=cls.YAML)\n",
    "        else:\n",
    "            return cls(parser_mode=cls.BASIC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of MultipleChoiceTester class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleChoiceTester:\n",
    "    \"\"\"Framework for testing Qwen models on multiple choice questions\"\"\"\n",
    "\n",
    "    def __init__(self, model_handler, prompt_creator=None):\n",
    "        \"\"\"\n",
    "        Initialize with model handler and prompt configuration\n",
    "        \n",
    "        Args:\n",
    "            model_handler: The QwenModelHandler instance\n",
    "            prompt_creator: Optional PromptCreator instance (will create one if not provided)\n",
    "        \"\"\"\n",
    "        self.model_handler = model_handler\n",
    "        self.prompt_creator = prompt_creator or PromptCreator(PromptCreator.BASIC)\n",
    "        # Create a response parser matching the prompt type\n",
    "        self.response_parser = ResponseParser.from_prompt_type(self.prompt_creator.prompt_type)\n",
    "\n",
    "    def infer_example(self, example, temperature=0.7, max_tokens=1024, prompt_type=None, stream=False, use_cache=False):\n",
    "        \"\"\"\n",
    "        Mode 1: Inference on a single example for visualization/demonstration\n",
    "        \n",
    "        Args:\n",
    "            example: Single example to infer (dict with question, choices, etc.)\n",
    "            temperature: Sampling temperature for generation\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            prompt_type: Optional override for prompt type\n",
    "            stream: Whether to stream the output\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with prediction and metrics\n",
    "        \"\"\"\n",
    "        # Allow temporary override of prompt type\n",
    "        original_prompt_type = None\n",
    "        if prompt_type is not None:\n",
    "            original_prompt_type = self.prompt_creator.prompt_type\n",
    "            self.prompt_creator.set_prompt_type(prompt_type)\n",
    "            # Update response parser to match prompt type\n",
    "            self.response_parser = ResponseParser.from_prompt_type(prompt_type)\n",
    "        \n",
    "        # Prepare data\n",
    "        question = example[\"question\"]\n",
    "        \n",
    "        # Handle different formats of choices\n",
    "        if isinstance(example[\"choices\"], list):\n",
    "            choices = example[\"choices\"]\n",
    "        elif isinstance(example[\"choices\"], str) and example[\"choices\"].startswith(\"[\"):\n",
    "            # Parse string representation of list\n",
    "            import ast\n",
    "            choices = ast.literal_eval(example[\"choices\"]) if \"[\" in example[\"choices\"] else example[\"choices\"].split(\",\")\n",
    "        else:\n",
    "            choices = str(example[\"choices\"]).split(\",\")\n",
    "        \n",
    "        # Generate the prompt using prompt creator\n",
    "        prompt = self.prompt_creator.create_inference_prompt(question, choices)\n",
    "        \n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if stream:\n",
    "            # Use streaming generation\n",
    "            streamer = self.model_handler.generate_with_streaming(\n",
    "                prompt=prompt,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                stream=stream, \n",
    "                use_cache=use_cache\n",
    "            )\n",
    "            \n",
    "            # Collect output from streamer\n",
    "            raw_response = \"\"\n",
    "            print(\"Model response:\")\n",
    "            for text_chunk in streamer:\n",
    "                print(text_chunk, end=\"\", flush=True)\n",
    "                raw_response += text_chunk\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "            # Generate without streaming\n",
    "            raw_response = self.model_handler.generate_with_streaming(\n",
    "                prompt=prompt,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                stream=stream,\n",
    "                use_cache=use_cache\n",
    "            )\n",
    "        \n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        # Parse the response using the response parser\n",
    "        predicted_answer, reasoning = self.response_parser.parse(raw_response)\n",
    "        \n",
    "        # Prepare results\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"choices\": choices,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"reasoning\": reasoning,\n",
    "            \"response_time\": response_time,\n",
    "            \"raw_response\": raw_response,\n",
    "            \"prompt_type\": self.prompt_creator.prompt_type,\n",
    "        }\n",
    "        \n",
    "        # Add task_id if available\n",
    "        if \"task_id\" in example:\n",
    "            result[\"task_id\"] = example[\"task_id\"]\n",
    "            \n",
    "        # Calculate metrics if label is provided\n",
    "        if \"answer\" in example:\n",
    "            label = example[\"answer\"]\n",
    "            result[\"correct_answer\"] = label\n",
    "            result[\"is_correct\"] = predicted_answer == label\n",
    "            \n",
    "            # Calculate perplexity if requested\n",
    "            if hasattr(self.model_handler, \"calculate_perplexity\"):\n",
    "                perplexity = self.model_handler.calculate_perplexity(prompt, raw_response)\n",
    "                result[\"perplexity\"] = perplexity\n",
    "        \n",
    "        # Restore original prompt type if it was overridden\n",
    "        if original_prompt_type is not None:\n",
    "            self.prompt_creator.set_prompt_type(original_prompt_type)\n",
    "            # Restore the original response parser\n",
    "            self.response_parser = ResponseParser.from_prompt_type(original_prompt_type)\n",
    "            \n",
    "        return result\n",
    "\n",
    "    def infer_batch(self, examples, temperature=0.7, max_tokens=1024, prompt_type=None, batch_size=4):\n",
    "        \"\"\"\n",
    "        Mode 2: Inference on a batch of examples\n",
    "        \n",
    "        Args:\n",
    "            examples: List of examples to infer\n",
    "            temperature: Sampling temperature\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            prompt_type: Optional override for prompt type\n",
    "            batch_size: Size of batches for processing\n",
    "            \n",
    "        Returns:\n",
    "            List of result dictionaries and summary metrics\n",
    "        \"\"\"\n",
    "        # Allow temporary override of prompt type\n",
    "        original_prompt_type = None\n",
    "        if prompt_type is not None:\n",
    "            original_prompt_type = self.prompt_creator.prompt_type\n",
    "            self.prompt_creator.set_prompt_type(prompt_type)\n",
    "            # Update response parser to match prompt type\n",
    "            self.response_parser = ResponseParser.from_prompt_type(prompt_type)\n",
    "        \n",
    "        # Prepare all prompts\n",
    "        prompts = []\n",
    "        metadata = []\n",
    "        \n",
    "        for i, example in enumerate(examples):\n",
    "            # Extract data\n",
    "            question = example[\"question\"]\n",
    "            \n",
    "            # Handle different formats of choices\n",
    "            if isinstance(example[\"choices\"], list):\n",
    "                choices = example[\"choices\"]\n",
    "            elif isinstance(example[\"choices\"], str) and example[\"choices\"].startswith(\"[\"):\n",
    "                # Parse string representation of list\n",
    "                import ast\n",
    "                choices = ast.literal_eval(example[\"choices\"]) if \"[\" in example[\"choices\"] else example[\"choices\"].split(\",\")\n",
    "            else:\n",
    "                choices = str(example[\"choices\"]).split(\",\")\n",
    "            \n",
    "            # Generate the prompt using prompt creator\n",
    "            prompt = self.prompt_creator.create_inference_prompt(question, choices)\n",
    "            prompts.append(prompt)\n",
    "            \n",
    "            # Store metadata for later\n",
    "            meta = {\n",
    "                \"question\": question,\n",
    "                \"choices\": choices,\n",
    "                \"index\": i,\n",
    "            }\n",
    "            \n",
    "            # Add label if available\n",
    "            if \"answer\" in example:\n",
    "                meta[\"label\"] = example[\"answer\"]\n",
    "                \n",
    "            if \"task_id\" in example:\n",
    "                meta[\"task_id\"] = example[\"task_id\"]\n",
    "            \n",
    "            metadata.append(meta)\n",
    "        \n",
    "        # Process in batches\n",
    "        results = []\n",
    "        correct_count = 0\n",
    "        total_count = 0\n",
    "        perplexities = []\n",
    "        \n",
    "        for i in range(0, len(prompts), batch_size):\n",
    "            batch_prompts = prompts[i:i+batch_size]\n",
    "            batch_meta = metadata[i:i+batch_size]\n",
    "            \n",
    "            # Process batch\n",
    "            start_time = time.time()\n",
    "            batch_responses = []\n",
    "            \n",
    "            for prompt in batch_prompts:\n",
    "                response = self.model_handler.generate_with_streaming(\n",
    "                    prompt=prompt,\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=max_tokens,\n",
    "                    stream=False\n",
    "                )\n",
    "                batch_responses.append(response)\n",
    "            \n",
    "            batch_time = time.time() - start_time\n",
    "            \n",
    "            # Process each response in the batch\n",
    "            for j, (response, meta) in enumerate(zip(batch_responses, batch_meta)):\n",
    "                # Parse response\n",
    "                predicted_answer, reasoning = self.response_parser.parse(response)\n",
    "                \n",
    "                # Create result\n",
    "                result = {\n",
    "                    \"question\": meta[\"question\"],\n",
    "                    \"choices\": meta[\"choices\"],\n",
    "                    \"predicted_answer\": predicted_answer,\n",
    "                    \"reasoning\": reasoning,\n",
    "                    \"raw_response\": response,\n",
    "                    \"prompt_type\": self.prompt_creator.prompt_type,\n",
    "                    \"response_time\": batch_time / len(batch_prompts),  # Approximate individual time\n",
    "                }\n",
    "                \n",
    "                # Add task_id if available\n",
    "                if \"task_id\" in meta:\n",
    "                    result[\"task_id\"] = meta[\"task_id\"]\n",
    "                \n",
    "                # Add metrics if label available\n",
    "                if \"label\" in meta:\n",
    "                    label = meta[\"label\"]\n",
    "                    result[\"correct_answer\"] = label\n",
    "                    result[\"is_correct\"] = predicted_answer == label\n",
    "                    \n",
    "                    # Update counts for accuracy\n",
    "                    total_count += 1\n",
    "                    if result[\"is_correct\"]:\n",
    "                        correct_count += 1\n",
    "                        \n",
    "                    # Calculate perplexity if possible\n",
    "                    if hasattr(self.model_handler, \"calculate_perplexity\"):\n",
    "                        prompt = batch_prompts[j]\n",
    "                        perplexity = self.model_handler.calculate_perplexity(prompt, response)\n",
    "                        result[\"perplexity\"] = perplexity\n",
    "                        perplexities.append(perplexity)\n",
    "                        \n",
    "                results.append(result)\n",
    "        \n",
    "        # Calculate aggregate metrics\n",
    "        summary_metrics = {}\n",
    "        if total_count > 0:\n",
    "            summary_metrics[\"accuracy\"] = correct_count / total_count\n",
    "            summary_metrics[\"correct_count\"] = correct_count\n",
    "            summary_metrics[\"total_count\"] = total_count\n",
    "            \n",
    "            if perplexities:\n",
    "                summary_metrics[\"avg_perplexity\"] = sum(perplexities) / len(perplexities)\n",
    "                summary_metrics[\"min_perplexity\"] = min(perplexities)\n",
    "                summary_metrics[\"max_perplexity\"] = max(perplexities)\n",
    "        \n",
    "        # Restore original prompt type if it was overridden\n",
    "        if original_prompt_type is not None:\n",
    "            self.prompt_creator.set_prompt_type(original_prompt_type)\n",
    "            # Restore the original response parser\n",
    "            self.response_parser = ResponseParser.from_prompt_type(original_prompt_type)\n",
    "            \n",
    "        return results, summary_metrics\n",
    "\n",
    "    def evaluate_dataset(self, dataset, temperature=0.7, max_tokens=1024, num_examples=None, \n",
    "                        verbose=True, prompt_type=None, batch_size=4, log_to_wandb=False):\n",
    "        \"\"\"\n",
    "        Mode 3: Inference on a whole dataset with metrics calculation\n",
    "        \n",
    "        Args:\n",
    "            dataset: Dataset to evaluate\n",
    "            temperature: Sampling temperature\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            num_examples: Number of examples to evaluate (None for all)\n",
    "            verbose: Whether to print progress information\n",
    "            prompt_type: Override the prompt type for this evaluation\n",
    "            batch_size: Size of batches for processing\n",
    "            log_to_wandb: Whether to log results to wandb\n",
    "            \n",
    "        Returns:\n",
    "            Summary dictionary with results and metrics\n",
    "        \"\"\"\n",
    "        # Allow overriding the prompt type for this evaluation\n",
    "        original_prompt_type = self.prompt_creator.prompt_type\n",
    "        if prompt_type is not None:\n",
    "            self.prompt_creator.set_prompt_type(prompt_type)\n",
    "            # Update response parser to match prompt type\n",
    "            self.response_parser = ResponseParser.from_prompt_type(prompt_type)\n",
    "            \n",
    "        # Select subset if specified\n",
    "        if num_examples is not None:\n",
    "            dataset = dataset.select(range(min(num_examples, len(dataset))))\n",
    "\n",
    "        results = []\n",
    "        correct_count = 0\n",
    "        total_count = 0\n",
    "        perplexities = []\n",
    "        \n",
    "        # Process examples in batches\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            batch_examples = dataset[i:i+batch_size]\n",
    "            \n",
    "            if verbose:\n",
    "                batch_desc = f\"Batch {i//batch_size + 1}/{(len(dataset) + batch_size - 1) // batch_size}\"\n",
    "                print(f\"\\nProcessing {batch_desc} with {len(batch_examples)} examples...\")\n",
    "            \n",
    "            # Infer batch\n",
    "            batch_results, batch_metrics = self.infer_batch(\n",
    "                examples=batch_examples,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                batch_size=batch_size\n",
    "            )\n",
    "            \n",
    "            # Update metrics\n",
    "            results.extend(batch_results)\n",
    "            if \"correct_count\" in batch_metrics:\n",
    "                correct_count += batch_metrics[\"correct_count\"]\n",
    "                total_count += batch_metrics[\"total_count\"]\n",
    "                \n",
    "                if verbose:\n",
    "                    batch_accuracy = batch_metrics[\"accuracy\"]\n",
    "                    overall_accuracy = correct_count / total_count\n",
    "                    print(f\"Batch accuracy: {batch_accuracy:.2%}, Overall: {overall_accuracy:.2%} ({correct_count}/{total_count})\")\n",
    "                    \n",
    "            # Collect perplexities\n",
    "            if \"avg_perplexity\" in batch_metrics:\n",
    "                for result in batch_results:\n",
    "                    if \"perplexity\" in result:\n",
    "                        perplexities.append(result[\"perplexity\"])\n",
    "                        \n",
    "        # Calculate final accuracy\n",
    "        accuracy = correct_count / total_count if total_count > 0 else 0.0\n",
    "        \n",
    "        if verbose:\n",
    "            prompt_type_str = self.prompt_creator.prompt_type\n",
    "            print(f\"\\nFinal accuracy with {prompt_type_str} prompts: {accuracy:.2%} ({correct_count}/{total_count})\")\n",
    "            if perplexities:\n",
    "                avg_perplexity = sum(perplexities) / len(perplexities)\n",
    "                print(f\"Average perplexity: {avg_perplexity:.4f}\")\n",
    "        \n",
    "        # Prepare comprehensive summary\n",
    "        summary = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"correct_count\": correct_count,\n",
    "            \"total_count\": total_count,\n",
    "            \"prompt_type\": self.prompt_creator.prompt_type,\n",
    "            \"results\": results,\n",
    "        }\n",
    "        \n",
    "        # Add perplexity metrics if available\n",
    "        if perplexities:\n",
    "            summary[\"avg_perplexity\"] = sum(perplexities) / len(perplexities)\n",
    "            summary[\"min_perplexity\"] = min(perplexities)\n",
    "            summary[\"max_perplexity\"] = max(perplexities)\n",
    "            \n",
    "        # Log results to wandb if requested\n",
    "        if log_to_wandb and wandb.run is not None:\n",
    "            metrics = {\n",
    "                \"test/accuracy\": accuracy,\n",
    "                \"test/correct_count\": correct_count,\n",
    "                \"test/total_count\": total_count,\n",
    "            }\n",
    "            if perplexities:\n",
    "                metrics[\"test/avg_perplexity\"] = summary[\"avg_perplexity\"]\n",
    "                metrics[\"test/min_perplexity\"] = summary[\"min_perplexity\"]\n",
    "                metrics[\"test/max_perplexity\"] = summary[\"max_perplexity\"]\n",
    "            \n",
    "            wandb.log(metrics)\n",
    "            \n",
    "            # Create a table of results for visualization if task_id exists\n",
    "            if \"task_id\" in dataset.features:\n",
    "                columns = [\"task_id\", \"question\", \"correct_answer\", \"predicted_answer\", \"is_correct\"]\n",
    "                table = wandb.Table(columns=columns)\n",
    "                \n",
    "                for res in results[:min(100, len(results))]:  # Limit to 100 examples\n",
    "                    table.add_data(\n",
    "                        res.get(\"task_id\", \"unknown\"),\n",
    "                        res[\"question\"][:100] + \"...\",\n",
    "                        res.get(\"correct_answer\", \"\"),\n",
    "                        res.get(\"predicted_answer\", \"\"),\n",
    "                        res.get(\"is_correct\", False)\n",
    "                    )\n",
    "                \n",
    "                wandb.log({\"test_samples\": table})\n",
    "        \n",
    "        # Restore original prompt type\n",
    "        self.prompt_creator.set_prompt_type(original_prompt_type)\n",
    "        # Restore the original response parser\n",
    "        self.response_parser = ResponseParser.from_prompt_type(original_prompt_type)\n",
    "            \n",
    "        return summary\n",
    "\n",
    "    def save_results(self, results, output_dir=\"./results\"):\n",
    "        \"\"\"Save evaluation results to file\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        results_file = os.path.join(output_dir, f\"results_{timestamp}.json\")\n",
    "\n",
    "        # Create serializable results\n",
    "        serializable_results = {\n",
    "            \"accuracy\": results.get(\"accuracy\", 0.0),\n",
    "            \"correct_count\": results.get(\"correct_count\", 0),\n",
    "            \"total_count\": results.get(\"total_count\", 0),\n",
    "            \"timestamp\": timestamp,\n",
    "            \"prompt_type\": results.get(\"prompt_type\", \"unknown\"),\n",
    "        }\n",
    "\n",
    "        # Add perplexity metrics if available\n",
    "        if \"avg_perplexity\" in results:\n",
    "            serializable_results[\"avg_perplexity\"] = results[\"avg_perplexity\"]\n",
    "            serializable_results[\"min_perplexity\"] = results[\"min_perplexity\"]\n",
    "            serializable_results[\"max_perplexity\"] = results[\"max_perplexity\"]\n",
    "\n",
    "        # Process individual results\n",
    "        serializable_results[\"individual_results\"] = []\n",
    "        for result in results[\"results\"]:\n",
    "            # Skip perplexity in individual results to save space\n",
    "            result_copy = result.copy()\n",
    "            if \"perplexity\" in result_copy:\n",
    "                del result_copy[\"perplexity\"]\n",
    "\n",
    "            # Convert choices if needed\n",
    "            choices = result_copy[\"choices\"]\n",
    "            if not isinstance(choices, list):\n",
    "                try:\n",
    "                    import ast\n",
    "                    result_copy[\"choices\"] = ast.literal_eval(choices)\n",
    "                except (SyntaxError, ValueError):\n",
    "                    # Keep as-is if conversion fails\n",
    "                    pass\n",
    "\n",
    "            serializable_results[\"individual_results\"].append(result_copy)\n",
    "\n",
    "        # Save to file\n",
    "        with open(results_file, \"w\") as f:\n",
    "            import json\n",
    "            json.dump(serializable_results, f, indent=2)\n",
    "\n",
    "        print(f\"Results saved to {results_file}\")\n",
    "        return results_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # QwenTrainer class implementation: concealed for confidentiality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for loading the latest model from HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from HuggingFace Hub: tuandunghcmut/Qwen25_Coder_MultipleChoice\n",
      "Loading tuandunghcmut/Qwen25_Coder_MultipleChoice with Unsloth, max_seq_length=2048\n",
      "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.50.3.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 21.964 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.19 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model from HuggingFace Hub!\n"
     ]
    }
   ],
   "source": [
    "# Load the latest model from HuggingFace Hub\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import os\n",
    "# ````\n",
    "# Set HuggingFace Hub credentials if available\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "# Model ID on HuggingFace Hub\n",
    "hub_model_id = \"tuandunghcmut/Qwen25_Coder_MultipleChoice\"\n",
    "\n",
    "print(f\"Loading model from HuggingFace Hub: {hub_model_id}\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "try:\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        hub_model_id,\n",
    "        token=hf_token,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Load model with appropriate parameters for inference\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        hub_model_id,\n",
    "        token=hf_token,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Create a new model handler with the loaded model and tokenizer\n",
    "    # from model_handler import ModelHandler  # Assuming ModelHandler class is available\n",
    "    \n",
    "    # lastest_model_handler_hub = QwenModelHandler(model_name=hub_model_id, max_seq_length=2048, quantization=\"4bit\")\n",
    "    lastest_model_handler_hub = QwenModelHandler(model_name=hub_model_id, max_seq_length=2048)\n",
    "    #  quantization=\"16bit\")\n",
    "\n",
    "    # Use FastLanguageModel\n",
    "    from unsloth.models import FastLanguageModel\n",
    "    FastLanguageModel.for_inference(lastest_model_handler_hub.model)\n",
    "    prompt_creator = PromptCreator(PromptCreator.YAML_REASONING)\n",
    "    # Create a tester with the loaded model\n",
    "    latest_tester_hub = MultipleChoiceTester(lastest_model_handler_hub, prompt_creator=prompt_creator)\n",
    "    \n",
    "    print(\"Successfully loaded model from HuggingFace Hub!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model from HuggingFace Hub: {e}\")\n",
    "    print(\"Continuing with locally trained model...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for fast streaming inference and 10 coding examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Example 1\n",
       "\n",
       "**Question:** Which of the following is NOT a valid way to initialize a variable in C++?\n",
       "\n",
       "**Choices:**\n",
       "- **A.** int x = 5;\n",
       "- **B.** int x(5);\n",
       "- **C.** int x{5};\n",
       "- **D.** int x := 5;\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model Response (streaming):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have set `use_cache` to `False`, but cache_implementation is set to dynamic. cache_implementation will have no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "understanding: |\n",
      "  The question asks which of the given options correctly initializes a variable in C++, focusing on syntax and correctness.\n",
      "analysis: |\n",
      "  A. `int x = 5;`: This is correct because it uses the standard C++ initialization syntax for integers.\n",
      "  B. `int x(5);`: This is incorrect because the parentheses are not used for integer literals in C++. They should be omitted.\n",
      "  C. `int x{5};`: This is correct because it uses curly braces for integer literals in C++11 and later versions.\n",
      "  D. `int x := 5;`: This is incorrect because the colon (:) is not a valid operator for initializing variables in C++.\n",
      "reasoning: |\n",
      "  - Option A uses the standard C++ initialization syntax, which is correct.\n",
      "  - Option B omits the parentheses around the literal value, which is incorrect.\n",
      "  - Option C uses curly braces for integer literals, which is correct.\n",
      "  - Option D uses the colon (:=), which is not a valid operator for initializing variables in C++.\n",
      "conclusion: |\n",
      "  Both options A and C use valid syntax for initializing an integer variable in C++, but option A is more commonly used due to its simplicity and widespread support.\n",
      "answer: A\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer:** A\n",
       "\n",
       "**Correct Answer:** D\n",
       "\n",
       "**Reasoning:**\n",
       "\n",
       "Understanding: The question asks which of the given options correctly initializes a variable in C++, focusing on syntax and correctness.\n",
       "\n",
       "\n",
       "- Option A uses the standard C++ initialization syntax, which is correct.\n",
       "- Option B omits the parentheses around the literal value, which is incorrect.\n",
       "- Option C uses curly braces for integer literals, which is correct.\n",
       "- Option D uses the colon (:=), which is not a valid operator for initializing variables in C++.\n",
       "\n",
       "\n",
       "Conclusion: Both options A and C use valid syntax for initializing an integer variable in C++, but option A is more commonly used due to its simplicity and widespread support.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Example 2\n",
       "\n",
       "**Question:** In C, what does the 'malloc' function do?\n",
       "\n",
       "**Choices:**\n",
       "- **A.** Frees allocated memory\n",
       "- **B.** Allocates memory dynamically\n",
       "- **C.** Manages automatic memory\n",
       "- **D.** Moves allocated memory\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model Response (streaming):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "understanding: |\n",
      "  The question asks about the behavior of the `printf` function when used with an invalid format specifier. This involves understanding how the function handles unexpected input types.\n",
      "analysis: |\n",
      "  A. Incorrectly prints \"Invalid Input\" because it uses a string literal instead of a variable to store the message.\n",
      "  B. Incorrectly prints \"Invalid Input\" because it uses a string literal instead of a variable to store the message.\n",
      "  C. Correctly prints \"Invalid Input\" because it uses a variable to store the message and then prints it.\n",
      "  D. Incorrectly prints \"Invalid Input\" because it uses a string literal instead of a variable to store the message.\n",
      "reasoning: |\n",
      "  The correct approach is to use a variable to hold the message and then print it using `printf`. Option C correctly initializes a variable `msg` with the string \"Invalid Input\", which is then printed using `printf`.\n",
      "conclusion: |\n",
      "  Answer C is correct because it properly initializes a variable to hold the message and uses `printf` to display it, adhering to the requirements of the question.\n",
      "answer: C\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer:** C\n",
       "\n",
       "**Correct Answer:** B\n",
       "\n",
       "**Reasoning:**\n",
       "\n",
       "Understanding: The question asks about the behavior of the `printf` function when used with an invalid format specifier. This involves understanding how the function handles unexpected input types.\n",
       "\n",
       "\n",
       "The correct approach is to use a variable to hold the message and then print it using `printf`. Option C correctly initializes a variable `msg` with the string \"Invalid Input\", which is then printed using `printf`.\n",
       "\n",
       "\n",
       "Conclusion: Answer C is correct because it properly initializes a variable to hold the message and uses `printf` to display it, adhering to the requirements of the question.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Example 3\n",
       "\n",
       "**Question:** Which C++ keyword is used to define a class template?\n",
       "\n",
       "**Choices:**\n",
       "- **A.** class\n",
       "- **B.** virtual\n",
       "- **C.** template\n",
       "- **D.** typename\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model Response (streaming):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "understanding: |\n",
      "  The question asks which header file contains the definition for the `std::vector` container, which is part of the Standard Template Library (STL).\n",
      "analysis: |\n",
      "  A. `<vector>`: This is the correct header file that defines the `std::vector` container.\n",
      "  B. `<list>`: While related to containers, it does not specifically define `std::vector`.\n",
      "  C. `<deque>`: Similar to `<list>`, it also does not define `std::vector`.\n",
      "  D. `<stack>`: Also related to containers, but not specifically `std::vector`.\n",
      "reasoning: |\n",
      "  The `<vector>` header file is included in C++ programs to use the `std::vector` container, which is a dynamic array with contiguous storage. It provides efficient operations for adding, removing, and accessing elements.\n",
      "conclusion: |\n",
      "  The `<vector>` header file is essential for using the `std::vector` container in C++, providing the necessary definitions and functionalities.\n",
      "answer: A\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer:** A\n",
       "\n",
       "**Correct Answer:** C\n",
       "\n",
       "**Reasoning:**\n",
       "\n",
       "Understanding: The question asks which header file contains the definition for the `std::vector` container, which is part of the Standard Template Library (STL).\n",
       "\n",
       "\n",
       "The `<vector>` header file is included in C++ programs to use the `std::vector` container, which is a dynamic array with contiguous storage. It provides efficient operations for adding, removing, and accessing elements.\n",
       "\n",
       "\n",
       "Conclusion: The `<vector>` header file is essential for using the `std::vector` container in C++, providing the necessary definitions and functionalities.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Example 4\n",
       "\n",
       "**Question:** What is the correct way to access a member of a structure through a pointer in C?\n",
       "\n",
       "**Choices:**\n",
       "- **A.** pointer.member\n",
       "- **B.** pointer->member\n",
       "- **C.** pointer::member\n",
       "- **D.** pointer@member\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model Response (streaming):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "understanding: |\n",
      "  The question asks which syntax correctly accesses a member of a structure using a pointer in C.\n",
      "analysis: |\n",
      "  A. `pointer.member`: This attempts to use the dot operator on a pointer, which is not valid in C for accessing members of structures.\n",
      "  B. `pointer->member`: This uses the arrow operator, which is used to dereference pointers and access members of structures.\n",
      "  C. `pointer::member`: This is incorrect because it does not follow C's syntax for accessing members of structures through pointers.\n",
      "  D. `pointer@member`: This is also incorrect due to the lack of proper syntax for accessing members through pointers.\n",
      "reasoning: |\n",
      "  In C, when working with pointers to structures, you must dereference the pointer first using the arrow operator (`->`) before accessing the structure's members. Option B correctly implements this syntax.\n",
      "conclusion: |\n",
      "  Answer B is correct because it uses the arrow operator to dereference the pointer and then access the structure's member, adhering to C's syntax for such operations.\n",
      "answer: B\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer:** B\n",
       "\n",
       "**Correct Answer:** B\n",
       "\n",
       "**Reasoning:**\n",
       "\n",
       "Understanding: The question asks which syntax correctly accesses a member of a structure using a pointer in C.\n",
       "\n",
       "\n",
       "In C, when working with pointers to structures, you must dereference the pointer first using the arrow operator (`->`) before accessing the structure's members. Option B correctly implements this syntax.\n",
       "\n",
       "\n",
       "Conclusion: Answer B is correct because it uses the arrow operator to dereference the pointer and then access the structure's member, adhering to C's syntax for such operations.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Example 5\n",
       "\n",
       "**Question:** Which of the following is NOT a storage class specifier in C?\n",
       "\n",
       "**Choices:**\n",
       "- **A.** static\n",
       "- **B.** extern\n",
       "- **C.** register\n",
       "- **D.** virtual\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model Response (streaming):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "understanding: |\n",
      "  The question asks which of the given options is not a valid data type in Python.\n",
      "analysis: |\n",
      "  A. int: This is a built-in integer data type in Python.\n",
      "  B. float: This is a built-in floating-point number data type in Python.\n",
      "  C. str: This is a built-in string data type in Python.\n",
      "  D. bool: This is a built-in boolean data type in Python.\n",
      "reasoning: |\n",
      "  Each of these options represents a fundamental data type available in Python, specifically for handling integers, floats, strings, and booleans respectively. There are no other data types listed that would be considered invalid or non-existent in Python's standard library.\n",
      "conclusion: |\n",
      "  Answer A is correct because it refers to an existing data type in Python.\n",
      "answer: A\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer:** A\n",
       "\n",
       "**Correct Answer:** D\n",
       "\n",
       "**Reasoning:**\n",
       "\n",
       "Understanding: The question asks which of the given options is not a valid data type in Python.\n",
       "\n",
       "\n",
       "Each of these options represents a fundamental data type available in Python, specifically for handling integers, floats, strings, and booleans respectively. There are no other data types listed that would be considered invalid or non-existent in Python's standard library.\n",
       "\n",
       "\n",
       "Conclusion: Answer A is correct because it refers to an existing data type in Python.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Example 6\n",
       "\n",
       "**Question:** What does the 'const' keyword signify in C++?\n",
       "\n",
       "**Choices:**\n",
       "- **A.** The variable can be modified indirectly\n",
       "- **B.** The variable cannot be modified\n",
       "- **C.** The variable is stored in constant memory\n",
       "- **D.** The variable is initialized at compile time\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model Response (streaming):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "understanding: |\n",
      "  The question asks about the behavior of the `printf` function when used with an invalid format specifier. This involves understanding how the function handles unexpected input types.\n",
      "analysis: |\n",
      "  A. Incorrectly assumes that the program will terminate due to an error, which is not guaranteed.\n",
      "  B. Incorrectly suggests that the program will crash because it uses an undefined value, which is not possible.\n",
      "  C. Correctly identifies that the program will print \"Invalid Input\" because the format specifier `%d` expects an integer but receives a string.\n",
      "  D. Incorrectly implies that the program will execute normally without any issues, which is not true given the invalid format specifier.\n",
      "reasoning: |\n",
      "  When the `printf` function encounters an invalid format specifier like `%d`, it typically results in undefined behavior. In this case, since the format specifier is incorrect, the program may attempt to interpret the input as an integer, leading to an error or unexpected output.\n",
      "conclusion: |\n",
      "  Answer C is correct because the program will print \"Invalid Input\" due to the invalid format specifier, indicating an issue with the input handling.\n",
      "answer: C\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer:** C\n",
       "\n",
       "**Correct Answer:** B\n",
       "\n",
       "**Reasoning:**\n",
       "\n",
       "Understanding: The question asks about the behavior of the `printf` function when used with an invalid format specifier. This involves understanding how the function handles unexpected input types.\n",
       "\n",
       "\n",
       "When the `printf` function encounters an invalid format specifier like `%d`, it typically results in undefined behavior. In this case, since the format specifier is incorrect, the program may attempt to interpret the input as an integer, leading to an error or unexpected output.\n",
       "\n",
       "\n",
       "Conclusion: Answer C is correct because the program will print \"Invalid Input\" due to the invalid format specifier, indicating an issue with the input handling.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Example 7\n",
       "\n",
       "**Question:** Which C++ feature provides runtime polymorphism?\n",
       "\n",
       "**Choices:**\n",
       "- **A.** Virtual functions\n",
       "- **B.** Templates\n",
       "- **C.** Operator overloading\n",
       "- **D.** Friend functions\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model Response (streaming):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "understanding: |\n",
      "  The question asks which C++ feature allows for dynamic binding at runtime, enabling method calls to be resolved based on the actual object type rather than its base class type.\n",
      "analysis: |\n",
      "  A. Virtual functions: These allow for compile-time polymorphism but do not support runtime polymorphism.\n",
      "  B. Templates: While templates can create generic code, they do not inherently support runtime polymorphism.\n",
      "  C. Operator overloading: This does not enable runtime polymorphism; it only changes the behavior of operators.\n",
      "  D. Friend functions: These cannot be used to implement runtime polymorphism.\n",
      "reasoning: |\n",
      "  Runtime polymorphism requires that methods or operations can be called dynamically based on the actual object type, not just the base class type. Friend functions cannot be used for this purpose. Templates and operator overloading do not support runtime polymorphism. Only virtual functions allow for runtime polymorphism because they enable the compiler to choose the most appropriate function to call at runtime based on the actual object's type.\n",
      "conclusion: |\n",
      "  Answer D is correct because friend functions cannot be used for runtime polymorphism, unlike virtual functions which allow dynamic dispatch at runtime.\n",
      "answer: D\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer:** D\n",
       "\n",
       "**Correct Answer:** A\n",
       "\n",
       "**Reasoning:**\n",
       "\n",
       "Understanding: The question asks which C++ feature allows for dynamic binding at runtime, enabling method calls to be resolved based on the actual object type rather than its base class type.\n",
       "\n",
       "\n",
       "Runtime polymorphism requires that methods or operations can be called dynamically based on the actual object type, not just the base class type. Friend functions cannot be used for this purpose. Templates and operator overloading do not support runtime polymorphism. Only virtual functions allow for runtime polymorphism because they enable the compiler to choose the most appropriate function to call at runtime based on the actual object's type.\n",
       "\n",
       "\n",
       "Conclusion: Answer D is correct because friend functions cannot be used for runtime polymorphism, unlike virtual functions which allow dynamic dispatch at runtime.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Example 8\n",
       "\n",
       "**Question:** In C++, what is the purpose of the 'new' operator?\n",
       "\n",
       "**Choices:**\n",
       "- **A.** To create a new class\n",
       "- **B.** To allocate memory dynamically\n",
       "- **C.** To initialize a new variable\n",
       "- **D.** To create a new scope\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model Response (streaming):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "understanding: |\n",
      "  The question asks about the behavior of the `printf` function when used with an incorrect format specifier. It focuses on how the program will execute and terminate based on the input provided.\n",
      "analysis: |\n",
      "  A. The program will crash because it attempts to use an invalid format specifier `%d` for a string.\n",
      "  B. The program will not crash but will print \"Hello World\" followed by a newline, then terminate abnormally due to an error.\n",
      "  C. The program will not crash but will print \"Hello World\" followed by a newline, then terminate normally.\n",
      "  D. The program will not crash but will print \"Hello World\" followed by a newline, then terminate normally.\n",
      "reasoning: |\n",
      "  When using `printf`, if the format specifier does not match the type of the argument, it results in undefined behavior. In this case, `%d` is used to print a string, which is incorrect. This causes the program to terminate unexpectedly, likely due to an internal error or a misinterpretation of the input.\n",
      "conclusion: |\n",
      "  Answer B is correct because the program will not crash but will terminate abnormally due to the invalid format specifier, leading to unexpected termination.\n",
      "answer: B\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer:** B\n",
       "\n",
       "**Correct Answer:** B\n",
       "\n",
       "**Reasoning:**\n",
       "\n",
       "Understanding: The question asks about the behavior of the `printf` function when used with an incorrect format specifier. It focuses on how the program will execute and terminate based on the input provided.\n",
       "\n",
       "\n",
       "When using `printf`, if the format specifier does not match the type of the argument, it results in undefined behavior. In this case, `%d` is used to print a string, which is incorrect. This causes the program to terminate unexpectedly, likely due to an internal error or a misinterpretation of the input.\n",
       "\n",
       "\n",
       "Conclusion: Answer B is correct because the program will not crash but will terminate abnormally due to the invalid format specifier, leading to unexpected termination.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Example 9\n",
       "\n",
       "**Question:** What is the correct way to declare a function pointer in C?\n",
       "\n",
       "**Choices:**\n",
       "- **A.** void (*func)(int);\n",
       "- **B.** void *func(int);\n",
       "- **C.** func->void(int);\n",
       "- **D.** pointer void func(int);\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model Response (streaming):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "understanding: |\n",
      "  The question asks about the syntax for declaring a function pointer in C, which involves specifying the return type and parameter types.\n",
      "analysis: |\n",
      "  A. `void (*func)(int);`: This correctly declares a function pointer that returns `void` and takes an integer argument.\n",
      "  B. `void *func(int);`: This incorrectly uses `*` instead of `()` for the function pointer declaration, which is incorrect syntax.\n",
      "  C. `func->void(int);`: This attempts to use a member access operator on a function pointer, which is not valid syntax.\n",
      "  D. `pointer void func(int);`: This is misspelled and does not follow standard C syntax for function pointers.\n",
      "reasoning: |\n",
      "  Option A correctly uses parentheses around the function signature, indicating it's a function pointer declaration. Option B has a syntax error due to using `*` instead of `()`, making it invalid. Options C and D are incorrect because they either use the wrong syntax or do not properly define a function pointer.\n",
      "conclusion: |\n",
      "  Answer A is correct because it follows the standard C syntax for declaring a function pointer with the correct return type and parameter list.\n",
      "answer: A\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer:** A\n",
       "\n",
       "**Correct Answer:** A\n",
       "\n",
       "**Reasoning:**\n",
       "\n",
       "Understanding: The question asks about the syntax for declaring a function pointer in C, which involves specifying the return type and parameter types.\n",
       "\n",
       "\n",
       "Option A correctly uses parentheses around the function signature, indicating it's a function pointer declaration. Option B has a syntax error due to using `*` instead of `()`, making it invalid. Options C and D are incorrect because they either use the wrong syntax or do not properly define a function pointer.\n",
       "\n",
       "\n",
       "Conclusion: Answer A is correct because it follows the standard C syntax for declaring a function pointer with the correct return type and parameter list.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Example 10\n",
       "\n",
       "**Question:** Which of these is NOT a valid C++ smart pointer type?\n",
       "\n",
       "**Choices:**\n",
       "- **A.** std::unique_ptr\n",
       "- **B.** std::shared_ptr\n",
       "- **C.** std::weak_ptr\n",
       "- **D.** std::auto_ptr\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model Response (streaming):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "understanding: |\n",
      "  The question asks which of the given options is not a valid C++ smart pointer type. Smart pointers are used to manage dynamically allocated memory more safely than raw pointers.\n",
      "analysis: |\n",
      "  A. `std::unique_ptr`: This is a unique pointer that owns its resource and cannot be copied or moved.\n",
      "  B. `std::shared_ptr`: This is a shared pointer that shares ownership of its resource with multiple owners.\n",
      "  C. `std::weak_ptr`: This is a weak pointer that does not own the resource but can be used to check if the resource is still alive.\n",
      "  D. `std::auto_ptr`: This was deprecated in C++11 and is no longer part of the standard library. It is not a valid smart pointer type.\n",
      "reasoning: |\n",
      "  - `std::unique_ptr` is a valid smart pointer for owning resources.\n",
      "  - `std::shared_ptr` is a valid smart pointer for sharing resources among multiple owners.\n",
      "  - `std::weak_ptr` is a valid smart pointer for checking resource ownership without taking ownership.\n",
      "  - `std::auto_ptr` is invalid because it has been deprecated and removed from modern C++ standards.\n",
      "conclusion: |\n",
      "  `std::auto_ptr` is not a valid C++ smart pointer type because it has been deprecated and removed from the standard library.\n",
      "answer: D\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer:** D\n",
       "\n",
       "**Correct Answer:** D\n",
       "\n",
       "**Reasoning:**\n",
       "\n",
       "Understanding: The question asks which of the given options is not a valid C++ smart pointer type. Smart pointers are used to manage dynamically allocated memory more safely than raw pointers.\n",
       "\n",
       "\n",
       "- `std::unique_ptr` is a valid smart pointer for owning resources.\n",
       "- `std::shared_ptr` is a valid smart pointer for sharing resources among multiple owners.\n",
       "- `std::weak_ptr` is a valid smart pointer for checking resource ownership without taking ownership.\n",
       "- `std::auto_ptr` is invalid because it has been deprecated and removed from modern C++ standards.\n",
       "\n",
       "\n",
       "Conclusion: `std::auto_ptr` is not a valid C++ smart pointer type because it has been deprecated and removed from the standard library.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import yaml\n",
    "from IPython.display import Markdown, display\n",
    "import time\n",
    "\n",
    "# Define test examples with varied correct answers\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Which of the following is NOT a valid way to initialize a variable in C++?\",\n",
    "        \"choices\": [\"int x = 5;\", \"int x(5);\", \"int x{5};\", \"int x := 5;\"],\n",
    "        \"answer\": \"D\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"In C, what does the 'malloc' function do?\",\n",
    "        \"choices\": [\"Frees allocated memory\", \"Allocates memory dynamically\", \"Manages automatic memory\", \"Moves allocated memory\"],\n",
    "        \"answer\": \"B\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which C++ keyword is used to define a class template?\",\n",
    "        \"choices\": [\"class\", \"virtual\", \"template\", \"typename\"],\n",
    "        \"answer\": \"C\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the correct way to access a member of a structure through a pointer in C?\",\n",
    "        \"choices\": [\n",
    "            \"pointer.member\",\n",
    "            \"pointer->member\",\n",
    "            \"pointer::member\",\n",
    "            \"pointer@member\"\n",
    "        ],\n",
    "        \"answer\": \"B\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which of the following is NOT a storage class specifier in C?\",\n",
    "        \"choices\": [\"static\", \"extern\", \"register\", \"virtual\"],\n",
    "        \"answer\": \"D\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What does the 'const' keyword signify in C++?\",\n",
    "        \"choices\": [\"The variable can be modified indirectly\", \"The variable cannot be modified\", \"The variable is stored in constant memory\", \"The variable is initialized at compile time\"],\n",
    "        \"answer\": \"B\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which C++ feature provides runtime polymorphism?\",\n",
    "        \"choices\": [\"Virtual functions\", \"Templates\", \"Operator overloading\", \"Friend functions\"],\n",
    "        \"answer\": \"A\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"In C++, what is the purpose of the 'new' operator?\",\n",
    "        \"choices\": [\"To create a new class\", \"To allocate memory dynamically\", \"To initialize a new variable\", \"To create a new scope\"],\n",
    "        \"answer\": \"B\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the correct way to declare a function pointer in C?\",\n",
    "        \"choices\": [\"void (*func)(int);\", \"void *func(int);\", \"func->void(int);\", \"pointer void func(int);\"],\n",
    "        \"answer\": \"A\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which of these is NOT a valid C++ smart pointer type?\",\n",
    "        \"choices\": [\n",
    "            \"std::unique_ptr\",\n",
    "            \"std::shared_ptr\",\n",
    "            \"std::weak_ptr\",\n",
    "            \"std::auto_ptr\"\n",
    "        ],\n",
    "        \"answer\": \"D\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to process and display examples with streaming markdown\n",
    "def process_example(example, index):\n",
    "    md_content = f\"## Example {index+1}\\n\\n\"\n",
    "    md_content += f\"**Question:** {example['question']}\\n\\n\"\n",
    "    md_content += \"**Choices:**\\n\"\n",
    "    \n",
    "    for i, choice in enumerate(example[\"choices\"]):\n",
    "        md_content += f\"- **{chr(65+i)}.** {choice}\\n\"\n",
    "    \n",
    "    display(Markdown(md_content))\n",
    "    \n",
    "    # Convert to YAML format if needed (for examples 4-6 and 8-10)\n",
    "    if index >= 3:\n",
    "        example_dict = yaml.safe_load(yaml.safe_dump(example))\n",
    "    else:\n",
    "        example_dict = example\n",
    "    \n",
    "    # Start streaming response\n",
    "    display(Markdown(\"**Model Response (streaming):**\"))\n",
    "    \n",
    "    result = latest_tester_hub.infer_example(example_dict, temperature=0.00001, stream=True, use_cache=False)\n",
    "    \n",
    "    # Display final result\n",
    "    result_md = f\"**Predicted Answer:** {result['predicted_answer']}\\n\\n\"\n",
    "    result_md += f\"**Correct Answer:** {example['answer']}\\n\\n\"\n",
    "    result_md += \"**Reasoning:**\\n\\n\"\n",
    "    \n",
    "    try:\n",
    "        result_md += result['reasoning']\n",
    "    except Exception as e:\n",
    "        result_md += f\"Error: {e}\"\n",
    "    \n",
    "    display(Markdown(result_md))\n",
    "    display(Markdown(\"---\"))\n",
    "\n",
    "# Process all examples\n",
    "for i, example in enumerate(examples):\n",
    "    process_example(example, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
