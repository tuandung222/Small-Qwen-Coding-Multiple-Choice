# Qwen-Coder-MCQ: Fine-tuning Qwen2.5 for Multiple-Choice Coding Questions

<div align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-blue.svg" alt="Python Version">
  <img src="https://img.shields.io/badge/PyTorch-2.0+-red.svg" alt="PyTorch Version">
  <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License">
</div>

This project provides a framework for fine-tuning **Qwen2.5-Coder-1.5B-Instruct** models on multiple-choice coding questions with structured reasoning. It uses LoRA (Low-Rank Adaptation) for efficient training and includes a comprehensive pipeline for data processing, training, and evaluation.

## üìë Table of Contents

- [Features](#features)
  - [Parameter-Efficient Fine-Tuning](#parameter-efficient-fine-tuning)
  - [Optimized Training](#optimized-training)
  - [Advanced Optimizers and Schedulers](#advanced-optimizers-and-schedulers)
  - [Structured Reasoning](#structured-reasoning)
  - [Comprehensive Evaluation](#comprehensive-evaluation)
  - [Advanced Monitoring](#advanced-monitoring)
  - [HuggingFace Hub Integration](#huggingface-hub-integration)
  - [Development and Testing](#development-and-testing)
- [Advanced Features](#advanced-features)
  - [Parameter-Efficient Fine-Tuning (PEFT)](#parameter-efficient-fine-tuning-peft)
  - [Attention Implementations](#attention-implementations)
  - [Response-Only Training](#response-only-training)
  - [Optimizer and Scheduler Options](#optimizer-and-scheduler-options)
  - [Monitoring and Visualization](#monitoring-and-visualization)
  - [Teacher Synthesis](#teacher-synthesis)
- [Command-Line Arguments](#command-line-arguments)
  - [Prompt Monitoring Arguments](#prompt-monitoring-arguments)
  - [Training Arguments](#training-arguments)
- [Callbacks](#callbacks)
- [Setup](#setup)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
- [Usage](#usage)
  - [Training](#training)
  - [Teacher Synthesis](#teacher-synthesis-1)

## ‚ú® Features

### Parameter-Efficient Fine-Tuning

- **LoRA (Low-Rank Adaptation)** with configurable parameters
- **AdaLoRA** with dynamic rank adjustment
- Support for multiple **PEFT methods** (prefix, prompt, ia3, lokr, oft)
- **Gradient checkpointing** for memory efficiency

### Optimized Training

- **Unsloth integration** for faster training and reduced memory usage
- Multiple **attention implementations** (Flash Attention 2, SDPA, xFormers)
- **Mixed precision training** (FP16/BF16)
- **Gradient accumulation** for effective batch size control

### Advanced Optimizers and Schedulers

- Multiple **optimizer options** (adamw_torch, adam8bit, pagedadam, lion, adafactor)
- Configurable **learning rate schedulers** (cosine, linear, polynomial, etc.)
- **Warmup strategies** with customizable ratios
- **Gradient clipping** and weight decay

### Structured Reasoning

- **YAML-format outputs** for clear reasoning steps
- Multiple **prompt templates** for different approaches
- **Teacher-reasoned training** methodology
- **Response-only training** option for focused learning

### Comprehensive Evaluation

- Multiple **evaluation metrics**
- **Validation strategies** with configurable frequency
- **Best model checkpointing**
- **Early stopping** with customizable patience

### Advanced Monitoring

#### Prompt Monitoring
- **Real-time display** of random training prompts
- **Token distribution analysis** and visualization
- **Prompt diversity tracking** with similarity metrics
- **Quality metrics** (length, complexity, readability)
- **Automatic prompt categorization**
- **Interactive prompt selection** and comparison
- **WandB integration** for prompt analytics
- **Configurable logging** frequency

#### Training Metrics
- **Learning rate tracking**
- **Model loading alerts**
- **GPU memory and gradient monitoring**
- **WandB integration** for experiment tracking

### HuggingFace Hub Integration

- **Automatic repository creation**
- **Configurable push strategies**
- Support for **private repositories**
- Multiple **save formats** (LoRA, merged 16bit, merged 4bit, GGUF)

### Development and Testing

- **Test modes** for rapid iteration
- **Debug sampling** for data inspection
- **Comprehensive logging**
- **Flexible configuration** via CLI

## üöÄ Advanced Features

### Parameter-Efficient Fine-Tuning (PEFT)

The framework supports multiple PEFT methods for efficient model adaptation:

| Method | Description |
|--------|-------------|
| **LoRA** | Low-Rank Adaptation with configurable rank, alpha, and dropout |
| **AdaLoRA** | Adaptive LoRA that dynamically adjusts ranks during training |
| **Prefix Tuning** | Adds trainable continuous prompts |
| **Prompt Tuning** | Adds trainable prompt vectors |
| **IA¬≥** | Scales activations with learned vectors |
| **LoKr** | Combines LoRA with Kronecker product |
| **OFT** | Orthogonal fine-tuning approach |

### Attention Implementations

Multiple attention implementations are supported for optimal performance:

| Implementation | Description |
|----------------|-------------|
| **Flash Attention 2** | Significantly faster training with appropriate hardware |
| **SDPA** | PyTorch's Scaled Dot Product Attention |
| **xFormers** | Memory-efficient attention implementation |
| **Eager** | Standard eager execution mode |
| **Default** | Model's default implementation |

### Response-Only Training

Unsloth's response-only training feature allows focusing on assistant responses:

- Identifies instruction and response segments
- Applies special masking for focused learning
- Configurable instruction and response tokens
- Optional token ID specification

### Optimizer and Scheduler Options

Comprehensive training optimization options:

| Category | Options |
|----------|---------|
| **Optimizers** | adamw_torch, adamw_hf, adam8bit, pagedadam, lion, adafactor |
| **Schedulers** | cosine, linear, cosine_with_restarts, polynomial, constant, constant_with_warmup, inverse_sqrt |
| **Warmup** | Configurable warmup ratio and steps |
| **Regularization** | Weight decay and gradient clipping |

### Monitoring and Visualization

Advanced monitoring capabilities:

#### Prompt Monitoring
- **Real-time display** of random training prompts
- **Token distribution analysis** and visualization
- **Prompt diversity tracking** with similarity metrics
- **Quality metrics** (length, complexity, readability)
- **Automatic prompt categorization**
- **Interactive prompt selection** and comparison
- **WandB integration** for prompt analytics
- **Configurable logging** frequency

#### Training Metrics
- **Learning rate tracking**
- **Model loading alerts**
- **GPU memory and gradient monitoring**
- **WandB integration** for experiment tracking

### Teacher Synthesis

The project includes a teacher synthesis framework for generating high-quality explanations for multiple-choice questions. See [Teacher Synthesis Documentation](src/data_synthesis/README.md) for detailed information about:

- **Supported OpenAI models** (GPT-4o, GPT-4, GPT-3.5-turbo)
- **Generation parameters** and configuration
- **Concurrent processing** capabilities
- **Metrics tracking** and analysis
- **Output formats** and structure

## üõ†Ô∏è Command-Line Arguments

The framework supports extensive configuration via command-line arguments:

### Prompt Monitoring Arguments

| Argument | Description | Default |
|----------|-------------|---------|
| `--prompt-track-diversity` | Enable/disable prompt diversity tracking | True |
| `--prompt-track-quality` | Enable/disable prompt quality metrics | True |
| `--prompt-interactive` | Enable/disable interactive prompt selection mode | False |
| `--prompt-categorize` | Enable/disable automatic prompt categorization | True |
| `--prompt-comparison` | Enable/disable prompt comparison features | True |
| `--max-prompts-to-save` | Maximum number of prompts to save for analysis | 100 |

### Training Arguments

| Argument | Description | Default |
|----------|-------------|---------|
| `--experiment-name` | Name for this experiment | auto-generated timestamp |
| `--source-model` | Base model to fine-tune | unsloth/Qwen2.5-Coder-1.5B-Instruct |
| `--destination-repo` | HF Hub repo for the model | tuandunghcmut/Qwen25_Coder_MultipleChoice_v3 |
| `--max-seq-length` | Maximum sequence length for input | 2048 |
| `--quantization` | Model quantization level | 4bit |
| `--epochs` | Number of training epochs | 3 |
| `--batch-size` | Per device batch size | 24 |
| `--grad-accum` | Gradient accumulation steps | 4 |
| `--learning-rate` | Learning rate | 2e-4 |
| `--warmup-ratio` | Proportion of steps for warmup | 0.1 |
| `--weight-decay` | Weight decay for optimizer | 0.01 |
| `--lora-r` | LoRA attention dimension | 8 |
| `--lora-alpha` | LoRA alpha parameter | 32 |
| `--push-strategy` | When to push to HF Hub | best |
| `--private` | Make the repository private | False |
| `--test-mode` | Use only 2 examples | False |
| `--test-training-mode` | Use only one batch of data | False |
| `--train-on-responses-only` | Enable response-only training | False |
| `--instruction-token` | Token/prefix indicating start of instruction | `<|im_start|>user\n` |
| `--response-token` | Token/prefix indicating start of response | `<|im_start|>assistant\n` |
| `--instruction-token-id` | Token ID for instruction start (optional) | None |
| `--response-token-id` | Token ID for response start (optional) | None |
| `--attention-implementation` | Type of attention implementation to use | default |
| `--use-flash-attention` | Use Flash Attention 2 if available | False |
| `--force-attn-implementation` | Force the attention implementation | False |

## üîÑ Callbacks

The training process includes several specialized callbacks for monitoring and optimization:

| Callback | Description |
|----------|-------------|
| **LRMonitorCallback** | Tracks learning rates and optimizer parameters during training |
| **PromptMonitorCallback** | Displays random training prompts after each logging step |
| **ModelLoadingAlertCallback** | Alerts when model loading method changes |
| **EarlyStoppingCallback** | Implements early stopping to prevent overfitting |
| **ValidationCallback** | Manages validation metrics and best model checkpointing |

## üöÄ Setup

### Prerequisites

- Python 3.10+
- CUDA-capable GPU (recommended)
- HuggingFace account with access token
- Weights & Biases account (optional, for experiment tracking)
- OpenAI API key (required for teacher synthesis)

### Installation

1. Clone the repository:

```bash
git clone https://github.com/tuandung222/Small-Qwen-Coding-Multiple-Choice.git
cd Small-Qwen-Coding-Multiple-Choice
```

2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. Set up environment variables:

You can set up environment variables in two ways:

#### a. Using a `.env` file (recommended):
```bash
# Copy the example .env file
cp .env.example .env

# Edit the .env file with your API keys
nano .env  # or use your preferred text editor
```

Required environment variables in `.env`:
```
HF_TOKEN=your_huggingface_token_here
WANDB_API_KEY=your_wandb_api_key_here
OPENAI_API_KEY=your_openai_api_key_here  # Required for teacher synthesis
```

#### b. Using environment variables directly:
```bash
export HF_TOKEN=your_huggingface_token_here
export WANDB_API_KEY=your_wandb_api_key_here
export OPENAI_API_KEY=your_openai_api_key_here  # Required for teacher synthesis
```

> **Note**: The OpenAI API key is only required if you plan to use the teacher synthesis framework. For regular training without teacher synthesis, this key is optional.

## üìö Usage

### Training

The project provides a Python module with a command-line interface for training. You can import the module in your code or run it directly from the command line.

#### Python Module Import

```python
# Import the main module
from src.run import main

# Run the training with default parameters
if __name__ == "__main__":
    main()
```

#### Direct Script Execution

```bash
# Basic usage
python src/run.py

# Advanced usage with custom parameters
python src/run.py --experiment-name "my_experiment" --source-model "unsloth/Qwen2.5-Coder-1.5B-Instruct" --epochs 5 --batch-size 16 --learning-rate 1e-4
```

### Teacher Synthesis

To generate synthetic explanations for multiple-choice questions using OpenAI models:

```bash
# Basic usage
python src/data_synthesis/gpt4o_generated.py --model gpt-4o --data-path /path/to/dataset --api-key YOUR_API_KEY

# Advanced usage
python src/data_synthesis/gpt4o_generated.py --model gpt-4o --data-path /path/to/dataset --sample-size 100 --temperature 0.2 --max-tokens 2048 --concurrent-requests 5 --output-dir ./my_results
```

See [Teacher Synthesis Documentation](src/data_synthesis/README.md) for more details.
