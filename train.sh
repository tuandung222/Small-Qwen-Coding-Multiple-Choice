python src/run.py \
--experiment-name "Qwen2.5-Coder-1.5B-Instruct_Experiment" \
--source-model "unsloth/Qwen2.5-Coder-1.5B-Instruct" \
--epochs 5 \
--batch-size 32 \
--learning-rate 1e-4 \
--grad-accum 4 \
--warmup-ratio 0.1 \
--weight-decay 0.01 \
--max-seq-length 2048 \
--quantization "4bit" \
--lora-r 8 \
--lora-alpha 32 \
--lora-dropout 0.05 \
--peft-type "lora" \
--target-modules "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj" \
--optimizer "adamw_torch" \
--lr-scheduler "cosine" \
--early-stopping-patience 5 \
--early-stopping-delta 0.01 \
--prompt-template "teacher_reasoned" \
--logging-steps 100 \
--save-steps 500 \
--save-total-limit 3 \
--push-strategy "best" \
--dataset "tuandunghcmut/coding-mcq-reasoning" \
--val-split 0.1 \
--random-seed 42 \
--output-dir "model_output" \
--use-flash-attention True \
--attention-implementation "flash_attention_2" \
--train-on-responses-only True \
--instruction-token "<|im_start|>user\n" \
--response-token "<|im_start|>assistant\n" \
--prompt-track-diversity True \
--prompt-track-quality True \
--prompt-categorize True \
--prompt-comparison True \
--max-prompts-to-save 100 \
--adam-beta1 0.9 \
--adam-beta2 0.999 \
--adam-epsilon 1e-8 \
--max-grad-norm 1.0 \
--optim-bits 8 \
--lr-scheduler-num-cycles 1 \
--lr-scheduler-power 1.0 \
--debug-samples 3
