{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone the repository if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please set this to True if you want to clone the repository\n",
    "SHOULD_CLONE = False\n",
    "project_name = \"Small-Qwen-Coding-Multiple-Choice\"\n",
    "\n",
    "import os, subprocess\n",
    "\n",
    "if SHOULD_CLONE:\n",
    "    subprocess.run(\n",
    "        [\"git\", \"clone\", \"https://github.com/tuandung222/Small-Qwen-Coding-Multiple-Choice.git\"],\n",
    "        check=True,\n",
    "    )\n",
    "    os.chdir(project_name)  # Change directory to the cloned project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If running this notebook in the notebooks folder, go outside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Small-Qwen-Coding-Multiple-Choice\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if in the notebooks folder and go outside\n",
    "if os.getcwd().endswith(\"notebooks\"):\n",
    "    os.chdir(\"..\")  # Change directory to the parent folder\n",
    "\n",
    "print(os.getcwd())  # Print the current working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37725/1378891790.py:16: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  import unsloth, transformers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datasets\n",
    "from pprint import pprint\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login, HfApi\n",
    "import os\n",
    "from getpass import getpass\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "from peft import PeftModel\n",
    "import unsloth, transformers\n",
    "from unsloth.models import FastLanguageModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary modules from the src folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.qwen_handler import QwenModelHandler\n",
    "from src.testing.tester import MultipleChoiceTester\n",
    "from src.prompt_processors.prompt_creator import PromptCreator\n",
    "from src.testing.tester import MultipleChoiceTester"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['task_id', 'question', 'choices'],\n",
      "    num_rows: 1253\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "test_path = os.path.join(\"src\", \"data\", \"b6_test_data.csv\")\n",
    "if not os.path.exists(test_path):\n",
    "    raise FileNotFoundError(f\"Data file not found at: {test_path}\")\n",
    "\n",
    "test_data = datasets.load_dataset(\"csv\", data_files=test_path)[\"train\"]\n",
    "\n",
    "# pprint(test_data[0])\n",
    "# pprint(test_data.features)\n",
    "# pprint(test_data.info)\n",
    "pprint(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the lastest model checkpoint from HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from HuggingFace Hub: tuandunghcmut/Qwen25_Coder_MultipleChoice_v4\n",
      "2025-04-04 19:18:25 - src.model.qwen_handler - INFO - Loading tuandunghcmut/Qwen25_Coder_MultipleChoice_v4 from unsloth, max_seq_length=2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:Loading tuandunghcmut/Qwen25_Coder_MultipleChoice_v4 from unsloth, max_seq_length=2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 19:18:25 - src.model.qwen_handler - INFO - Flash Attention 2 is available (package flash-attn detected)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:Flash Attention 2 is available (package flash-attn detected)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 19:18:25 - src.model.qwen_handler - INFO - Flash Attention 2 version: 2.7.4.post1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:Flash Attention 2 version: 2.7.4.post1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 19:18:25 - src.model.qwen_handler - INFO - xFormers is available (version: 0.0.29.post3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:xFormers is available (version: 0.0.29.post3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 19:18:25 - src.model.qwen_handler - INFO - CUDA is available (version: 12.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:CUDA is available (version: 12.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 19:18:25 - src.model.qwen_handler - INFO - Using attention implementation: flash_attention_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:Using attention implementation: flash_attention_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 19:18:25 - src.model.qwen_handler - INFO - Setting max memory: {0: '27620MiB'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:Setting max memory: {0: '27620MiB'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 19:18:25 - src.model.qwen_handler - INFO - Using attention implementation: flash_attention_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:Using attention implementation: flash_attention_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.50.3.\n",
      "   \\\\   /|    Tesla V100-SXM2-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.19 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 19:18:34 - src.model.qwen_handler - INFO - Model loaded successfully: tuandunghcmut/Qwen25_Coder_MultipleChoice_v4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:Model loaded successfully: tuandunghcmut/Qwen25_Coder_MultipleChoice_v4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 19:18:34 - src.model.qwen_handler - INFO - Model type: qwen2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:Model type: qwen2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 19:18:34 - src.model.qwen_handler - INFO - hidden_size: 1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:hidden_size: 1536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 19:18:34 - src.model.qwen_handler - INFO - intermediate_size: 8960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:intermediate_size: 8960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 19:18:34 - src.model.qwen_handler - INFO - num_hidden_layers: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:num_hidden_layers: 28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 19:18:34 - src.model.qwen_handler - INFO - num_attention_heads: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:num_attention_heads: 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 19:18:34 - src.model.qwen_handler - INFO - torch_dtype: float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:torch_dtype: float16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model from HuggingFace Hub!\n"
     ]
    }
   ],
   "source": [
    "# Set HuggingFace Hub credentials if available\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "# Model ID on HuggingFace Hub\n",
    "hub_model_id = \"tuandunghcmut/Qwen25_Coder_MultipleChoice_v4\"\n",
    "\n",
    "print(f\"Loading model from HuggingFace Hub: {hub_model_id}\")\n",
    "\n",
    "\n",
    "lastest_model_handler = QwenModelHandler(\n",
    "    model_name=hub_model_id,\n",
    "    max_seq_length=2048,\n",
    "    quantization=\"4bit\",\n",
    "    model_source=\"unsloth\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "# Use FastLanguageModel\n",
    "\n",
    "FastLanguageModel.for_inference(lastest_model_handler.model)\n",
    "prompt_creator = PromptCreator(PromptCreator.YAML_REASONING)\n",
    "# Create a tester with the loaded model\n",
    "latest_tester = MultipleChoiceTester(lastest_model_handler, prompt_creator=prompt_creator)\n",
    "\n",
    "print(\"Successfully loaded model from HuggingFace Hub!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left\n",
      "151645\n",
      "151665\n"
     ]
    }
   ],
   "source": [
    "print(lastest_model_handler.tokenizer.padding_side)\n",
    "print(lastest_model_handler.tokenizer.eos_token_id)\n",
    "print(lastest_model_handler.tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to perform batch inference\n",
    "* Generate answers for multiple-choice questions \n",
    "* Save results to a CSV file\n",
    "* Track the results for each example\n",
    "* Display the results as yaml format\n",
    "* Display a sample of the results\n",
    "* Push the results to Hugging Face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_csv_with_batch_inference(\n",
    "    test_dataset,\n",
    "    model_handler,\n",
    "    prompt_creator,\n",
    "    tester,\n",
    "    output_folder=\"./outputs/inference_batch\",\n",
    "    batch_size=16,\n",
    "    temperature=0.001,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    top_k=80,\n",
    "    debug_samples=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate answers for multiple-choice questions in batch mode.\n",
    "\n",
    "    Args:\n",
    "        test_dataset: Dataset containing test examples\n",
    "        model_handler: Model handler instance\n",
    "        prompt_creator: Prompt creator instance\n",
    "        tester: Tester instance\n",
    "        output_folder: Folder to save all output files\n",
    "        batch_size: Number of examples to process at once\n",
    "        temperature: Sampling temperature for generation\n",
    "        max_new_tokens: Maximum number of tokens to generate\n",
    "        do_sample: Whether to use sampling for generation\n",
    "        top_p: Top-p sampling parameter\n",
    "        top_k: Top-k sampling parameter\n",
    "        debug_samples: Number of random samples to debug\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import re\n",
    "    import random\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    from tqdm import tqdm\n",
    "    from datetime import datetime\n",
    "\n",
    "    \n",
    "    # Create timestamp for the run\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    output_folder = os.path.join(output_folder, timestamp)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Create a dictionary to store results\n",
    "    results_dict = {}\n",
    "    \n",
    "    # Create a list to store all detailed results for each example\n",
    "    all_completions = []\n",
    "\n",
    "    # Create a list to store detailed results for debugging\n",
    "    debug_results = []\n",
    "\n",
    "    # Select a few random examples for detailed debugging\n",
    "    all_examples = list(test_dataset)\n",
    "    num_examples = len(all_examples)\n",
    "    debug_indices = random.sample(range(num_examples), min(debug_samples, num_examples))\n",
    "\n",
    "    # Process examples in batches\n",
    "    for batch_start in tqdm(range(0, num_examples, batch_size), desc=\"Processing batches\"):\n",
    "        # Get the current batch\n",
    "        batch_end = min(batch_start + batch_size, num_examples)\n",
    "        batch = all_examples[batch_start:batch_end]\n",
    "\n",
    "        # Prepare batch inputs\n",
    "        batch_inputs = []\n",
    "        batch_task_ids = []\n",
    "        batch_prompts = []\n",
    "        batch_examples = []\n",
    "\n",
    "        for example in batch:\n",
    "            # Create prompt for each example\n",
    "            prompt = prompt_creator.create_inference_prompt(\n",
    "                example[\"question\"], eval(example[\"choices\"])\n",
    "            )\n",
    "\n",
    "            # Format as chat\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            chat_text = model_handler.tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            batch_inputs.append(chat_text)\n",
    "            batch_task_ids.append(example[\"task_id\"])\n",
    "            batch_prompts.append(prompt)\n",
    "            batch_examples.append(example)\n",
    "\n",
    "        # Tokenize all inputs at once\n",
    "        model_handler.tokenizer.padding_side = \"left\"\n",
    "        tokenized_inputs = model_handler.tokenizer(\n",
    "            batch_inputs,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=model_handler.max_seq_length,\n",
    "        ).to(model_handler.model.device)\n",
    "\n",
    "        # Perform batch inference using the model directly\n",
    "        with torch.inference_mode():\n",
    "            generated_ids = model_handler.model.generate(\n",
    "                input_ids=tokenized_inputs.input_ids,\n",
    "                attention_mask=tokenized_inputs.attention_mask,\n",
    "                temperature=temperature,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=do_sample,\n",
    "                top_p=top_p,\n",
    "                top_k=top_k,\n",
    "                use_cache=True,\n",
    "                pad_token_id=model_handler.tokenizer.pad_token_id,\n",
    "            )\n",
    "\n",
    "            # Extract only the generated part (not the input)\n",
    "            batch_outputs = []\n",
    "            for i, gen_ids in enumerate(generated_ids):\n",
    "                # Get the length of the input\n",
    "                input_length = tokenized_inputs.input_ids[i].shape[0]\n",
    "                # Decode only the generated part\n",
    "                output_text = model_handler.tokenizer.decode(\n",
    "                    gen_ids[input_length:], skip_special_tokens=True\n",
    "                )\n",
    "                batch_outputs.append(output_text)\n",
    "\n",
    "        # Process batch results and write to batch file\n",
    "        batch_file = os.path.join(output_folder, f\"batch_{batch_start//batch_size + 1}_{timestamp}.txt\")\n",
    "        batch_results = []\n",
    "        \n",
    "        with open(batch_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for i, (task_id, output, example) in enumerate(zip(batch_task_ids, batch_outputs, batch_examples)):\n",
    "                # Extract predicted answer from model output\n",
    "                result = {}\n",
    "                \n",
    "                # Simple regex to find the answer (A, B, C, or D)\n",
    "                answer_match = re.search(r\"answer\\s*(?:is|:)?\\s*([ABCD])\", output, re.IGNORECASE)\n",
    "                if answer_match:\n",
    "                    result[\"predicted_answer\"] = answer_match.group(1).upper()\n",
    "                else:\n",
    "                    # Fallback: look for the first occurrence of A, B, C, or D\n",
    "                    for letter in [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\"]:\n",
    "                        if (\n",
    "                            f\"({letter})\" in output\n",
    "                            or f\"{letter}.\" in output\n",
    "                            or f\"answer {letter}\" in output.lower()\n",
    "                        ):\n",
    "                            result[\"predicted_answer\"] = letter\n",
    "                            break\n",
    "                    else:\n",
    "                        # If no answer found, default to A\n",
    "                        result[\"predicted_answer\"] = \"A\"\n",
    "\n",
    "                # Extract reasoning if available\n",
    "                result[\"reasoning\"] = output\n",
    "                result[\"task_id\"] = task_id\n",
    "                \n",
    "                # Store all original example data along with completions\n",
    "                completion_record = {\n",
    "                    \"task_id\": task_id,\n",
    "                    \"question\": example[\"question\"],\n",
    "                    \"choices\": example[\"choices\"],\n",
    "                    \"predicted_answer\": result[\"predicted_answer\"],\n",
    "                    \"completion\": output,\n",
    "                }\n",
    "                if \"answer\" in example:\n",
    "                    completion_record[\"ground_truth\"] = example[\"answer\"]\n",
    "                \n",
    "                all_completions.append(completion_record)\n",
    "                \n",
    "                # Write to batch file\n",
    "                f.write(f\"Task ID: {task_id}\\n\")\n",
    "                f.write(f\"Question: {example['question']}\\n\")\n",
    "                f.write(f\"Choices: {example['choices']}\\n\")\n",
    "                f.write(f\"Predicted Answer: {result['predicted_answer']}\\n\")\n",
    "                f.write(f\"Completion: {output}\\n\")\n",
    "                if \"answer\" in example:\n",
    "                    f.write(f\"Ground Truth: {example['answer']}\\n\")\n",
    "                f.write(\"\\n\" + \"-\"*80 + \"\\n\\n\")  # Separator\n",
    "                \n",
    "                # Store the result for submission\n",
    "                results_dict[task_id] = result[\"predicted_answer\"]\n",
    "                batch_results.append(result)\n",
    "\n",
    "                # Print progress update for every 10th example or last in batch\n",
    "                if i % 10 == 0 or i == len(batch) - 1:\n",
    "                    print(f\"Batch {batch_start//batch_size + 1}: Processed {i+1}/{len(batch)} examples\")\n",
    "\n",
    "                # For selected examples, save detailed results for debugging\n",
    "                global_index = batch_start + i\n",
    "                if global_index in debug_indices:\n",
    "                    example = batch[i]\n",
    "                    debug_results.append(\n",
    "                        {\n",
    "                            \"task_id\": task_id,\n",
    "                            \"question\": example[\"question\"],\n",
    "                            \"choices\": example[\"choices\"],\n",
    "                            \"predicted_answer\": result[\"predicted_answer\"],\n",
    "                            \"reasoning\": result.get(\"reasoning\", \"No reasoning provided\"),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    # Print detailed debug information for these examples\n",
    "                    print(f\"\\n--- DETAILED DEBUG FOR TASK {task_id} ---\")\n",
    "                    print(f\"Question: {example['question'][:100]}...\")\n",
    "                    print(f\"Choices: {example['choices']}\")\n",
    "                    print(f\"Predicted: {result['predicted_answer']}\")\n",
    "                    print(f\"Reasoning snippet: {str(result.get('reasoning', 'No reasoning'))[:200]}...\")\n",
    "                    print(\"-----------------------------------\\n\")\n",
    "\n",
    "    # Create a DataFrame from the results dictionary for submission\n",
    "    submission_df = pd.DataFrame(list(results_dict.items()), columns=[\"task_id\", \"answer\"])\n",
    "\n",
    "    # Create a DataFrame with all completions\n",
    "    all_completions_df = pd.DataFrame(all_completions)\n",
    "\n",
    "    # Save all the files to the output folder\n",
    "    submission_file = os.path.join(output_folder, f\"submission_{timestamp}.csv\")\n",
    "    debug_file = os.path.join(output_folder, f\"debug_results_{timestamp}.csv\")\n",
    "    all_completions_file = os.path.join(output_folder, f\"all_completions_{timestamp}.csv\")\n",
    "    all_completions_json = os.path.join(output_folder, f\"all_completions_{timestamp}.json\")\n",
    "\n",
    "    # Save files\n",
    "    submission_df.to_csv(submission_file, index=False)\n",
    "    pd.DataFrame(debug_results).to_csv(debug_file, index=False)\n",
    "    all_completions_df.to_csv(all_completions_file, index=False)\n",
    "    \n",
    "    # Save as JSON for better inspection of large text fields\n",
    "    import json\n",
    "    with open(all_completions_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_completions, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Submission saved to {submission_file}\")\n",
    "    print(f\"Debug results saved to {debug_file}\")\n",
    "    print(f\"All completions saved to {all_completions_file} and {all_completions_json}\")\n",
    "    \n",
    "    return {\n",
    "        \"submission_file\": submission_file,\n",
    "        \"debug_file\": debug_file, \n",
    "        \"all_completions_file\": all_completions_file,\n",
    "        \"all_completions_json\": all_completions_json,\n",
    "        \"results\": results_dict,\n",
    "        \"debug_results\": debug_results,\n",
    "        \"all_completions\": all_completions, \n",
    "        \"output_folder\": output_folder\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on the test dataset and save the results to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|▏         | 1/79 [00:34<44:35, 34.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: Processed 1/16 examples\n",
      "Batch 1: Processed 11/16 examples\n",
      "Batch 1: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|▎         | 2/79 [00:56<34:56, 27.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2: Processed 1/16 examples\n",
      "Batch 2: Processed 11/16 examples\n",
      "Batch 2: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|▍         | 3/79 [01:18<31:41, 25.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3: Processed 1/16 examples\n",
      "\n",
      "--- DETAILED DEBUG FOR TASK k10466 ---\n",
      "Question: Question: Leapfrog migration strategy allows faster development of new features with less risk that ...\n",
      "Choices: ['True', 'False']\n",
      "Predicted: A\n",
      "Reasoning snippet: understanding: |\n",
      "  The question is evaluating the distinction between leapfrog and strangler migration strategies in software development. It asks whether one strategy facilitates faster feature devel...\n",
      "-----------------------------------\n",
      "\n",
      "Batch 3: Processed 11/16 examples\n",
      "Batch 3: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|▌         | 4/79 [01:47<32:50, 26.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4: Processed 1/16 examples\n",
      "Batch 4: Processed 11/16 examples\n",
      "Batch 4: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|▋         | 5/79 [02:16<33:49, 27.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5: Processed 1/16 examples\n",
      "Batch 5: Processed 11/16 examples\n",
      "Batch 5: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|▊         | 6/79 [02:52<36:55, 30.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6: Processed 1/16 examples\n",
      "Batch 6: Processed 11/16 examples\n",
      "Batch 6: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   9%|▉         | 7/79 [03:48<46:26, 38.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7: Processed 1/16 examples\n",
      "\n",
      "--- DETAILED DEBUG FOR TASK k10869 ---\n",
      "Question: Question: The distance between two stations M and N is L kilometers. All frames are K bits long. The...\n",
      "Choices: ['A', 'B', 'C', 'D']\n",
      "Predicted: C\n",
      "Reasoning snippet: understanding: |\n",
      "  The question asks for the minimum number of bits needed for the sequence number field in a frame using the sliding window protocol to maximize channel utilization. Key factors inclu...\n",
      "-----------------------------------\n",
      "\n",
      "Batch 7: Processed 11/16 examples\n",
      "Batch 7: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  10%|█         | 8/79 [04:45<52:47, 44.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8: Processed 1/16 examples\n",
      "Batch 8: Processed 11/16 examples\n",
      "Batch 8: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  11%|█▏        | 9/79 [05:34<53:37, 45.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9: Processed 1/16 examples\n",
      "Batch 9: Processed 11/16 examples\n",
      "Batch 9: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  13%|█▎        | 10/79 [06:09<48:50, 42.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10: Processed 1/16 examples\n",
      "Batch 10: Processed 11/16 examples\n",
      "Batch 10: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  14%|█▍        | 11/79 [06:48<47:03, 41.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11: Processed 1/16 examples\n",
      "Batch 11: Processed 11/16 examples\n",
      "Batch 11: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  15%|█▌        | 12/79 [07:24<44:29, 39.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 12: Processed 1/16 examples\n",
      "Batch 12: Processed 11/16 examples\n",
      "Batch 12: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  16%|█▋        | 13/79 [07:41<36:09, 32.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 13: Processed 1/16 examples\n",
      "Batch 13: Processed 11/16 examples\n",
      "Batch 13: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  18%|█▊        | 14/79 [08:01<31:28, 29.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 14: Processed 1/16 examples\n",
      "Batch 14: Processed 11/16 examples\n",
      "Batch 14: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  19%|█▉        | 15/79 [08:23<28:29, 26.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 15: Processed 1/16 examples\n",
      "Batch 15: Processed 11/16 examples\n",
      "Batch 15: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  20%|██        | 16/79 [08:45<26:33, 25.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 16: Processed 1/16 examples\n",
      "\n",
      "--- DETAILED DEBUG FOR TASK rt00563 ---\n",
      "Question: Question: Look at the problem below, the solution is missing a part, which option is the most likely...\n",
      "Choices: [\"    d = {3: 'Fizz', 5: 'Buzz'}\", \"    d = {4: 'Quux', 9: 'Quuz'}\", \"    d = {3: 'F', 5: 'B'}\", \"    d = {2: 'Foo', 7: 'Bar'}\"]\n",
      "Predicted: A\n",
      "Reasoning snippet: understanding: |\n",
      "  The question asks for the completion of a function that generates a list of strings representing numbers from 1 to n, replacing multiples of 3 with \"Fizz\", multiples of 5 with \"Buzz...\n",
      "-----------------------------------\n",
      "\n",
      "Batch 16: Processed 11/16 examples\n",
      "Batch 16: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  22%|██▏       | 17/79 [09:07<25:15, 24.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 17: Processed 1/16 examples\n",
      "Batch 17: Processed 11/16 examples\n",
      "Batch 17: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  23%|██▎       | 18/79 [09:29<24:07, 23.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 18: Processed 1/16 examples\n",
      "Batch 18: Processed 11/16 examples\n",
      "Batch 18: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  24%|██▍       | 19/79 [09:51<23:06, 23.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 19: Processed 1/16 examples\n",
      "Batch 19: Processed 11/16 examples\n",
      "Batch 19: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  25%|██▌       | 20/79 [10:10<21:31, 21.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20: Processed 1/16 examples\n",
      "Batch 20: Processed 11/16 examples\n",
      "\n",
      "--- DETAILED DEBUG FOR TASK rt01153 ---\n",
      "Question: Question: Look at the problem below, the solution is missing a part, which option is the most likely...\n",
      "Choices: ['        prefix += 1 << index', '        prefix ^= 1 << index', '        prefix |= 1 << index', '        prefix |= (1 << index) - 1']\n",
      "Predicted: C\n",
      "Reasoning snippet: understanding: |\n",
      "  The problem requires finding the length of the longest substring where each vowel appears an even number of times. The solution involves using bitwise operations to track the parity...\n",
      "-----------------------------------\n",
      "\n",
      "Batch 20: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  27%|██▋       | 21/79 [10:35<22:06, 22.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 21: Processed 1/16 examples\n",
      "Batch 21: Processed 11/16 examples\n",
      "Batch 21: Processed 16/16 examples\n",
      "\n",
      "--- DETAILED DEBUG FOR TASK rt01288 ---\n",
      "Question: Question: Look at the problem below, the solution is missing a part, which option is the most likely...\n",
      "Choices: ['    return self.minimumOneBitOperations(n & (x | x >> 1)) + 1 + x - 1', '    return self.minimumOneBitOperations(n ^ (x | x >> 1)) + 1 + x - 1', '    return self.minimumOneBitOperations(n ^ (x | x << 1)) + 1 + x - 1', '    return self.minimumOneBitOperations(n | (x | x >> 1)) + 1 + x - 1']\n",
      "Predicted: B\n",
      "Reasoning snippet: understanding: |\n",
      "  The question asks for the completion of a recursive function that calculates the minimum number of operations needed to convert an integer `n` to zero using specific bitwise operati...\n",
      "-----------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  28%|██▊       | 22/79 [10:56<21:19, 22.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 22: Processed 1/16 examples\n",
      "Batch 22: Processed 11/16 examples\n",
      "Batch 22: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  29%|██▉       | 23/79 [11:21<21:38, 23.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 23: Processed 1/16 examples\n",
      "Batch 23: Processed 11/16 examples\n",
      "Batch 23: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  30%|███       | 24/79 [11:47<21:55, 23.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 24: Processed 1/16 examples\n",
      "Batch 24: Processed 11/16 examples\n",
      "Batch 24: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  32%|███▏      | 25/79 [12:08<20:46, 23.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 25: Processed 1/16 examples\n",
      "Batch 25: Processed 11/16 examples\n",
      "Batch 25: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  33%|███▎      | 26/79 [12:31<20:26, 23.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 26: Processed 1/16 examples\n",
      "Batch 26: Processed 11/16 examples\n",
      "Batch 26: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  34%|███▍      | 27/79 [12:59<21:10, 24.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 27: Processed 1/16 examples\n",
      "Batch 27: Processed 11/16 examples\n",
      "Batch 27: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  35%|███▌      | 28/79 [13:21<20:08, 23.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 28: Processed 1/16 examples\n",
      "\n",
      "--- DETAILED DEBUG FOR TASK rt02309 ---\n",
      "Question: Question: Look at the problem below, the solution is missing a part, which option is the most likely...\n",
      "Choices: ['    return l if l <= len(changeIndices) else -1', '    return min(l, len(changeIndices)) if l > 0 else -1', '    return l if l <= len(changeIndices) and l > 0 else -1', '    return l if 1 <= l <= len(changeIndices) else -1']\n",
      "Predicted: A\n",
      "Reasoning snippet: understanding: |\n",
      "  The problem involves determining the earliest second when all indices in an array can be marked using specific operations. The solution requires checking if marking can occur up to ...\n",
      "-----------------------------------\n",
      "\n",
      "Batch 28: Processed 11/16 examples\n",
      "Batch 28: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  37%|███▋      | 29/79 [13:41<18:56, 22.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 29: Processed 1/16 examples\n",
      "Batch 29: Processed 11/16 examples\n",
      "Batch 29: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  38%|███▊      | 30/79 [14:07<19:13, 23.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 30: Processed 1/16 examples\n",
      "Batch 30: Processed 11/16 examples\n",
      "Batch 30: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  39%|███▉      | 31/79 [14:29<18:37, 23.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 31: Processed 1/16 examples\n",
      "Batch 31: Processed 11/16 examples\n",
      "Batch 31: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  41%|████      | 32/79 [14:48<17:04, 21.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 32: Processed 1/16 examples\n",
      "Batch 32: Processed 11/16 examples\n",
      "Batch 32: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  42%|████▏     | 33/79 [15:07<16:05, 20.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 33: Processed 1/16 examples\n",
      "Batch 33: Processed 11/16 examples\n",
      "Batch 33: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  43%|████▎     | 34/79 [15:32<16:36, 22.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 34: Processed 1/16 examples\n",
      "Batch 34: Processed 11/16 examples\n",
      "Batch 34: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  44%|████▍     | 35/79 [16:04<18:26, 25.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 35: Processed 1/16 examples\n",
      "Batch 35: Processed 11/16 examples\n",
      "Batch 35: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  46%|████▌     | 36/79 [16:30<18:17, 25.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 36: Processed 1/16 examples\n",
      "Batch 36: Processed 11/16 examples\n",
      "Batch 36: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  47%|████▋     | 37/79 [16:59<18:30, 26.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 37: Processed 1/16 examples\n",
      "Batch 37: Processed 11/16 examples\n",
      "Batch 37: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  48%|████▊     | 38/79 [17:20<17:01, 24.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 38: Processed 1/16 examples\n",
      "Batch 38: Processed 11/16 examples\n",
      "\n",
      "--- DETAILED DEBUG FOR TASK rt06381 ---\n",
      "Question: Question: Given a code snippet below, which behavior most likely to occur when execute it?\n",
      "A,B=map(i...\n",
      "Choices: ['Compile Error', 'Memory Limit Exceeded', 'Runtime Error', 'Internal error']\n",
      "Predicted: A\n",
      "Reasoning snippet: understanding: |\n",
      "  The question asks about the expected behavior when executing a Python code snippet that reads two integers from input, performs arithmetic operations, and prints the maximum result....\n",
      "-----------------------------------\n",
      "\n",
      "Batch 38: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  49%|████▉     | 39/79 [18:11<21:51, 32.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 39: Processed 1/16 examples\n",
      "Batch 39: Processed 11/16 examples\n",
      "Batch 39: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  51%|█████     | 40/79 [18:45<21:28, 33.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 40: Processed 1/16 examples\n",
      "Batch 40: Processed 11/16 examples\n",
      "Batch 40: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  52%|█████▏    | 41/79 [19:14<20:09, 31.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 41: Processed 1/16 examples\n",
      "Batch 41: Processed 11/16 examples\n",
      "Batch 41: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  53%|█████▎    | 42/79 [19:51<20:37, 33.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 42: Processed 1/16 examples\n",
      "Batch 42: Processed 11/16 examples\n",
      "Batch 42: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  54%|█████▍    | 43/79 [20:16<18:30, 30.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 43: Processed 1/16 examples\n",
      "Batch 43: Processed 11/16 examples\n",
      "Batch 43: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  56%|█████▌    | 44/79 [20:46<17:49, 30.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 44: Processed 1/16 examples\n",
      "Batch 44: Processed 11/16 examples\n",
      "Batch 44: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  57%|█████▋    | 45/79 [21:02<14:50, 26.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 45: Processed 1/16 examples\n",
      "Batch 45: Processed 11/16 examples\n",
      "Batch 45: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  58%|█████▊    | 46/79 [21:23<13:30, 24.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 46: Processed 1/16 examples\n",
      "Batch 46: Processed 11/16 examples\n",
      "Batch 46: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  59%|█████▉    | 47/79 [21:44<12:32, 23.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 47: Processed 1/16 examples\n",
      "Batch 47: Processed 11/16 examples\n",
      "Batch 47: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  61%|██████    | 48/79 [22:07<12:03, 23.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 48: Processed 1/16 examples\n",
      "Batch 48: Processed 11/16 examples\n",
      "Batch 48: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  62%|██████▏   | 49/79 [22:30<11:36, 23.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 49: Processed 1/16 examples\n",
      "Batch 49: Processed 11/16 examples\n",
      "\n",
      "--- DETAILED DEBUG FOR TASK k08745 ---\n",
      "Question: Question: Consider a double hashing scheme in which the primary hash function is h1(k) = k mod 23, a...\n",
      "Choices: ['13', '15', '21', '23']\n",
      "Predicted: A\n",
      "Reasoning snippet: understanding: |\n",
      "  The question involves using double hashing to determine the address where a key should be placed in a hash table. The primary hash function uses modular arithmetic, and the secondar...\n",
      "-----------------------------------\n",
      "\n",
      "Batch 49: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  63%|██████▎   | 50/79 [22:49<10:39, 22.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 50: Processed 1/16 examples\n",
      "Batch 50: Processed 11/16 examples\n",
      "Batch 50: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  65%|██████▍   | 51/79 [23:12<10:27, 22.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 51: Processed 1/16 examples\n",
      "Batch 51: Processed 11/16 examples\n",
      "Batch 51: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  66%|██████▌   | 52/79 [23:36<10:14, 22.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 52: Processed 1/16 examples\n",
      "Batch 52: Processed 11/16 examples\n",
      "Batch 52: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  67%|██████▋   | 53/79 [23:55<09:24, 21.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 53: Processed 1/16 examples\n",
      "Batch 53: Processed 11/16 examples\n",
      "Batch 53: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  68%|██████▊   | 54/79 [24:17<09:02, 21.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 54: Processed 1/16 examples\n",
      "Batch 54: Processed 11/16 examples\n",
      "Batch 54: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  70%|██████▉   | 55/79 [24:38<08:34, 21.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 55: Processed 1/16 examples\n",
      "Batch 55: Processed 11/16 examples\n",
      "Batch 55: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  71%|███████   | 56/79 [25:02<08:32, 22.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 56: Processed 1/16 examples\n",
      "Batch 56: Processed 11/16 examples\n",
      "Batch 56: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  72%|███████▏  | 57/79 [25:20<07:43, 21.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 57: Processed 1/16 examples\n",
      "Batch 57: Processed 11/16 examples\n",
      "Batch 57: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  73%|███████▎  | 58/79 [25:43<07:36, 21.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 58: Processed 1/16 examples\n",
      "Batch 58: Processed 11/16 examples\n",
      "Batch 58: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  75%|███████▍  | 59/79 [26:06<07:18, 21.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 59: Processed 1/16 examples\n",
      "Batch 59: Processed 11/16 examples\n",
      "Batch 59: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  76%|███████▌  | 60/79 [26:29<07:05, 22.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 60: Processed 1/16 examples\n",
      "Batch 60: Processed 11/16 examples\n",
      "Batch 60: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  77%|███████▋  | 61/79 [26:50<06:33, 21.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 61: Processed 1/16 examples\n",
      "Batch 61: Processed 11/16 examples\n",
      "Batch 61: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  78%|███████▊  | 62/79 [27:11<06:06, 21.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 62: Processed 1/16 examples\n",
      "Batch 62: Processed 11/16 examples\n",
      "Batch 62: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  80%|███████▉  | 63/79 [27:30<05:33, 20.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 63: Processed 1/16 examples\n",
      "Batch 63: Processed 11/16 examples\n",
      "Batch 63: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  81%|████████  | 64/79 [27:48<05:00, 20.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 64: Processed 1/16 examples\n",
      "Batch 64: Processed 11/16 examples\n",
      "Batch 64: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  82%|████████▏ | 65/79 [28:10<04:50, 20.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 65: Processed 1/16 examples\n",
      "Batch 65: Processed 11/16 examples\n",
      "Batch 65: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  84%|████████▎ | 66/79 [28:32<04:32, 20.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 66: Processed 1/16 examples\n",
      "Batch 66: Processed 11/16 examples\n",
      "Batch 66: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  85%|████████▍ | 67/79 [28:53<04:12, 21.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 67: Processed 1/16 examples\n",
      "\n",
      "--- DETAILED DEBUG FOR TASK k05597 ---\n",
      "Question: Question: The value printed by the following program is \n",
      "\n",
      "\n",
      "void f(int* p, int m)\n",
      "{\n",
      "    m = m + 5;\n",
      "  ...\n",
      "Choices: ['10', '20', '30', '40']\n",
      "Predicted: B\n",
      "Reasoning snippet: understanding: |\n",
      "  The question tests understanding of pointer usage and function parameters in C. It involves modifying variables through pointers and observing the effect on the original variables.\n",
      "...\n",
      "-----------------------------------\n",
      "\n",
      "Batch 67: Processed 11/16 examples\n",
      "Batch 67: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  86%|████████▌ | 68/79 [29:14<03:49, 20.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 68: Processed 1/16 examples\n",
      "Batch 68: Processed 11/16 examples\n",
      "Batch 68: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  87%|████████▋ | 69/79 [29:33<03:25, 20.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 69: Processed 1/16 examples\n",
      "Batch 69: Processed 11/16 examples\n",
      "Batch 69: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  89%|████████▊ | 70/79 [29:50<02:54, 19.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 70: Processed 1/16 examples\n",
      "Batch 70: Processed 11/16 examples\n",
      "Batch 70: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  90%|████████▉ | 71/79 [30:06<02:27, 18.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 71: Processed 1/16 examples\n",
      "Batch 71: Processed 11/16 examples\n",
      "Batch 71: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  91%|█████████ | 72/79 [30:22<02:04, 17.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 72: Processed 1/16 examples\n",
      "Batch 72: Processed 11/16 examples\n",
      "Batch 72: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  92%|█████████▏| 73/79 [30:38<01:42, 17.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 73: Processed 1/16 examples\n",
      "Batch 73: Processed 11/16 examples\n",
      "Batch 73: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  94%|█████████▎| 74/79 [30:55<01:25, 17.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 74: Processed 1/16 examples\n",
      "Batch 74: Processed 11/16 examples\n",
      "Batch 74: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  95%|█████████▍| 75/79 [31:14<01:10, 17.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 75: Processed 1/16 examples\n",
      "Batch 75: Processed 11/16 examples\n",
      "Batch 75: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  96%|█████████▌| 76/79 [31:30<00:51, 17.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 76: Processed 1/16 examples\n",
      "\n",
      "--- DETAILED DEBUG FOR TASK k00476 ---\n",
      "Question: Question: What is the purpose of the np.clip() function in NumPy?...\n",
      "Choices: ['Clips the array values to be within a specified range', 'Clips the array to its maximum value', 'Clips the array to its minimum value', 'Clips the array based on a boolean condition']\n",
      "Predicted: A\n",
      "Reasoning snippet: understanding: |\n",
      "  The question asks about the functionality of the np.clip() function in the NumPy library, which is used for clipping array elements.\n",
      "analysis: |\n",
      "  A. Correct. np.clip() clips array ...\n",
      "-----------------------------------\n",
      "\n",
      "Batch 76: Processed 11/16 examples\n",
      "Batch 76: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  97%|█████████▋| 77/79 [31:46<00:33, 16.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 77: Processed 1/16 examples\n",
      "Batch 77: Processed 11/16 examples\n",
      "Batch 77: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  99%|█████████▊| 78/79 [32:03<00:16, 16.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 78: Processed 1/16 examples\n",
      "Batch 78: Processed 11/16 examples\n",
      "Batch 78: Processed 16/16 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 79/79 [32:19<00:00, 24.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 79: Processed 1/5 examples\n",
      "Batch 79: Processed 5/5 examples\n",
      "Submission saved to ./outputs/inference_batch/20250404_191836/submission_20250404_191836.csv\n",
      "Debug results saved to ./outputs/inference_batch/20250404_191836/debug_results_20250404_191836.csv\n",
      "All completions saved to ./outputs/inference_batch/20250404_191836/all_completions_20250404_191836.csv and ./outputs/inference_batch/20250404_191836/all_completions_20250404_191836.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dict_results = generate_answer_csv_with_batch_inference(\n",
    "    test_data,\n",
    "    lastest_model_handler,\n",
    "    prompt_creator,\n",
    "    latest_tester\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump dict_results to a json file\n",
    "try:\n",
    "    import json\n",
    "    with open(\"dict_results.json\", \"w\") as f:\n",
    "        json.dump(dict_results, f, indent=2)\n",
    "except:\n",
    "    # write text to dict_results.txt\n",
    "    with open(\"dict_results.txt\", \"w\") as f:\n",
    "        f.write(str(dict_results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display the results as yaml format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Function to format the completion as HTML with proper formatting\n",
    "def format_completion_as_html(completion):\n",
    "    # Extract components from the completion\n",
    "    understanding = completion.get(\"understanding\", \"\")\n",
    "    analysis = completion.get(\"analysis\", \"\")\n",
    "    reasoning = completion.get(\"reasoning\", \"\")\n",
    "    conclusion = completion.get(\"conclusion\", \"\")\n",
    "    answer = completion.get(\"answer\", \"\")\n",
    "    \n",
    "    # Create HTML with proper formatting and styling\n",
    "    html = f\"\"\"\n",
    "    <div style=\"border: 1px solid #ddd; padding: 15px; border-radius: 5px; margin-bottom: 20px;\">\n",
    "        <h3 style=\"color: #2c3e50; border-bottom: 1px solid #eee; padding-bottom: 10px;\">Completion Analysis</h3>\n",
    "        \n",
    "        <div style=\"margin-top: 10px;\">\n",
    "            <h4 style=\"color: #3498db;\">Understanding</h4>\n",
    "            <p style=\"white-space: pre-wrap;\">{understanding}</p>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"margin-top: 10px;\">\n",
    "            <h4 style=\"color: #3498db;\">Analysis</h4>\n",
    "            <p style=\"white-space: pre-wrap;\">{analysis}</p>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"margin-top: 10px;\">\n",
    "            <h4 style=\"color: #3498db;\">Reasoning</h4>\n",
    "            <p style=\"white-space: pre-wrap;\">{reasoning}</p>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"margin-top: 10px;\">\n",
    "            <h4 style=\"color: #3498db;\">Conclusion</h4>\n",
    "            <p style=\"white-space: pre-wrap;\">{conclusion}</p>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"margin-top: 10px; font-weight: bold;\">\n",
    "            <h4 style=\"color: #e74c3c;\">Answer</h4>\n",
    "            <p>{answer}</p>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    return html\n",
    "\n",
    "# Display a sample of the results\n",
    "def display_random_example(dict_results):\n",
    "    \"\"\"\n",
    "    Display a random example from the completions with formatted output.\n",
    "    \n",
    "    Args:\n",
    "        dict_results (dict): Dictionary containing all completion results\n",
    "    \"\"\"\n",
    "    if dict_results[\"all_completions\"]:\n",
    "        # Get a random completion instead of always the first one\n",
    "        import random\n",
    "        sample_completion = random.choice(dict_results[\"all_completions\"])\n",
    "        \n",
    "        print(\"Sample Task ID:\", sample_completion[\"task_id\"])\n",
    "        print(\"Sample Question:\", sample_completion[\"question\"][:400] + \"...\" if len(sample_completion[\"question\"]) > 400 else sample_completion[\"question\"])\n",
    "        print(\"Sample Choices:\", sample_completion[\"choices\"])\n",
    "        print(\"Predicted Answer:\", sample_completion[\"predicted_answer\"])\n",
    "        \n",
    "        # Display the completion in YAML format for better readability\n",
    "        print(\"\\nCompletion in YAML format:\")\n",
    "        print(yaml.dump(sample_completion[\"completion\"], default_flow_style=False, sort_keys=False))\n",
    "        \n",
    "        # Display the completion in a nicely formatted HTML\n",
    "        print(\"\\nFormatted Completion:\")\n",
    "        # Parse the completion string into a dictionary if it's a string\n",
    "        completion_dict = {}\n",
    "        if isinstance(sample_completion[\"completion\"], str):\n",
    "            # Extract sections from the completion string\n",
    "            completion_text = sample_completion[\"completion\"]\n",
    "            \n",
    "            sections = [\"understanding\", \"analysis\", \"reasoning\", \"conclusion\", \"answer\"]\n",
    "            current_section = None\n",
    "            section_content = {}\n",
    "            \n",
    "            for line in completion_text.split('\\n'):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                    \n",
    "                for section in sections:\n",
    "                    if line.lower().startswith(f\"{section}:\") or line.lower() == f\"{section}: |\":\n",
    "                        current_section = section\n",
    "                        section_content[current_section] = \"\"\n",
    "                        break\n",
    "                        \n",
    "                if current_section and not any(line.lower().startswith(f\"{s}:\") for s in sections):\n",
    "                    if line.startswith('|'):\n",
    "                        line = line[1:].strip()\n",
    "                    section_content[current_section] += line + \"\\n\"\n",
    "            \n",
    "            # Clean up the extracted content\n",
    "            completion_dict = {k: v.strip() for k, v in section_content.items()}\n",
    "        else:\n",
    "            completion_dict = sample_completion[\"completion\"]\n",
    "            \n",
    "        display(HTML(format_completion_as_html(completion_dict)))\n",
    "        \n",
    "        # Display some statistics\n",
    "        print(f\"\\nTotal completions: {len(dict_results['all_completions'])}\")\n",
    "    else:\n",
    "        print(\"No completions available in the results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Task ID: k02198\n",
      "Sample Question: Question: What will be the output of the following C++ code?\n",
      "#include <iostream> \n",
      "#include <string>\n",
      "#include <cstring>\n",
      "using namespace std; \n",
      "int main(int argc, char const *argv[])\n",
      "{\n",
      "\tstring s(\"a\");\n",
      "\tcout<<s;\n",
      "\treturn 0;\n",
      "}\n",
      "Sample Choices: ['a', 'empty string', 'Error', 'Segmentation fault']\n",
      "Predicted Answer: C\n",
      "\n",
      "Completion in YAML format:\n",
      "\"understanding: |\\n  The question tests knowledge of how C++ handles string objects\\\n",
      "  \\ and their default behavior when initialized without arguments.\\nanalysis: |\\n\\\n",
      "  \\  A. \\\"a\\\" - This would be correct if the string object was properly constructed\\\n",
      "  \\ with an initial value. However, the code does not initialize it correctly.\\n \\\n",
      "  \\ B. empty string - This is incorrect because the code attempts to print the string\\\n",
      "  \\ object `s`, which is not empty.\\n  C. Error - This is correct because the code\\\n",
      "  \\ uses `#include <cstring>` instead of `<cstring>`, leading to a compilation error\\\n",
      "  \\ due to undefined behavior.\\n  D. Segmentation fault - This is incorrect because\\\n",
      "  \\ a segmentation fault occurs due to invalid memory access, which is not applicable\\\n",
      "  \\ here since the issue is at compile-time.\\nreasoning: |\\n  The code snippet includes\\\n",
      "  \\ an incorrect inclusion of `<cstring>` instead of `<cstring>`. This results in\\\n",
      "  \\ a compilation error because the compiler cannot resolve the standard library functions\\\n",
      "  \\ from `<cstring>`. This mistake prevents the program from compiling successfully,\\\n",
      "  \\ making option C the correct choice.\\nconclusion: |\\n  The code contains a syntax\\\n",
      "  \\ error due to incorrect inclusion of `<cstring>`, resulting in a compilation error.\\\n",
      "  \\ Therefore, the correct answer is C.\\nanswer: C\"\n",
      "\n",
      "\n",
      "Formatted Completion:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"border: 1px solid #ddd; padding: 15px; border-radius: 5px; margin-bottom: 20px;\">\n",
       "        <h3 style=\"color: #2c3e50; border-bottom: 1px solid #eee; padding-bottom: 10px;\">Completion Analysis</h3>\n",
       "        \n",
       "        <div style=\"margin-top: 10px;\">\n",
       "            <h4 style=\"color: #3498db;\">Understanding</h4>\n",
       "            <p style=\"white-space: pre-wrap;\">The question tests knowledge of how C++ handles string objects and their default behavior when initialized without arguments.</p>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-top: 10px;\">\n",
       "            <h4 style=\"color: #3498db;\">Analysis</h4>\n",
       "            <p style=\"white-space: pre-wrap;\">A. \"a\" - This would be correct if the string object was properly constructed with an initial value. However, the code does not initialize it correctly.\n",
       "B. empty string - This is incorrect because the code attempts to print the string object `s`, which is not empty.\n",
       "C. Error - This is correct because the code uses `#include <cstring>` instead of `<cstring>`, leading to a compilation error due to undefined behavior.\n",
       "D. Segmentation fault - This is incorrect because a segmentation fault occurs due to invalid memory access, which is not applicable here since the issue is at compile-time.</p>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-top: 10px;\">\n",
       "            <h4 style=\"color: #3498db;\">Reasoning</h4>\n",
       "            <p style=\"white-space: pre-wrap;\">The code snippet includes an incorrect inclusion of `<cstring>` instead of `<cstring>`. This results in a compilation error because the compiler cannot resolve the standard library functions from `<cstring>`. This mistake prevents the program from compiling successfully, making option C the correct choice.</p>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-top: 10px;\">\n",
       "            <h4 style=\"color: #3498db;\">Conclusion</h4>\n",
       "            <p style=\"white-space: pre-wrap;\">The code contains a syntax error due to incorrect inclusion of `<cstring>`, resulting in a compilation error. Therefore, the correct answer is C.</p>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-top: 10px; font-weight: bold;\">\n",
       "            <h4 style=\"color: #e74c3c;\">Answer</h4>\n",
       "            <p></p>\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total completions: 1253\n"
     ]
    }
   ],
   "source": [
    "# Call the function to display a random example,\n",
    "# Each time you run the cell, it will display a different example\n",
    "display_random_example(dict_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset for upload...\n",
      "Dataset prepared with 1253 entries\n",
      "Already logged in to Hugging Face\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a358782f22d4ea5818b42cb270100c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e74f068f065648d48f68a69e951eb0c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully pushed to tuandunghcmut/coding-mcq-reasoning_evaluated_on_test_set\n"
     ]
    }
   ],
   "source": [
    "# Post-processing and pushing the test dataset with completions to Hugging Face\n",
    "\n",
    "# Create a dataset from the completions\n",
    "def prepare_dataset_for_upload():\n",
    "    # Extract the necessary data from all_completions\n",
    "    upload_data = []\n",
    "    \n",
    "    for item in dict_results[\"all_completions\"]:\n",
    "        entry = {\n",
    "            \"task_id\": item[\"task_id\"],\n",
    "            \"question\": item[\"question\"],\n",
    "            \"choices\": item[\"choices\"],\n",
    "            \"predicted_answer\": item[\"predicted_answer\"],\n",
    "            \"completion\": item[\"completion\"],\n",
    "            \"reasoning\": item.get(\"reasoning\", \"\"),\n",
    "            \"understanding\": item.get(\"understanding\", \"\"),\n",
    "            \"analysis\": item.get(\"analysis\", \"\"),\n",
    "            \"conclusion\": item.get(\"conclusion\", \"\")\n",
    "        }\n",
    "        upload_data.append(entry)\n",
    "    \n",
    "    # Convert to Hugging Face dataset\n",
    "    dataset = Dataset.from_list(upload_data)\n",
    "    return dataset\n",
    "\n",
    "# Function to push the dataset to Hugging Face\n",
    "def push_to_huggingface(dataset, repo_id=\"tuandunghcmut/coding-mcq-reasoning_evaluated_on_test_set\"):\n",
    "    # Ask for Hugging Face token if not already logged in\n",
    "    try:\n",
    "        # Check if already logged in\n",
    "        api = HfApi()\n",
    "        api.whoami()\n",
    "        print(\"Already logged in to Hugging Face\")\n",
    "    except Exception:\n",
    "        # If not logged in, ask for token\n",
    "        print(\"Please enter your Hugging Face token:\")\n",
    "        token = getpass()\n",
    "        login(token=token)\n",
    "    \n",
    "    # Push the dataset to Hugging Face\n",
    "    dataset.push_to_hub(\n",
    "        repo_id,\n",
    "        commit_message=\"Add model evaluation results on test set\",\n",
    "        private=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset successfully pushed to {repo_id}\")\n",
    "\n",
    "# Execute the functions\n",
    "print(\"Preparing dataset for upload...\")\n",
    "dataset = prepare_dataset_for_upload()\n",
    "print(f\"Dataset prepared with {len(dataset)} entries\")\n",
    "push_to_huggingface(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
