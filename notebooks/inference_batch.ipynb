{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone the repository if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please set this to True if you want to clone the repository\n",
    "SHOULD_CLONE = False\n",
    "project_name = \"Small-Qwen-Coding-Multiple-Choice\"\n",
    "\n",
    "import os, subprocess\n",
    "\n",
    "if SHOULD_CLONE:\n",
    "    subprocess.run(\n",
    "        [\"git\", \"clone\", \"https://github.com/tuandung222/Small-Qwen-Coding-Multiple-Choice.git\"],\n",
    "        check=True,\n",
    "    )\n",
    "    os.chdir(project_name)  # Change directory to the cloned project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If running this notebook in the notebooks folder, go outside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Small-Qwen-Coding-Multiple-Choice\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if in the notebooks folder and go outside\n",
    "if os.getcwd().endswith(\"notebooks\"):\n",
    "    os.chdir(\"..\")  # Change directory to the parent folder\n",
    "\n",
    "print(os.getcwd())  # Print the current working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datasets\n",
    "from pprint import pprint\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login, HfApi\n",
    "import os\n",
    "from getpass import getpass\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "from peft import PeftModel\n",
    "import unsloth, transformers\n",
    "from unsloth.models import FastLanguageModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary modules from the src folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.qwen_handler import QwenModelHandler\n",
    "from src.testing.tester import MultipleChoiceTester\n",
    "from src.prompt_processors.prompt_creator import PromptCreator\n",
    "from src.testing.tester import MultipleChoiceTester"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['task_id', 'question', 'choices'],\n",
      "    num_rows: 1253\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "test_path = os.path.join(\"src\", \"data\", \"b6_test_data.csv\")\n",
    "if not os.path.exists(test_path):\n",
    "    raise FileNotFoundError(f\"Data file not found at: {test_path}\")\n",
    "\n",
    "test_data = datasets.load_dataset(\"csv\", data_files=test_path)[\"train\"]\n",
    "\n",
    "# pprint(test_data[0])\n",
    "# pprint(test_data.features)\n",
    "# pprint(test_data.info)\n",
    "pprint(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the lastest model checkpoint from HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from HuggingFace Hub: tuandunghcmut/Qwen25_Coder_MultipleChoice_v4\n",
      "2025-04-04 18:14:08 - src.model.qwen_handler - INFO - Loading tuandunghcmut/Qwen25_Coder_MultipleChoice_v4 from unsloth, max_seq_length=2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:Loading tuandunghcmut/Qwen25_Coder_MultipleChoice_v4 from unsloth, max_seq_length=2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 18:14:08 - src.model.qwen_handler - INFO - Flash Attention 2 is available (package flash-attn detected)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:Flash Attention 2 is available (package flash-attn detected)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 18:14:08 - src.model.qwen_handler - INFO - Flash Attention 2 version: 2.7.4.post1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:Flash Attention 2 version: 2.7.4.post1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 18:14:08 - src.model.qwen_handler - INFO - xFormers is available (version: 0.0.29.post3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:xFormers is available (version: 0.0.29.post3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 18:14:08 - src.model.qwen_handler - INFO - CUDA is available (version: 12.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:CUDA is available (version: 12.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 18:14:08 - src.model.qwen_handler - INFO - Using attention implementation: flash_attention_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:Using attention implementation: flash_attention_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 18:14:08 - src.model.qwen_handler - INFO - Setting max memory: {0: '27620MiB'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:Setting max memory: {0: '27620MiB'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 18:14:08 - src.model.qwen_handler - INFO - Using attention implementation: flash_attention_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:Using attention implementation: flash_attention_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.50.3.\n",
      "   \\\\   /|    Tesla V100-SXM2-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.19 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 18:14:19 - src.model.qwen_handler - INFO - Model loaded successfully: tuandunghcmut/Qwen25_Coder_MultipleChoice_v4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:Model loaded successfully: tuandunghcmut/Qwen25_Coder_MultipleChoice_v4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 18:14:19 - src.model.qwen_handler - INFO - Model type: qwen2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:Model type: qwen2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 18:14:19 - src.model.qwen_handler - INFO - hidden_size: 1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:hidden_size: 1536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 18:14:19 - src.model.qwen_handler - INFO - intermediate_size: 8960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:intermediate_size: 8960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 18:14:19 - src.model.qwen_handler - INFO - num_hidden_layers: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:num_hidden_layers: 28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 18:14:19 - src.model.qwen_handler - INFO - num_attention_heads: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:num_attention_heads: 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 18:14:19 - src.model.qwen_handler - INFO - torch_dtype: float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.model.qwen_handler:torch_dtype: float16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model from HuggingFace Hub!\n"
     ]
    }
   ],
   "source": [
    "# Set HuggingFace Hub credentials if available\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "# Model ID on HuggingFace Hub\n",
    "hub_model_id = \"tuandunghcmut/Qwen25_Coder_MultipleChoice_v4\"\n",
    "\n",
    "print(f\"Loading model from HuggingFace Hub: {hub_model_id}\")\n",
    "\n",
    "\n",
    "lastest_model_handler = QwenModelHandler(\n",
    "    model_name=hub_model_id,\n",
    "    max_seq_length=2048,\n",
    "    quantization=\"4bit\",\n",
    "    model_source=\"unsloth\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "# Use FastLanguageModel\n",
    "\n",
    "FastLanguageModel.for_inference(lastest_model_handler.model)\n",
    "prompt_creator = PromptCreator(PromptCreator.YAML_REASONING)\n",
    "# Create a tester with the loaded model\n",
    "latest_tester = MultipleChoiceTester(lastest_model_handler, prompt_creator=prompt_creator)\n",
    "\n",
    "print(\"Successfully loaded model from HuggingFace Hub!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left\n",
      "151645\n",
      "151665\n"
     ]
    }
   ],
   "source": [
    "print(lastest_model_handler.tokenizer.padding_side)\n",
    "print(lastest_model_handler.tokenizer.eos_token_id)\n",
    "print(lastest_model_handler.tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to perform batch inference, generate answers for multiple-choice questions and debug results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_csv_with_batch_inference(\n",
    "    test_dataset,\n",
    "    model_handler,\n",
    "    prompt_creator,\n",
    "    tester,\n",
    "    output_folder=\"./outputs/inference_batch\",\n",
    "    batch_size=64,\n",
    "    temperature=0.001,\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    top_k=90,\n",
    "    debug_samples=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate answers for multiple-choice questions in batch mode.\n",
    "\n",
    "    Args:\n",
    "        test_dataset: Dataset containing test examples\n",
    "        model_handler: Model handler instance\n",
    "        prompt_creator: Prompt creator instance\n",
    "        tester: Tester instance\n",
    "        output_folder: Folder to save all output files\n",
    "        batch_size: Number of examples to process at once\n",
    "        temperature: Sampling temperature for generation\n",
    "        max_new_tokens: Maximum number of tokens to generate\n",
    "        do_sample: Whether to use sampling for generation\n",
    "        top_p: Top-p sampling parameter\n",
    "        top_k: Top-k sampling parameter\n",
    "        debug_samples: Number of random samples to debug\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import re\n",
    "    import random\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    from tqdm import tqdm\n",
    "    from datetime import datetime\n",
    "\n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Create timestamp for the run\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Create a dictionary to store results\n",
    "    results_dict = {}\n",
    "    \n",
    "    # Create a list to store all detailed results for each example\n",
    "    all_completions = []\n",
    "\n",
    "    # Create a list to store detailed results for debugging\n",
    "    debug_results = []\n",
    "\n",
    "    # Select a few random examples for detailed debugging\n",
    "    all_examples = list(test_dataset)\n",
    "    num_examples = len(all_examples)\n",
    "    debug_indices = random.sample(range(num_examples), min(debug_samples, num_examples))\n",
    "\n",
    "    # Process examples in batches\n",
    "    for batch_start in tqdm(range(0, num_examples, batch_size), desc=\"Processing batches\"):\n",
    "        # Get the current batch\n",
    "        batch_end = min(batch_start + batch_size, num_examples)\n",
    "        batch = all_examples[batch_start:batch_end]\n",
    "\n",
    "        # Prepare batch inputs\n",
    "        batch_inputs = []\n",
    "        batch_task_ids = []\n",
    "        batch_prompts = []\n",
    "        batch_examples = []\n",
    "\n",
    "        for example in batch:\n",
    "            # Create prompt for each example\n",
    "            prompt = prompt_creator.create_inference_prompt(\n",
    "                example[\"question\"], eval(example[\"choices\"])\n",
    "            )\n",
    "\n",
    "            # Format as chat\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            chat_text = model_handler.tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            batch_inputs.append(chat_text)\n",
    "            batch_task_ids.append(example[\"task_id\"])\n",
    "            batch_prompts.append(prompt)\n",
    "            batch_examples.append(example)\n",
    "\n",
    "        # Tokenize all inputs at once\n",
    "        model_handler.tokenizer.padding_side = \"left\"\n",
    "        tokenized_inputs = model_handler.tokenizer(\n",
    "            batch_inputs,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=model_handler.max_seq_length,\n",
    "        ).to(model_handler.model.device)\n",
    "\n",
    "        # Perform batch inference using the model directly\n",
    "        with torch.inference_mode():\n",
    "            generated_ids = model_handler.model.generate(\n",
    "                input_ids=tokenized_inputs.input_ids,\n",
    "                attention_mask=tokenized_inputs.attention_mask,\n",
    "                temperature=temperature,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=do_sample,\n",
    "                top_p=top_p,\n",
    "                top_k=top_k,\n",
    "                use_cache=True,\n",
    "                pad_token_id=model_handler.tokenizer.pad_token_id,\n",
    "            )\n",
    "\n",
    "            # Extract only the generated part (not the input)\n",
    "            batch_outputs = []\n",
    "            for i, gen_ids in enumerate(generated_ids):\n",
    "                # Get the length of the input\n",
    "                input_length = tokenized_inputs.input_ids[i].shape[0]\n",
    "                # Decode only the generated part\n",
    "                output_text = model_handler.tokenizer.decode(\n",
    "                    gen_ids[input_length:], skip_special_tokens=True\n",
    "                )\n",
    "                batch_outputs.append(output_text)\n",
    "\n",
    "        # Process batch results and write to batch file\n",
    "        batch_file = os.path.join(output_folder, f\"batch_{batch_start//batch_size + 1}_{timestamp}.txt\")\n",
    "        batch_results = []\n",
    "        \n",
    "        with open(batch_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for i, (task_id, output, example) in enumerate(zip(batch_task_ids, batch_outputs, batch_examples)):\n",
    "                # Extract predicted answer from model output\n",
    "                result = {}\n",
    "                \n",
    "                # Simple regex to find the answer (A, B, C, or D)\n",
    "                answer_match = re.search(r\"answer\\s*(?:is|:)?\\s*([ABCD])\", output, re.IGNORECASE)\n",
    "                if answer_match:\n",
    "                    result[\"predicted_answer\"] = answer_match.group(1).upper()\n",
    "                else:\n",
    "                    # Fallback: look for the first occurrence of A, B, C, or D\n",
    "                    for letter in [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\"]:\n",
    "                        if (\n",
    "                            f\"({letter})\" in output\n",
    "                            or f\"{letter}.\" in output\n",
    "                            or f\"answer {letter}\" in output.lower()\n",
    "                        ):\n",
    "                            result[\"predicted_answer\"] = letter\n",
    "                            break\n",
    "                    else:\n",
    "                        # If no answer found, default to A\n",
    "                        result[\"predicted_answer\"] = \"A\"\n",
    "\n",
    "                # Extract reasoning if available\n",
    "                result[\"reasoning\"] = output\n",
    "                result[\"task_id\"] = task_id\n",
    "                \n",
    "                # Store all original example data along with completions\n",
    "                completion_record = {\n",
    "                    \"task_id\": task_id,\n",
    "                    \"question\": example[\"question\"],\n",
    "                    \"choices\": example[\"choices\"],\n",
    "                    \"predicted_answer\": result[\"predicted_answer\"],\n",
    "                    \"completion\": output,\n",
    "                }\n",
    "                if \"answer\" in example:\n",
    "                    completion_record[\"ground_truth\"] = example[\"answer\"]\n",
    "                \n",
    "                all_completions.append(completion_record)\n",
    "                \n",
    "                # Write to batch file\n",
    "                f.write(f\"Task ID: {task_id}\\n\")\n",
    "                f.write(f\"Question: {example['question']}\\n\")\n",
    "                f.write(f\"Choices: {example['choices']}\\n\")\n",
    "                f.write(f\"Predicted Answer: {result['predicted_answer']}\\n\")\n",
    "                f.write(f\"Completion: {output}\\n\")\n",
    "                if \"answer\" in example:\n",
    "                    f.write(f\"Ground Truth: {example['answer']}\\n\")\n",
    "                f.write(\"\\n\" + \"-\"*80 + \"\\n\\n\")  # Separator\n",
    "                \n",
    "                # Store the result for submission\n",
    "                results_dict[task_id] = result[\"predicted_answer\"]\n",
    "                batch_results.append(result)\n",
    "\n",
    "                # Print progress update for every 10th example or last in batch\n",
    "                if i % 10 == 0 or i == len(batch) - 1:\n",
    "                    print(f\"Batch {batch_start//batch_size + 1}: Processed {i+1}/{len(batch)} examples\")\n",
    "\n",
    "                # For selected examples, save detailed results for debugging\n",
    "                global_index = batch_start + i\n",
    "                if global_index in debug_indices:\n",
    "                    example = batch[i]\n",
    "                    debug_results.append(\n",
    "                        {\n",
    "                            \"task_id\": task_id,\n",
    "                            \"question\": example[\"question\"],\n",
    "                            \"choices\": example[\"choices\"],\n",
    "                            \"predicted_answer\": result[\"predicted_answer\"],\n",
    "                            \"reasoning\": result.get(\"reasoning\", \"No reasoning provided\"),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    # Print detailed debug information for these examples\n",
    "                    print(f\"\\n--- DETAILED DEBUG FOR TASK {task_id} ---\")\n",
    "                    print(f\"Question: {example['question'][:100]}...\")\n",
    "                    print(f\"Choices: {example['choices']}\")\n",
    "                    print(f\"Predicted: {result['predicted_answer']}\")\n",
    "                    print(f\"Reasoning snippet: {str(result.get('reasoning', 'No reasoning'))[:200]}...\")\n",
    "                    print(\"-----------------------------------\\n\")\n",
    "\n",
    "    # Create a DataFrame from the results dictionary for submission\n",
    "    submission_df = pd.DataFrame(list(results_dict.items()), columns=[\"task_id\", \"answer\"])\n",
    "\n",
    "    # Create a DataFrame with all completions\n",
    "    all_completions_df = pd.DataFrame(all_completions)\n",
    "\n",
    "    # Save all the files to the output folder\n",
    "    submission_file = os.path.join(output_folder, f\"submission_{timestamp}.csv\")\n",
    "    debug_file = os.path.join(output_folder, f\"debug_results_{timestamp}.csv\")\n",
    "    all_completions_file = os.path.join(output_folder, f\"all_completions_{timestamp}.csv\")\n",
    "    all_completions_json = os.path.join(output_folder, f\"all_completions_{timestamp}.json\")\n",
    "\n",
    "    # Save files\n",
    "    submission_df.to_csv(submission_file, index=False)\n",
    "    pd.DataFrame(debug_results).to_csv(debug_file, index=False)\n",
    "    all_completions_df.to_csv(all_completions_file, index=False)\n",
    "    \n",
    "    # Save as JSON for better inspection of large text fields\n",
    "    import json\n",
    "    with open(all_completions_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_completions, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Submission saved to {submission_file}\")\n",
    "    print(f\"Debug results saved to {debug_file}\")\n",
    "    print(f\"All completions saved to {all_completions_file} and {all_completions_json}\")\n",
    "    \n",
    "    return {\n",
    "        \"submission_file\": submission_file,\n",
    "        \"debug_file\": debug_file, \n",
    "        \"all_completions_file\": all_completions_file,\n",
    "        \"all_completions_json\": all_completions_json,\n",
    "        \"results\": results_dict,\n",
    "        \"debug_results\": debug_results,\n",
    "        \"all_completions\": all_completions\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on the test dataset and save the results to a CSV file, then push the results to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|▌         | 1/20 [01:07<21:13, 67.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: Processed 1/64 examples\n",
      "Batch 1: Processed 11/64 examples\n",
      "\n",
      "--- DETAILED DEBUG FOR TASK k10254 ---\n",
      "Question: Question: What is the output of this program?\n",
      "   #! /usr/bin/awk -f   BEGIN {       a=5       while ...\n",
      "Choices: ['nothing will print', '“sanfoundry” will print 5 times', 'program will generate syntax error', 'none of the mentioned']\n",
      "Predicted: C\n",
      "Reasoning snippet: understanding: |\n",
      "  The question asks for the output of an AWK script that prints \"hello_world\" repeatedly until a condition is met. Key concepts include understanding AWK's control structures and synt...\n",
      "-----------------------------------\n",
      "\n",
      "Batch 1: Processed 21/64 examples\n",
      "Batch 1: Processed 31/64 examples\n",
      "Batch 1: Processed 41/64 examples\n",
      "Batch 1: Processed 51/64 examples\n",
      "Batch 1: Processed 61/64 examples\n",
      "Batch 1: Processed 64/64 examples\n"
     ]
    }
   ],
   "source": [
    "dict_results = generate_answer_csv_with_batch_inference(\n",
    "    test_data,\n",
    "    lastest_model_handler,\n",
    "    prompt_creator,\n",
    "    latest_tester\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing and pushing the test dataset with completions to Hugging Face\n",
    "\n",
    "# Create a dataset from the completions\n",
    "def prepare_dataset_for_upload():\n",
    "    # Extract the necessary data from all_completions\n",
    "    upload_data = []\n",
    "    \n",
    "    for item in dict_results[\"all_completions\"]:\n",
    "        entry = {\n",
    "            \"task_id\": item[\"task_id\"],\n",
    "            \"question\": item[\"question\"],\n",
    "            \"choices\": item[\"choices\"],\n",
    "            \"predicted_answer\": item[\"predicted_answer\"],\n",
    "            \"completion\": item[\"completion\"],\n",
    "            \"reasoning\": item.get(\"reasoning\", \"\"),\n",
    "            \"understanding\": item.get(\"understanding\", \"\"),\n",
    "            \"analysis\": item.get(\"analysis\", \"\"),\n",
    "            \"conclusion\": item.get(\"conclusion\", \"\")\n",
    "        }\n",
    "        upload_data.append(entry)\n",
    "    \n",
    "    # Convert to Hugging Face dataset\n",
    "    dataset = Dataset.from_list(upload_data)\n",
    "    return dataset\n",
    "\n",
    "# Function to push the dataset to Hugging Face\n",
    "def push_to_huggingface(dataset, repo_id=\"tuandunghcmut/coding-mcq-reasoning_evaluated_on_test_set\"):\n",
    "    # Ask for Hugging Face token if not already logged in\n",
    "    try:\n",
    "        # Check if already logged in\n",
    "        api = HfApi()\n",
    "        api.whoami()\n",
    "        print(\"Already logged in to Hugging Face\")\n",
    "    except Exception:\n",
    "        # If not logged in, ask for token\n",
    "        print(\"Please enter your Hugging Face token:\")\n",
    "        token = getpass()\n",
    "        login(token=token)\n",
    "    \n",
    "    # Push the dataset to Hugging Face\n",
    "    dataset.push_to_hub(\n",
    "        repo_id,\n",
    "        commit_message=\"Add model evaluation results on test set\",\n",
    "        private=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset successfully pushed to {repo_id}\")\n",
    "\n",
    "# Execute the functions\n",
    "print(\"Preparing dataset for upload...\")\n",
    "dataset = prepare_dataset_for_upload()\n",
    "print(f\"Dataset prepared with {len(dataset)} entries\")\n",
    "\n",
    "# Ask for confirmation before pushing\n",
    "push_confirmation = input(\"Do you want to push this dataset to Hugging Face? (yes/no): \")\n",
    "if push_confirmation.lower() in [\"yes\", \"y\"]:\n",
    "    push_to_huggingface(dataset)\n",
    "else:\n",
    "    print(\"Dataset push cancelled\")\n",
    "\n",
    "# Display a sample of the dataset\n",
    "print(\"\\nSample of the dataset that would be pushed:\")\n",
    "print(dataset[:2])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
