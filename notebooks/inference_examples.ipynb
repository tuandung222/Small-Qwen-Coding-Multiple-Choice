{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install core dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This notebook can be run everywhere without clone the repo\n",
    "* Commented out anything related to flash-att and HF_TOKEN if you have some trouble with them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /venv/main/lib/python3.10/site-packages (4.50.3)\n",
      "Requirement already satisfied: torch in /venv/main/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: pandas in /venv/main/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /venv/main/lib/python3.10/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.10/site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /venv/main/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /venv/main/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /venv/main/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /venv/main/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /venv/main/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /venv/main/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.10/site-packages (from torch) (4.13.0)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /venv/main/lib/python3.10/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /venv/main/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /venv/main/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /venv/main/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /venv/main/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /venv/main/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /venv/main/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /venv/main/lib/python3.10/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /venv/main/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /venv/main/lib/python3.10/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /venv/main/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /venv/main/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /venv/main/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /venv/main/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: unsloth in /venv/main/lib/python3.10/site-packages (2025.3.19)\n",
      "Requirement already satisfied: accelerate in /venv/main/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: bitsandbytes in /venv/main/lib/python3.10/site-packages (0.45.4)\n",
      "Requirement already satisfied: unsloth_zoo>=2025.3.17 in /venv/main/lib/python3.10/site-packages (from unsloth) (2025.3.17)\n",
      "Requirement already satisfied: torch>=2.4.0 in /venv/main/lib/python3.10/site-packages (from unsloth) (2.6.0)\n",
      "Requirement already satisfied: xformers>=0.0.27.post2 in /venv/main/lib/python3.10/site-packages (from unsloth) (0.0.29.post3)\n",
      "Requirement already satisfied: triton>=3.0.0 in /venv/main/lib/python3.10/site-packages (from unsloth) (3.2.0)\n",
      "Requirement already satisfied: packaging in /venv/main/lib/python3.10/site-packages (from unsloth) (24.2)\n",
      "Requirement already satisfied: tyro in /venv/main/lib/python3.10/site-packages (from unsloth) (0.9.18)\n",
      "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /venv/main/lib/python3.10/site-packages (from unsloth) (4.50.3)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /venv/main/lib/python3.10/site-packages (from unsloth) (3.5.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /venv/main/lib/python3.10/site-packages (from unsloth) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /venv/main/lib/python3.10/site-packages (from unsloth) (4.67.1)\n",
      "Requirement already satisfied: psutil in /venv/main/lib/python3.10/site-packages (from unsloth) (7.0.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /venv/main/lib/python3.10/site-packages (from unsloth) (0.45.1)\n",
      "Requirement already satisfied: numpy in /venv/main/lib/python3.10/site-packages (from unsloth) (2.2.4)\n",
      "Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 in /venv/main/lib/python3.10/site-packages (from unsloth) (0.15.2)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /venv/main/lib/python3.10/site-packages (from unsloth) (0.15.1)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /venv/main/lib/python3.10/site-packages (from unsloth) (3.20.3)\n",
      "Requirement already satisfied: huggingface_hub in /venv/main/lib/python3.10/site-packages (from unsloth) (0.29.3)\n",
      "Requirement already satisfied: hf_transfer in /venv/main/lib/python3.10/site-packages (from unsloth) (0.1.9)\n",
      "Requirement already satisfied: diffusers in /venv/main/lib/python3.10/site-packages (from unsloth) (0.32.2)\n",
      "Requirement already satisfied: torchvision in /venv/main/lib/python3.10/site-packages (from unsloth) (0.21.0)\n",
      "Requirement already satisfied: pyyaml in /venv/main/lib/python3.10/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /venv/main/lib/python3.10/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /venv/main/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /venv/main/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth) (0.3.8)\n",
      "Requirement already satisfied: pandas in /venv/main/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /venv/main/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /venv/main/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /venv/main/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /venv/main/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /venv/main/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth) (3.11.16)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.10/site-packages (from huggingface_hub->unsloth) (4.13.0)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /venv/main/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /venv/main/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /venv/main/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /venv/main/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /venv/main/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /venv/main/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /venv/main/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /venv/main/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /venv/main/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /venv/main/lib/python3.10/site-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /venv/main/lib/python3.10/site-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.1)\n",
      "Requirement already satisfied: rich in /venv/main/lib/python3.10/site-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (14.0.0)\n",
      "Requirement already satisfied: cut_cross_entropy in /venv/main/lib/python3.10/site-packages (from unsloth_zoo>=2025.3.17->unsloth) (25.1.1)\n",
      "Requirement already satisfied: pillow in /venv/main/lib/python3.10/site-packages (from unsloth_zoo>=2025.3.17->unsloth) (11.1.0)\n",
      "Requirement already satisfied: importlib-metadata in /venv/main/lib/python3.10/site-packages (from diffusers->unsloth) (8.6.1)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /venv/main/lib/python3.10/site-packages (from tyro->unsloth) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /venv/main/lib/python3.10/site-packages (from tyro->unsloth) (1.7.1)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /venv/main/lib/python3.10/site-packages (from tyro->unsloth) (4.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /venv/main/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /venv/main/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /venv/main/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /venv/main/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /venv/main/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /venv/main/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /venv/main/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /venv/main/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /venv/main/lib/python3.10/site-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /venv/main/lib/python3.10/site-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.19.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /venv/main/lib/python3.10/site-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.10/site-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /venv/main/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /venv/main/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /venv/main/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth) (2025.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /venv/main/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\n",
      "Requirement already satisfied: flash-attn in /venv/main/lib/python3.10/site-packages (2.7.4.post1)\n",
      "Requirement already satisfied: torch in /venv/main/lib/python3.10/site-packages (from flash-attn) (2.6.0)\n",
      "Requirement already satisfied: einops in /venv/main/lib/python3.10/site-packages (from flash-attn) (0.8.1)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from torch->flash-attn) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.10/site-packages (from torch->flash-attn) (4.13.0)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.10/site-packages (from torch->flash-attn) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.10/site-packages (from torch->flash-attn) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /venv/main/lib/python3.10/site-packages (from torch->flash-attn) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch->flash-attn) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch->flash-attn) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch->flash-attn) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /venv/main/lib/python3.10/site-packages (from torch->flash-attn) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /venv/main/lib/python3.10/site-packages (from torch->flash-attn) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /venv/main/lib/python3.10/site-packages (from torch->flash-attn) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /venv/main/lib/python3.10/site-packages (from torch->flash-attn) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /venv/main/lib/python3.10/site-packages (from torch->flash-attn) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /venv/main/lib/python3.10/site-packages (from torch->flash-attn) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /venv/main/lib/python3.10/site-packages (from torch->flash-attn) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /venv/main/lib/python3.10/site-packages (from torch->flash-attn) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch->flash-attn) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch->flash-attn) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /venv/main/lib/python3.10/site-packages (from torch->flash-attn) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /venv/main/lib/python3.10/site-packages (from torch->flash-attn) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.10/site-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (3.0.2)\n",
      "Requirement already satisfied: datasets in /venv/main/lib/python3.10/site-packages (3.5.0)\n",
      "Requirement already satisfied: pyyaml in /venv/main/lib/python3.10/site-packages (6.0.2)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.10/site-packages (from datasets) (2.2.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /venv/main/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /venv/main/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /venv/main/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /venv/main/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /venv/main/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /venv/main/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /venv/main/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /venv/main/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /venv/main/lib/python3.10/site-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /venv/main/lib/python3.10/site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in /venv/main/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /venv/main/lib/python3.10/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /venv/main/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /venv/main/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /venv/main/lib/python3.10/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /venv/main/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /venv/main/lib/python3.10/site-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /venv/main/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /venv/main/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /venv/main/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /venv/main/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /venv/main/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install core dependencies\n",
    "!pip install transformers torch pandas\n",
    "\n",
    "# For faster inference (important)\n",
    "!pip install unsloth accelerate bitsandbytes\n",
    "\n",
    "# Commented out anything related to flash-att and HF_TOKEN if you have some trouble with them!\n",
    "# Flash Attention (highly recommended for speed)\n",
    "!pip install flash-attn --no-build-isolation\n",
    "\n",
    "# For dataset handling and YAML parsing\n",
    "!pip install datasets pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_from_disk\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import Wandb for experiment tracking\n",
    "import wandb\n",
    "\n",
    "# Import Unsloth\n",
    "\n",
    "import unsloth\n",
    "\n",
    "# Import HuggingFace libraries\n",
    "\n",
    "# Try to import HF token from environment\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)\n",
    "\n",
    "# Disable HuggingFace tokenizers parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of PromptCreator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptCreator:\n",
    "    \"\"\"\n",
    "    Creates and formats prompts for multiple choice questions\n",
    "    Supports different prompt styles for training and inference\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt types\n",
    "    BASIC = \"basic\"  # Simple answer-only format\n",
    "    YAML_REASONING = \"yaml\"  # YAML formatted reasoning\n",
    "    TEACHER_REASONED = (\n",
    "        \"teacher\"  # Same YAML format as YAML_REASONING but using teacher completions for training\n",
    "    )\n",
    "\n",
    "    def __init__(self, prompt_type=BASIC):\n",
    "        \"\"\"\n",
    "        Initialize prompt creator with the specified type\n",
    "\n",
    "        Args:\n",
    "            prompt_type: Type of prompts to generate - \"basic\", \"yaml\", or \"teacher\"\n",
    "                         Note: \"teacher\" uses same prompt format as \"yaml\" but with teacher completions\n",
    "        \"\"\"\n",
    "        # For prompt formatting, teacher_reasoned is equivalent to yaml_reasoning\n",
    "        # The difference only matters during training when using teacher completions\n",
    "        if prompt_type == self.TEACHER_REASONED:\n",
    "            prompt_type = self.YAML_REASONING\n",
    "\n",
    "        self.prompt_type = prompt_type\n",
    "        # Store the original prompt type to track if we're using teacher mode\n",
    "        self.original_type = prompt_type\n",
    "\n",
    "    def format_choices(self, choices):\n",
    "        \"\"\"Format choices as a lettered list\"\"\"\n",
    "        return \"\\n\".join([f\"{chr(65 + i)}. {choice}\" for i, choice in enumerate(choices)])\n",
    "\n",
    "    def get_max_letter(self, choices):\n",
    "        \"\"\"Get the maximum letter based on number of choices\"\"\"\n",
    "        return chr(65 + len(choices) - 1)\n",
    "\n",
    "    def create_inference_prompt(self, question, choices):\n",
    "        \"\"\"\n",
    "        Create a prompt for inference based on current prompt type\n",
    "\n",
    "        Args:\n",
    "            question: The question text\n",
    "            choices: List of choices\n",
    "\n",
    "        Returns:\n",
    "            Formatted prompt string\n",
    "        \"\"\"\n",
    "        formatted_choices = self.format_choices(choices)\n",
    "        max_letter = self.get_max_letter(choices)\n",
    "\n",
    "        if self.prompt_type == self.YAML_REASONING:\n",
    "            return self._create_yaml_prompt(question, formatted_choices, max_letter)\n",
    "        else:\n",
    "            return self._create_basic_prompt(question, formatted_choices, max_letter)\n",
    "\n",
    "    def _create_basic_prompt(self, question, formatted_choices, max_letter):\n",
    "        \"\"\"Create a basic prompt asking for just the answer letter\"\"\"\n",
    "        return f\"\"\"\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CHOICES:\n",
    "{formatted_choices}\n",
    "\n",
    "Answer with a single letter from A through {max_letter} without any additional explanation or commentary.\n",
    "\"\"\"\n",
    "\n",
    "    def _create_yaml_prompt(self, question, formatted_choices, max_letter):\n",
    "        \"\"\"Create a prompt requesting YAML-formatted reasoning\"\"\"\n",
    "        return f\"\"\"\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CHOICES:\n",
    "{formatted_choices}\n",
    "\n",
    "Analyze this question step-by-step and provide a detailed explanation.\n",
    "Your response MUST be in YAML format as follows:\n",
    "\n",
    "understanding: |\n",
    "  <your understanding of what the question is asking>\n",
    "analysis: |\n",
    "  <your analysis of each option>\n",
    "reasoning: |\n",
    "  <your step-by-step reasoning process>\n",
    "conclusion: |\n",
    "  <your final conclusion>\n",
    "answer: <single letter A through {max_letter}>\n",
    "\n",
    "The answer field MUST contain ONLY a single character letter.\n",
    "\"\"\"\n",
    "\n",
    "    def create_training_prompt(self, question, choices):\n",
    "        \"\"\"\n",
    "        Create a prompt for training with the current prompt type\n",
    "\n",
    "        Args:\n",
    "            question: The question text\n",
    "            choices: List of choices\n",
    "\n",
    "        Returns:\n",
    "            Formatted prompt string for training\n",
    "        \"\"\"\n",
    "        formatted_choices = self.format_choices(choices)\n",
    "        max_letter = self.get_max_letter(choices)\n",
    "\n",
    "        if self.prompt_type == self.YAML_REASONING:\n",
    "            return self._create_yaml_training_prompt(question, formatted_choices, max_letter)\n",
    "        else:\n",
    "            return self._create_basic_training_prompt(question, formatted_choices, max_letter)\n",
    "\n",
    "    def _create_basic_training_prompt(self, question, formatted_choices, max_letter):\n",
    "        \"\"\"Create a basic training prompt\"\"\"\n",
    "        return f\"\"\"\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CHOICES:\n",
    "{formatted_choices}\n",
    "\n",
    "The answer is a single letter (A, B, C, etc.). Only provide ONE character as your answer:\n",
    "\"\"\"\n",
    "\n",
    "    def _create_yaml_training_prompt(self, question, formatted_choices, max_letter):\n",
    "        \"\"\"Create a YAML-formatted training prompt\"\"\"\n",
    "        return f\"\"\"\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CHOICES:\n",
    "{formatted_choices}\n",
    "\n",
    "Analyze this question step-by-step and provide a detailed explanation.\n",
    "Follow the YAML format in your response:\n",
    "\n",
    "understanding: |\n",
    "  <your understanding of the question>\n",
    "analysis: |\n",
    "  <your analysis of each option>\n",
    "reasoning: |\n",
    "  <your reasoning about the correct answer>\n",
    "conclusion: |\n",
    "  <your final conclusion>\n",
    "answer: <single letter A through {max_letter}>\n",
    "\"\"\"\n",
    "\n",
    "    def set_prompt_type(self, prompt_type):\n",
    "        \"\"\"Set the prompt type\"\"\"\n",
    "        # For prompt formatting, teacher_reasoned is equivalent to yaml_reasoning\n",
    "        self.original_type = prompt_type  # Store the original type\n",
    "\n",
    "        if prompt_type == self.TEACHER_REASONED:\n",
    "            # prompt_type = self.YAML_REASONING\n",
    "            pass\n",
    "\n",
    "        self.prompt_type = prompt_type\n",
    "        return self\n",
    "\n",
    "    def is_teacher_mode(self):\n",
    "        \"\"\"Check if we're using teacher mode (for training with teacher completions)\"\"\"\n",
    "        return self.original_type == self.TEACHER_REASONED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of ResponseParser class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseParser:\n",
    "    \"\"\"\n",
    "    Parser for model responses with support for different formats\n",
    "    Extracts answers and reasoning from model outputs\n",
    "    \"\"\"\n",
    "\n",
    "    # Parser modes\n",
    "    BASIC = \"basic\"  # Extract single letter answer\n",
    "    YAML = \"yaml\"  # Parse YAML formatted response with reasoning\n",
    "\n",
    "    def __init__(self, parser_mode=BASIC):\n",
    "        \"\"\"\n",
    "        Initialize with specified parser mode\n",
    "\n",
    "        Args:\n",
    "            parser_mode: Mode to use for parsing - \"basic\" or \"yaml\"\n",
    "        \"\"\"\n",
    "        self.parser_mode = parser_mode\n",
    "\n",
    "    def parse(self, response_text):\n",
    "        \"\"\"\n",
    "        Parse the model's response according to the current mode\n",
    "\n",
    "        Args:\n",
    "            response_text: Raw response text from the model\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (answer, reasoning)\n",
    "        \"\"\"\n",
    "        if self.parser_mode == self.YAML:\n",
    "            return self._parse_yaml_response(response_text)\n",
    "        else:\n",
    "            return self._parse_basic_response(response_text)\n",
    "\n",
    "    def _parse_basic_response(self, response_text):\n",
    "        \"\"\"\n",
    "        Parse basic response looking for a letter answer\n",
    "\n",
    "        For basic mode, we look for a single letter (A-Z) with minimal reasoning\n",
    "        \"\"\"\n",
    "        import re\n",
    "\n",
    "        # Try to extract a single letter answer (A-Z)\n",
    "        answer_match = re.search(r\"(?:^|\\s)([A-Z])(?:\\s|$|\\.)\", response_text)\n",
    "        if answer_match:\n",
    "            answer = answer_match.group(1)\n",
    "        else:\n",
    "            # Take first character if it's a letter\n",
    "            if response_text and response_text[0].isalpha():\n",
    "                answer = response_text[0].upper()\n",
    "            else:\n",
    "                answer = None\n",
    "\n",
    "        # For basic mode, we don't extract detailed reasoning\n",
    "        reasoning = \"\"\n",
    "\n",
    "        return answer, reasoning\n",
    "\n",
    "    def _parse_yaml_response(self, response_text):\n",
    "        \"\"\"\n",
    "        Parse YAML formatted response extracting answer and reasoning\n",
    "\n",
    "        For YAML mode, we try to extract both the answer and structured reasoning\n",
    "        \"\"\"\n",
    "        import re\n",
    "        import yaml\n",
    "\n",
    "        # First try to find answer in YAML format\n",
    "        yaml_match = re.search(r\"answer:\\s*([A-Z])\", response_text)\n",
    "        if yaml_match:\n",
    "            answer = yaml_match.group(1)\n",
    "        else:\n",
    "            # Fall back to basic extraction if YAML parsing fails\n",
    "            answer_match = re.search(r\"(?:^|\\s)([A-Z])(?:\\s|$|\\.)\", response_text)\n",
    "            if answer_match:\n",
    "                answer = answer_match.group(1)\n",
    "            elif response_text and response_text[0].isalpha():\n",
    "                answer = response_text[0].upper()\n",
    "            else:\n",
    "                answer = None\n",
    "\n",
    "        # Try to parse reasoning from YAML format\n",
    "        reasoning = \"\"\n",
    "        if \"reasoning:\" in response_text:\n",
    "            yaml_content = yaml.safe_load(\"---\\n\" + response_text)\n",
    "            if isinstance(yaml_content, dict) and \"reasoning\" in yaml_content:\n",
    "                reasoning = yaml_content[\"reasoning\"]\n",
    "\n",
    "                # Add other YAML fields if available\n",
    "                if \"understanding\" in yaml_content:\n",
    "                    reasoning = f\"Understanding: {yaml_content['understanding']}\\n\\n{reasoning}\"\n",
    "                if \"conclusion\" in yaml_content:\n",
    "                    reasoning = f\"{reasoning}\\n\\nConclusion: {yaml_content['conclusion']}\"\n",
    "        else:\n",
    "            # Use the full response as reasoning if not in YAML format\n",
    "            reasoning = response_text\n",
    "\n",
    "        return answer, reasoning\n",
    "\n",
    "    def set_parser_mode(self, parser_mode):\n",
    "        \"\"\"Set the parser mode\"\"\"\n",
    "        self.parser_mode = parser_mode\n",
    "        return self\n",
    "\n",
    "    @classmethod\n",
    "    def from_prompt_type(cls, prompt_type):\n",
    "        \"\"\"\n",
    "        Create a parser instance with mode matching the prompt type\n",
    "\n",
    "        Args:\n",
    "            prompt_type: Prompt type from PromptCreator\n",
    "\n",
    "        Returns:\n",
    "            ResponseParser instance with appropriate mode\n",
    "        \"\"\"\n",
    "        if (\n",
    "            prompt_type == PromptCreator.YAML_REASONING\n",
    "            or prompt_type == PromptCreator.TEACHER_REASONED\n",
    "        ):\n",
    "            return cls(parser_mode=cls.YAML)\n",
    "        else:\n",
    "            return cls(parser_mode=cls.BASIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of QwenModelHandler class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QwenModelHandler:\n",
    "    \"\"\"Handler for Qwen models with inference and saving capabilities using Unsloth\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"unsloth/Qwen2.5-7B\",\n",
    "        max_seq_length=768,\n",
    "        quantization=None,\n",
    "        device_map=\"auto\",\n",
    "        cache_dir=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize model and tokenizer using Unsloth\n",
    "\n",
    "        Args:\n",
    "            model_name: Name or path of the model (preferably an unsloth model)\n",
    "            max_seq_length: Maximum sequence length for the model\n",
    "            quantization: Quantization type (None, '4bit', '8bit') - for compatibility\n",
    "            device_map: Device mapping strategy\n",
    "            cache_dir: Cache directory for models\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.device_map = device_map\n",
    "        self.quantization = quantization\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "        # Convert quantization parameter to load_in_4bit parameter for Unsloth\n",
    "        self.load_in_4bit = quantization == \"4bit\"\n",
    "\n",
    "        # Load tokenizer and model\n",
    "        self.tokenizer, self.model = self._load_model()\n",
    "        self.response_parser = ResponseParser()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load model and tokenizer with Unsloth for optimization\"\"\"\n",
    "        from unsloth import FastLanguageModel\n",
    "        import torch\n",
    "\n",
    "        print(f\"Loading {self.model_name} with Unsloth, max_seq_length={self.max_seq_length}\")\n",
    "\n",
    "        # Set dtype based on hardware\n",
    "        dtype = None  # None for auto detection\n",
    "\n",
    "        # Load model and tokenizer with Unsloth\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=self.model_name,\n",
    "            max_seq_length=self.max_seq_length,\n",
    "            dtype=dtype,\n",
    "            load_in_4bit=self.load_in_4bit,\n",
    "            cache_dir=self.cache_dir,\n",
    "        )\n",
    "\n",
    "        return tokenizer, model\n",
    "\n",
    "    def generate_with_streaming(\n",
    "        self, prompt, temperature=0.7, max_tokens=1024, stream=True, use_cache=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate completion with optional streaming using Unsloth's optimized inference\n",
    "        \"\"\"\n",
    "        # Enable faster inference\n",
    "        from unsloth import FastLanguageModel\n",
    "\n",
    "        FastLanguageModel.for_inference(self.model)\n",
    "\n",
    "        # Format as chat\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        chat_text = self.tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Tokenize input\n",
    "        model_inputs = self.tokenizer([chat_text], return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "        # Generate with streaming if requested\n",
    "        if stream:\n",
    "            from transformers import TextIteratorStreamer\n",
    "            import threading\n",
    "\n",
    "            # Set up streamer\n",
    "            streamer = TextIteratorStreamer(\n",
    "                self.tokenizer, skip_prompt=True, skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            # Start generation in a thread\n",
    "            generation_kwargs = {\n",
    "                \"input_ids\": model_inputs.input_ids,\n",
    "                \"attention_mask\": model_inputs.attention_mask,\n",
    "                \"temperature\": temperature,\n",
    "                \"max_new_tokens\": max_tokens,\n",
    "                \"streamer\": streamer,\n",
    "                \"do_sample\": temperature > 0.0,\n",
    "                \"use_cache\": use_cache,  # Important for Unsloth performance\n",
    "                \"min_p\": (\n",
    "                    0.1 if temperature > 0.0 else None\n",
    "                ),  # Optional: Unsloth recommends this for better quality\n",
    "            }\n",
    "\n",
    "            thread = threading.Thread(target=self.model.generate, kwargs=generation_kwargs)\n",
    "            thread.start()\n",
    "\n",
    "            # Return the streamer that yields text chunks\n",
    "            return streamer\n",
    "        else:\n",
    "            # Generate without streaming\n",
    "            generated_ids = self.model.generate(\n",
    "                input_ids=model_inputs.input_ids,\n",
    "                attention_mask=model_inputs.attention_mask,\n",
    "                temperature=temperature,\n",
    "                max_new_tokens=max_tokens,\n",
    "                do_sample=temperature > 0.0,\n",
    "                use_cache=use_cache,  # Important for Unsloth performance\n",
    "                min_p=0.1 if temperature > 0.0 else None,  # Optional: Unsloth recommends this\n",
    "            )\n",
    "\n",
    "            # Decode the generated text\n",
    "            generated_text = self.tokenizer.decode(\n",
    "                generated_ids[0][model_inputs.input_ids.shape[1] :], skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            return generated_text\n",
    "\n",
    "    def calculate_perplexity(self, prompt, answer, temperature=0.0):\n",
    "        \"\"\"\n",
    "        Calculate perplexity for a prompt and answer pair\n",
    "\n",
    "        Args:\n",
    "            prompt: The input prompt\n",
    "            answer: The expected answer\n",
    "            temperature: Sampling temperature\n",
    "\n",
    "        Returns:\n",
    "            Perplexity score\n",
    "        \"\"\"\n",
    "        import torch\n",
    "\n",
    "        # Format chat for perplexity calculation\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": answer}]\n",
    "        chat_text = self.tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "        # Tokenize the text\n",
    "        encodings = self.tokenizer(chat_text, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "        # Calculate loss\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**encodings, labels=encodings.input_ids)\n",
    "\n",
    "        # Get loss and calculate perplexity\n",
    "        neg_log_likelihood = outputs.loss.item()\n",
    "        perplexity = torch.exp(torch.tensor(neg_log_likelihood)).item()\n",
    "\n",
    "        return perplexity\n",
    "\n",
    "    def save_model(self, output_dir, save_method=\"lora\"):\n",
    "        \"\"\"\n",
    "        Save model to disk using Unsloth's optimized methods\n",
    "\n",
    "        Args:\n",
    "            output_dir: Directory to save the model\n",
    "            save_method: Method to use for saving (\"lora\", \"merged_16bit\", \"merged_4bit\", \"gguf\")\n",
    "        \"\"\"\n",
    "        import os\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Use Unsloth's saving methods\n",
    "        if save_method == \"lora\":\n",
    "            # Save LoRA weights\n",
    "            self.model.save_pretrained(output_dir)\n",
    "            self.tokenizer.save_pretrained(output_dir)\n",
    "        elif save_method == \"merged_16bit\":\n",
    "            # Save merged model in float16\n",
    "            self.model.save_pretrained_merged(\n",
    "                output_dir, self.tokenizer, save_method=\"merged_16bit\"\n",
    "            )\n",
    "        elif save_method == \"merged_4bit\":\n",
    "            # Save merged model in 4bit\n",
    "            self.model.save_pretrained_merged(output_dir, self.tokenizer, save_method=\"merged_4bit\")\n",
    "        elif save_method == \"gguf\":\n",
    "            # Save in GGUF format for llama.cpp\n",
    "            self.model.save_pretrained_gguf(\n",
    "                output_dir, self.tokenizer, quantization_method=\"q4_k_m\"\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown save method: {save_method}\")\n",
    "\n",
    "        print(f\"Model saved to {output_dir} using method {save_method}\")\n",
    "        return output_dir\n",
    "\n",
    "    def push_to_hub(self, repo_id, token=None, save_method=\"lora\", private=False):\n",
    "        \"\"\"\n",
    "        Push model to Hugging Face Hub using Unsloth's optimized methods\n",
    "        \"\"\"\n",
    "        # Use Unsloth's hub methods directly\n",
    "        if save_method == \"lora\":\n",
    "            self.model.push_to_hub_merged(repo_id, self.tokenizer, save_method=\"lora\", token=token)\n",
    "        elif save_method == \"merged_16bit\":\n",
    "            self.model.push_to_hub_merged(\n",
    "                repo_id, self.tokenizer, save_method=\"merged_16bit\", token=token\n",
    "            )\n",
    "        elif save_method == \"merged_4bit\":\n",
    "            self.model.push_to_hub_merged(\n",
    "                repo_id, self.tokenizer, save_method=\"merged_4bit\", token=token\n",
    "            )\n",
    "        elif save_method == \"gguf\":\n",
    "            # Push multiple GGUF variants\n",
    "            self.model.push_to_hub_gguf(\n",
    "                repo_id, self.tokenizer, quantization_method=[\"q4_k_m\", \"q5_k_m\"], token=token\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown save method: {save_method}\")\n",
    "\n",
    "        print(f\"Model successfully pushed to: https://huggingface.co/{repo_id}\")\n",
    "        return f\"https://huggingface.co/{repo_id}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of ResponseParser class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of MultipleChoiceTester class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleChoiceTester:\n",
    "    \"\"\"Framework for testing Qwen models on multiple choice questions\"\"\"\n",
    "\n",
    "    def __init__(self, model_handler, prompt_creator=None):\n",
    "        \"\"\"\n",
    "        Initialize with model handler and prompt configuration\n",
    "\n",
    "        Args:\n",
    "            model_handler: The QwenModelHandler instance\n",
    "            prompt_creator: Optional PromptCreator instance (will create one if not provided)\n",
    "        \"\"\"\n",
    "        self.model_handler = model_handler\n",
    "        self.prompt_creator = prompt_creator or PromptCreator(PromptCreator.BASIC)\n",
    "        # Create a response parser matching the prompt type\n",
    "        self.response_parser = ResponseParser.from_prompt_type(self.prompt_creator.prompt_type)\n",
    "\n",
    "    def infer_example(\n",
    "        self,\n",
    "        example,\n",
    "        temperature=0.7,\n",
    "        max_tokens=1024,\n",
    "        prompt_type=None,\n",
    "        stream=False,\n",
    "        use_cache=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Mode 1: Inference on a single example for visualization/demonstration\n",
    "\n",
    "        Args:\n",
    "            example: Single example to infer (dict with question, choices, etc.)\n",
    "            temperature: Sampling temperature for generation\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            prompt_type: Optional override for prompt type\n",
    "            stream: Whether to stream the output\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with prediction and metrics\n",
    "        \"\"\"\n",
    "        # Allow temporary override of prompt type\n",
    "        original_prompt_type = None\n",
    "        if prompt_type is not None:\n",
    "            original_prompt_type = self.prompt_creator.prompt_type\n",
    "            self.prompt_creator.set_prompt_type(prompt_type)\n",
    "            # Update response parser to match prompt type\n",
    "            self.response_parser = ResponseParser.from_prompt_type(prompt_type)\n",
    "\n",
    "        # Prepare data\n",
    "        question = example[\"question\"]\n",
    "\n",
    "        # Handle different formats of choices\n",
    "        if isinstance(example[\"choices\"], list):\n",
    "            choices = example[\"choices\"]\n",
    "        elif isinstance(example[\"choices\"], str) and example[\"choices\"].startswith(\"[\"):\n",
    "            # Parse string representation of list\n",
    "            import ast\n",
    "\n",
    "            choices = (\n",
    "                ast.literal_eval(example[\"choices\"])\n",
    "                if \"[\" in example[\"choices\"]\n",
    "                else example[\"choices\"].split(\",\")\n",
    "            )\n",
    "        else:\n",
    "            choices = str(example[\"choices\"]).split(\",\")\n",
    "\n",
    "        # Generate the prompt using prompt creator\n",
    "        prompt = self.prompt_creator.create_inference_prompt(question, choices)\n",
    "\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "\n",
    "        if stream:\n",
    "            # Use streaming generation\n",
    "            streamer = self.model_handler.generate_with_streaming(\n",
    "                prompt=prompt,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                stream=stream,\n",
    "                use_cache=use_cache,\n",
    "            )\n",
    "\n",
    "            # Collect output from streamer\n",
    "            raw_response = \"\"\n",
    "            print(\"Model response:\")\n",
    "            for text_chunk in streamer:\n",
    "                print(text_chunk, end=\"\", flush=True)\n",
    "                raw_response += text_chunk\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "            # Generate without streaming\n",
    "            raw_response = self.model_handler.generate_with_streaming(\n",
    "                prompt=prompt,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                stream=stream,\n",
    "                use_cache=use_cache,\n",
    "            )\n",
    "\n",
    "        response_time = time.time() - start_time\n",
    "\n",
    "        # Parse the response using the response parser\n",
    "        predicted_answer, reasoning = self.response_parser.parse(raw_response)\n",
    "\n",
    "        # Prepare results\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"choices\": choices,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"reasoning\": reasoning,\n",
    "            \"response_time\": response_time,\n",
    "            \"raw_response\": raw_response,\n",
    "            \"prompt_type\": self.prompt_creator.prompt_type,\n",
    "        }\n",
    "\n",
    "        # Add task_id if available\n",
    "        if \"task_id\" in example:\n",
    "            result[\"task_id\"] = example[\"task_id\"]\n",
    "\n",
    "        # Calculate metrics if label is provided\n",
    "        if \"answer\" in example:\n",
    "            label = example[\"answer\"]\n",
    "            result[\"correct_answer\"] = label\n",
    "            result[\"is_correct\"] = predicted_answer == label\n",
    "\n",
    "            # Calculate perplexity if requested\n",
    "            if hasattr(self.model_handler, \"calculate_perplexity\"):\n",
    "                perplexity = self.model_handler.calculate_perplexity(prompt, raw_response)\n",
    "                result[\"perplexity\"] = perplexity\n",
    "\n",
    "        # Restore original prompt type if it was overridden\n",
    "        if original_prompt_type is not None:\n",
    "            self.prompt_creator.set_prompt_type(original_prompt_type)\n",
    "            # Restore the original response parser\n",
    "            self.response_parser = ResponseParser.from_prompt_type(original_prompt_type)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def infer_batch(\n",
    "        self, examples, temperature=0.7, max_tokens=1024, prompt_type=None, batch_size=4\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Mode 2: Inference on a batch of examples\n",
    "\n",
    "        Args:\n",
    "            examples: List of examples to infer\n",
    "            temperature: Sampling temperature\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            prompt_type: Optional override for prompt type\n",
    "            batch_size: Size of batches for processing\n",
    "\n",
    "        Returns:\n",
    "            List of result dictionaries and summary metrics\n",
    "        \"\"\"\n",
    "        # Allow temporary override of prompt type\n",
    "        original_prompt_type = None\n",
    "        if prompt_type is not None:\n",
    "            original_prompt_type = self.prompt_creator.prompt_type\n",
    "            self.prompt_creator.set_prompt_type(prompt_type)\n",
    "            # Update response parser to match prompt type\n",
    "            self.response_parser = ResponseParser.from_prompt_type(prompt_type)\n",
    "\n",
    "        # Prepare all prompts\n",
    "        prompts = []\n",
    "        metadata = []\n",
    "\n",
    "        for i, example in enumerate(examples):\n",
    "            # Extract data\n",
    "            question = example[\"question\"]\n",
    "\n",
    "            # Handle different formats of choices\n",
    "            if isinstance(example[\"choices\"], list):\n",
    "                choices = example[\"choices\"]\n",
    "            elif isinstance(example[\"choices\"], str) and example[\"choices\"].startswith(\"[\"):\n",
    "                # Parse string representation of list\n",
    "                import ast\n",
    "\n",
    "                choices = (\n",
    "                    ast.literal_eval(example[\"choices\"])\n",
    "                    if \"[\" in example[\"choices\"]\n",
    "                    else example[\"choices\"].split(\",\")\n",
    "                )\n",
    "            else:\n",
    "                choices = str(example[\"choices\"]).split(\",\")\n",
    "\n",
    "            # Generate the prompt using prompt creator\n",
    "            prompt = self.prompt_creator.create_inference_prompt(question, choices)\n",
    "            prompts.append(prompt)\n",
    "\n",
    "            # Store metadata for later\n",
    "            meta = {\n",
    "                \"question\": question,\n",
    "                \"choices\": choices,\n",
    "                \"index\": i,\n",
    "            }\n",
    "\n",
    "            # Add label if available\n",
    "            if \"answer\" in example:\n",
    "                meta[\"label\"] = example[\"answer\"]\n",
    "\n",
    "            if \"task_id\" in example:\n",
    "                meta[\"task_id\"] = example[\"task_id\"]\n",
    "\n",
    "            metadata.append(meta)\n",
    "\n",
    "        # Process in batches\n",
    "        results = []\n",
    "        correct_count = 0\n",
    "        total_count = 0\n",
    "        perplexities = []\n",
    "\n",
    "        for i in range(0, len(prompts), batch_size):\n",
    "            batch_prompts = prompts[i : i + batch_size]\n",
    "            batch_meta = metadata[i : i + batch_size]\n",
    "\n",
    "            # Process batch\n",
    "            start_time = time.time()\n",
    "            batch_responses = []\n",
    "\n",
    "            for prompt in batch_prompts:\n",
    "                response = self.model_handler.generate_with_streaming(\n",
    "                    prompt=prompt, temperature=temperature, max_tokens=max_tokens, stream=False\n",
    "                )\n",
    "                batch_responses.append(response)\n",
    "\n",
    "            batch_time = time.time() - start_time\n",
    "\n",
    "            # Process each response in the batch\n",
    "            for j, (response, meta) in enumerate(zip(batch_responses, batch_meta)):\n",
    "                # Parse response\n",
    "                predicted_answer, reasoning = self.response_parser.parse(response)\n",
    "\n",
    "                # Create result\n",
    "                result = {\n",
    "                    \"question\": meta[\"question\"],\n",
    "                    \"choices\": meta[\"choices\"],\n",
    "                    \"predicted_answer\": predicted_answer,\n",
    "                    \"reasoning\": reasoning,\n",
    "                    \"raw_response\": response,\n",
    "                    \"prompt_type\": self.prompt_creator.prompt_type,\n",
    "                    \"response_time\": batch_time / len(batch_prompts),  # Approximate individual time\n",
    "                }\n",
    "\n",
    "                # Add task_id if available\n",
    "                if \"task_id\" in meta:\n",
    "                    result[\"task_id\"] = meta[\"task_id\"]\n",
    "\n",
    "                # Add metrics if label available\n",
    "                if \"label\" in meta:\n",
    "                    label = meta[\"label\"]\n",
    "                    result[\"correct_answer\"] = label\n",
    "                    result[\"is_correct\"] = predicted_answer == label\n",
    "\n",
    "                    # Update counts for accuracy\n",
    "                    total_count += 1\n",
    "                    if result[\"is_correct\"]:\n",
    "                        correct_count += 1\n",
    "\n",
    "                    # Calculate perplexity if possible\n",
    "                    if hasattr(self.model_handler, \"calculate_perplexity\"):\n",
    "                        prompt = batch_prompts[j]\n",
    "                        perplexity = self.model_handler.calculate_perplexity(prompt, response)\n",
    "                        result[\"perplexity\"] = perplexity\n",
    "                        perplexities.append(perplexity)\n",
    "\n",
    "                results.append(result)\n",
    "\n",
    "        # Calculate aggregate metrics\n",
    "        summary_metrics = {}\n",
    "        if total_count > 0:\n",
    "            summary_metrics[\"accuracy\"] = correct_count / total_count\n",
    "            summary_metrics[\"correct_count\"] = correct_count\n",
    "            summary_metrics[\"total_count\"] = total_count\n",
    "\n",
    "            if perplexities:\n",
    "                summary_metrics[\"avg_perplexity\"] = sum(perplexities) / len(perplexities)\n",
    "                summary_metrics[\"min_perplexity\"] = min(perplexities)\n",
    "                summary_metrics[\"max_perplexity\"] = max(perplexities)\n",
    "\n",
    "        # Restore original prompt type if it was overridden\n",
    "        if original_prompt_type is not None:\n",
    "            self.prompt_creator.set_prompt_type(original_prompt_type)\n",
    "            # Restore the original response parser\n",
    "            self.response_parser = ResponseParser.from_prompt_type(original_prompt_type)\n",
    "\n",
    "        return results, summary_metrics\n",
    "\n",
    "    def evaluate_dataset(\n",
    "        self,\n",
    "        dataset,\n",
    "        temperature=0.7,\n",
    "        max_tokens=1024,\n",
    "        num_examples=None,\n",
    "        verbose=True,\n",
    "        prompt_type=None,\n",
    "        batch_size=4,\n",
    "        log_to_wandb=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Mode 3: Inference on a whole dataset with metrics calculation\n",
    "\n",
    "        Args:\n",
    "            dataset: Dataset to evaluate\n",
    "            temperature: Sampling temperature\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            num_examples: Number of examples to evaluate (None for all)\n",
    "            verbose: Whether to print progress information\n",
    "            prompt_type: Override the prompt type for this evaluation\n",
    "            batch_size: Size of batches for processing\n",
    "            log_to_wandb: Whether to log results to wandb\n",
    "\n",
    "        Returns:\n",
    "            Summary dictionary with results and metrics\n",
    "        \"\"\"\n",
    "        # Allow overriding the prompt type for this evaluation\n",
    "        original_prompt_type = self.prompt_creator.prompt_type\n",
    "        if prompt_type is not None:\n",
    "            self.prompt_creator.set_prompt_type(prompt_type)\n",
    "            # Update response parser to match prompt type\n",
    "            self.response_parser = ResponseParser.from_prompt_type(prompt_type)\n",
    "\n",
    "        # Select subset if specified\n",
    "        if num_examples is not None:\n",
    "            dataset = dataset.select(range(min(num_examples, len(dataset))))\n",
    "\n",
    "        results = []\n",
    "        correct_count = 0\n",
    "        total_count = 0\n",
    "        perplexities = []\n",
    "\n",
    "        # Process examples in batches\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            batch_examples = dataset[i : i + batch_size]\n",
    "\n",
    "            if verbose:\n",
    "                batch_desc = (\n",
    "                    f\"Batch {i//batch_size + 1}/{(len(dataset) + batch_size - 1) // batch_size}\"\n",
    "                )\n",
    "                print(f\"\\nProcessing {batch_desc} with {len(batch_examples)} examples...\")\n",
    "\n",
    "            # Infer batch\n",
    "            batch_results, batch_metrics = self.infer_batch(\n",
    "                examples=batch_examples,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "\n",
    "            # Update metrics\n",
    "            results.extend(batch_results)\n",
    "            if \"correct_count\" in batch_metrics:\n",
    "                correct_count += batch_metrics[\"correct_count\"]\n",
    "                total_count += batch_metrics[\"total_count\"]\n",
    "\n",
    "                if verbose:\n",
    "                    batch_accuracy = batch_metrics[\"accuracy\"]\n",
    "                    overall_accuracy = correct_count / total_count\n",
    "                    print(\n",
    "                        f\"Batch accuracy: {batch_accuracy:.2%}, Overall: {overall_accuracy:.2%} ({correct_count}/{total_count})\"\n",
    "                    )\n",
    "\n",
    "            # Collect perplexities\n",
    "            if \"avg_perplexity\" in batch_metrics:\n",
    "                for result in batch_results:\n",
    "                    if \"perplexity\" in result:\n",
    "                        perplexities.append(result[\"perplexity\"])\n",
    "\n",
    "        # Calculate final accuracy\n",
    "        accuracy = correct_count / total_count if total_count > 0 else 0.0\n",
    "\n",
    "        if verbose:\n",
    "            prompt_type_str = self.prompt_creator.prompt_type\n",
    "            print(\n",
    "                f\"\\nFinal accuracy with {prompt_type_str} prompts: {accuracy:.2%} ({correct_count}/{total_count})\"\n",
    "            )\n",
    "            if perplexities:\n",
    "                avg_perplexity = sum(perplexities) / len(perplexities)\n",
    "                print(f\"Average perplexity: {avg_perplexity:.4f}\")\n",
    "\n",
    "        # Prepare comprehensive summary\n",
    "        summary = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"correct_count\": correct_count,\n",
    "            \"total_count\": total_count,\n",
    "            \"prompt_type\": self.prompt_creator.prompt_type,\n",
    "            \"results\": results,\n",
    "        }\n",
    "\n",
    "        # Add perplexity metrics if available\n",
    "        if perplexities:\n",
    "            summary[\"avg_perplexity\"] = sum(perplexities) / len(perplexities)\n",
    "            summary[\"min_perplexity\"] = min(perplexities)\n",
    "            summary[\"max_perplexity\"] = max(perplexities)\n",
    "\n",
    "        # Log results to wandb if requested\n",
    "        if log_to_wandb and wandb.run is not None:\n",
    "            metrics = {\n",
    "                \"test/accuracy\": accuracy,\n",
    "                \"test/correct_count\": correct_count,\n",
    "                \"test/total_count\": total_count,\n",
    "            }\n",
    "            if perplexities:\n",
    "                metrics[\"test/avg_perplexity\"] = summary[\"avg_perplexity\"]\n",
    "                metrics[\"test/min_perplexity\"] = summary[\"min_perplexity\"]\n",
    "                metrics[\"test/max_perplexity\"] = summary[\"max_perplexity\"]\n",
    "\n",
    "            wandb.log(metrics)\n",
    "\n",
    "            # Create a table of results for visualization if task_id exists\n",
    "            if \"task_id\" in dataset.features:\n",
    "                columns = [\n",
    "                    \"task_id\",\n",
    "                    \"question\",\n",
    "                    \"correct_answer\",\n",
    "                    \"predicted_answer\",\n",
    "                    \"is_correct\",\n",
    "                ]\n",
    "                table = wandb.Table(columns=columns)\n",
    "\n",
    "                for res in results[: min(100, len(results))]:  # Limit to 100 examples\n",
    "                    table.add_data(\n",
    "                        res.get(\"task_id\", \"unknown\"),\n",
    "                        res[\"question\"][:100] + \"...\",\n",
    "                        res.get(\"correct_answer\", \"\"),\n",
    "                        res.get(\"predicted_answer\", \"\"),\n",
    "                        res.get(\"is_correct\", False),\n",
    "                    )\n",
    "\n",
    "                wandb.log({\"test_samples\": table})\n",
    "\n",
    "        # Restore original prompt type\n",
    "        self.prompt_creator.set_prompt_type(original_prompt_type)\n",
    "        # Restore the original response parser\n",
    "        self.response_parser = ResponseParser.from_prompt_type(original_prompt_type)\n",
    "\n",
    "        return summary\n",
    "\n",
    "    def save_results(self, results, output_dir=\"./results\"):\n",
    "        \"\"\"Save evaluation results to file\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        results_file = os.path.join(output_dir, f\"results_{timestamp}.json\")\n",
    "\n",
    "        # Create serializable results\n",
    "        serializable_results = {\n",
    "            \"accuracy\": results.get(\"accuracy\", 0.0),\n",
    "            \"correct_count\": results.get(\"correct_count\", 0),\n",
    "            \"total_count\": results.get(\"total_count\", 0),\n",
    "            \"timestamp\": timestamp,\n",
    "            \"prompt_type\": results.get(\"prompt_type\", \"unknown\"),\n",
    "        }\n",
    "\n",
    "        # Add perplexity metrics if available\n",
    "        if \"avg_perplexity\" in results:\n",
    "            serializable_results[\"avg_perplexity\"] = results[\"avg_perplexity\"]\n",
    "            serializable_results[\"min_perplexity\"] = results[\"min_perplexity\"]\n",
    "            serializable_results[\"max_perplexity\"] = results[\"max_perplexity\"]\n",
    "\n",
    "        # Process individual results\n",
    "        serializable_results[\"individual_results\"] = []\n",
    "        for result in results[\"results\"]:\n",
    "            # Skip perplexity in individual results to save space\n",
    "            result_copy = result.copy()\n",
    "            if \"perplexity\" in result_copy:\n",
    "                del result_copy[\"perplexity\"]\n",
    "\n",
    "            # Convert choices if needed\n",
    "            choices = result_copy[\"choices\"]\n",
    "            if not isinstance(choices, list):\n",
    "                try:\n",
    "                    import ast\n",
    "\n",
    "                    result_copy[\"choices\"] = ast.literal_eval(choices)\n",
    "                except (SyntaxError, ValueError):\n",
    "                    # Keep as-is if conversion fails\n",
    "                    pass\n",
    "\n",
    "            serializable_results[\"individual_results\"].append(result_copy)\n",
    "\n",
    "        # Save to file\n",
    "        with open(results_file, \"w\") as f:\n",
    "            import json\n",
    "\n",
    "            json.dump(serializable_results, f, indent=2)\n",
    "\n",
    "        print(f\"Results saved to {results_file}\")\n",
    "        return results_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QwenTrainer class implementation:\n",
    "* Please refer to the file `src/training/trainer.py` for the implementation.\n",
    "* This notebook is guiding for inference with the latest model from HuggingFace Hub.\n",
    "* For reproducing the training, please refer to `../train.sh`\n",
    "* For more details, please refer to the `README.md` file.\n",
    "* For the dataset, please refer to `https://huggingface.co/datasets/tuandunghcmut/coding-mcq-reasoning`\n",
    "* For the legacy model, please refer to `LEGACY.md`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for loading the latest model from HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from HuggingFace Hub: tuandunghcmut/Qwen25_Coder_MultipleChoice_v4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tuandunghcmut/Qwen25_Coder_MultipleChoice_v4 with Unsloth, max_seq_length=2048\n",
      "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.50.3.\n",
      "   \\\\   /|    Tesla V100-SXM2-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Successfully loaded model from HuggingFace Hub!\n"
     ]
    }
   ],
   "source": [
    "# Load the latest model from HuggingFace Hub\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# ````\n",
    "# Set HuggingFace Hub credentials if available\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "# Model ID on HuggingFace Hub\n",
    "hub_model_id = \"tuandunghcmut/Qwen25_Coder_MultipleChoice_v4\"\n",
    "\n",
    "print(f\"Loading model from HuggingFace Hub: {hub_model_id}\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "try:\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(hub_model_id, token=hf_token, trust_remote_code=True)\n",
    "\n",
    "    # Load model with appropriate parameters for inference\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        hub_model_id,\n",
    "        token=hf_token,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    # Create a new model handler with the loaded model and tokenizer\n",
    "    # from model_handler import ModelHandler  # Assuming ModelHandler class is available\n",
    "\n",
    "    # lastest_model_handler_hub = QwenModelHandler(model_name=hub_model_id, max_seq_length=2048, quantization=\"4bit\")\n",
    "    lastest_model_handler_hub = QwenModelHandler(\n",
    "        model_name=hub_model_id, max_seq_length=2048, quantization=\"8bit\"\n",
    "    )\n",
    "    #  quantization=\"16bit\")\n",
    "\n",
    "    # Use FastLanguageModel\n",
    "    from unsloth.models import FastLanguageModel\n",
    "\n",
    "    FastLanguageModel.for_inference(lastest_model_handler_hub.model)\n",
    "    prompt_creator = PromptCreator(PromptCreator.YAML_REASONING)\n",
    "    # Create a tester with the loaded model\n",
    "    latest_tester_hub = MultipleChoiceTester(\n",
    "        lastest_model_handler_hub, prompt_creator=prompt_creator\n",
    "    )\n",
    "\n",
    "    print(\"Successfully loaded model from HuggingFace Hub!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model from HuggingFace Hub: {e}\")\n",
    "    print(\"Continuing with locally trained model...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for fast streaming inference and 10 coding examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Example 1\n",
       "\n",
       "**Question:** Which of the following is NOT a valid way to initialize a variable in C++?\n",
       "\n",
       "**Choices:**\n",
       "- **A.** int x = 5;\n",
       "- **B.** int x(5);\n",
       "- **C.** int x{5};\n",
       "- **D.** int x := 5;\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model Response (streaming):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "understanding: |\n",
      "  The question asks which method is not a valid way to initialize a variable in C++. This involves understanding how different syntax for initializing variables works in C++.\n",
      "analysis: |\n",
      "  A. `int x = 5;` - This is correct because it uses the assignment initializer syntax, which is standard for initializing variables in C++.\n",
      "  B. `int x(5);` - This is incorrect because the constructor syntax is used for creating objects, not for initializing variables.\n",
      "  C. `int x{5};` - This is correct because it uses curly braces, which is the modern C++ initialization syntax for variables.\n",
      "  D. `int x := 5;` - This is incorrect because the colon-initializer syntax was used in older versions of C++ (before C++11) but has been deprecated and removed in newer standards.\n",
      "reasoning: |\n",
      "  The key distinction lies in the syntax used for initialization. Option A uses the assignment initializer, which is correct. Options B and D use constructs that were part of earlier C++ standards but have been phased out. Option C uses the modern C++ brace-initializer, which is correct. Therefore, options B and D are invalid methods for initializing variables in C++.\n",
      "conclusion: |\n",
      "  Answer D is correct because the colon-initializer syntax is no longer supported in modern C++, making it invalid for initializing variables.\n",
      "answer: D\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer:** D\n",
       "\n",
       "**Correct Answer:** D\n",
       "\n",
       "**Reasoning:**\n",
       "\n",
       "Understanding: The question asks which method is not a valid way to initialize a variable in C++. This involves understanding how different syntax for initializing variables works in C++.\n",
       "\n",
       "\n",
       "The key distinction lies in the syntax used for initialization. Option A uses the assignment initializer, which is correct. Options B and D use constructs that were part of earlier C++ standards but have been phased out. Option C uses the modern C++ brace-initializer, which is correct. Therefore, options B and D are invalid methods for initializing variables in C++.\n",
       "\n",
       "\n",
       "Conclusion: Answer D is correct because the colon-initializer syntax is no longer supported in modern C++, making it invalid for initializing variables.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Example 2\n",
       "\n",
       "**Question:** In C, what does the 'malloc' function do?\n",
       "\n",
       "**Choices:**\n",
       "- **A.** Frees allocated memory\n",
       "- **B.** Allocates memory dynamically\n",
       "- **C.** Manages automatic memory\n",
       "- **D.** Moves allocated memory\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model Response (streaming):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "understanding: |\n",
      "  The question asks about the purpose of the 'malloc' function in C programming, which involves understanding how it allocates memory dynamically.\n",
      "analysis: |\n",
      "  A. Incorrect. 'free' is used to deallocate memory, not allocate it.\n",
      "  B. Correct. 'malloc' stands for \"memory allocation\" and is used to request dynamic memory from the system.\n",
      "  C. Incorrect. 'calloc' and 'realloc' manage memory allocation, but 'malloc' itself does not.\n",
      "  D. Incorrect. Memory movement is typically handled by functions like 'memmove', not 'malloc'.\n",
      "reasoning: |\n",
      "  The 'malloc' function is specifically designed to allocate a block of memory of a specified size from the heap. This allows programs to dynamically allocate memory as needed during runtime, unlike static storage classes that require explicit declaration and definition.\n",
      "conclusion: |\n",
      "  Answer B is correct because 'malloc' is explicitly used to allocate memory dynamically, making it the correct choice among the options provided.\n",
      "answer: B\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer:** B\n",
       "\n",
       "**Correct Answer:** B\n",
       "\n",
       "**Reasoning:**\n",
       "\n",
       "Understanding: The question asks about the purpose of the 'malloc' function in C programming, which involves understanding how it allocates memory dynamically.\n",
       "\n",
       "\n",
       "The 'malloc' function is specifically designed to allocate a block of memory of a specified size from the heap. This allows programs to dynamically allocate memory as needed during runtime, unlike static storage classes that require explicit declaration and definition.\n",
       "\n",
       "\n",
       "Conclusion: Answer B is correct because 'malloc' is explicitly used to allocate memory dynamically, making it the correct choice among the options provided.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Example 3\n",
       "\n",
       "**Question:** Which C++ keyword is used to define a class template?\n",
       "\n",
       "**Choices:**\n",
       "- **A.** class\n",
       "- **B.** virtual\n",
       "- **C.** template\n",
       "- **D.** typename\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model Response (streaming):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "understanding: |\n",
      "  The question asks which C++ keyword is specifically used to define a class template. Class templates allow for the creation of generic classes that can work with different data types.\n",
      "analysis: |\n",
      "  A. class - This keyword is used to define a new user-defined type or class, but it does not specifically denote a class template.\n",
      "  B. virtual - This keyword is used in inheritance to specify that a function in a derived class overrides a function in its base class, not to define a class template.\n",
      "  C. template - This keyword is used to declare a class, function, or variable as a template, allowing it to be instantiated with various data types.\n",
      "  D. typename - This keyword is used within class templates to explicitly state that a name is a type, often used with nested types like class templates.\n",
      "reasoning: |\n",
      "  The correct keyword to define a class template is \"template.\" It allows developers to create reusable code snippets that can operate on different data types. While \"class\" and \"typename\" are related concepts in C++, they do not define class templates; \"virtual\" pertains to inheritance, and \"typename\" is used in templates to clarify type names.\n",
      "conclusion: |\n",
      "  Answer C is correct because \"template\" is the specific keyword used to define a class template in C++.\n",
      "answer: C\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer:** C\n",
       "\n",
       "**Correct Answer:** C\n",
       "\n",
       "**Reasoning:**\n",
       "\n",
       "Understanding: The question asks which C++ keyword is specifically used to define a class template. Class templates allow for the creation of generic classes that can work with different data types.\n",
       "\n",
       "\n",
       "The correct keyword to define a class template is \"template.\" It allows developers to create reusable code snippets that can operate on different data types. While \"class\" and \"typename\" are related concepts in C++, they do not define class templates; \"virtual\" pertains to inheritance, and \"typename\" is used in templates to clarify type names.\n",
       "\n",
       "\n",
       "Conclusion: Answer C is correct because \"template\" is the specific keyword used to define a class template in C++.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Example 4\n",
       "\n",
       "**Question:** What is the correct way to access a member of a structure through a pointer in C?\n",
       "\n",
       "**Choices:**\n",
       "- **A.** pointer.member\n",
       "- **B.** pointer->member\n",
       "- **C.** pointer::member\n",
       "- **D.** pointer@member\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model Response (streaming):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "understanding: |\n",
      "  The question asks how to correctly use a pointer to access a member of a structure in C. This involves understanding the syntax used for dereferencing pointers to access structure members.\n",
      "analysis: |\n",
      "  A. pointer.member - Incorrect because it uses dot notation, which requires the structure type or an instance to access members.\n",
      "  B. pointer->member - Correct because it uses arrow notation, which dereferences the pointer to access the member of the structure pointed to.\n",
      "  C. pointer::member - Incorrect due to incorrect scope resolution syntax; C does not support double colons for accessing structure members through a pointer.\n",
      "  D. pointer@member - Incorrect because there is no such operator as @ in C for accessing structure members through a pointer.\n",
      "reasoning: |\n",
      "  In C, when working with structures through pointers, you need to dereference the pointer using the arrow (->) operator to access the members of the structure. This allows you to navigate through the memory location that the pointer points to.\n",
      "conclusion: |\n",
      "  Answer B is correct because it uses the arrow operator to dereference the pointer and access the structure member, which is the proper method in C for this operation.\n",
      "answer: B\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer:** B\n",
       "\n",
       "**Correct Answer:** B\n",
       "\n",
       "**Reasoning:**\n",
       "\n",
       "Understanding: The question asks how to correctly use a pointer to access a member of a structure in C. This involves understanding the syntax used for dereferencing pointers to access structure members.\n",
       "\n",
       "\n",
       "In C, when working with structures through pointers, you need to dereference the pointer using the arrow (->) operator to access the members of the structure. This allows you to navigate through the memory location that the pointer points to.\n",
       "\n",
       "\n",
       "Conclusion: Answer B is correct because it uses the arrow operator to dereference the pointer and access the structure member, which is the proper method in C for this operation.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Example 5\n",
       "\n",
       "**Question:** Which of the following is NOT a storage class specifier in C?\n",
       "\n",
       "**Choices:**\n",
       "- **A.** static\n",
       "- **B.** extern\n",
       "- **C.** register\n",
       "- **D.** virtual\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model Response (streaming):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "understanding: |\n",
      "  The question asks which option is not a storage class specifier in C programming language. Storage classes determine how variables or functions are stored in memory and affect their scope and lifetime.\n",
      "analysis: |\n",
      "  A. static - This storage class specifier is used to declare a variable with local storage duration, meaning it retains its value between function calls.\n",
      "  B. extern - This specifies that a variable or function is defined elsewhere, allowing multiple files to share data.\n",
      "  C. register - This allows a variable to be stored in CPU registers for faster access, but it does not define a storage class.\n",
      "  D. virtual - This is not a standard storage class specifier in C; it might be confused with virtual functions in object-oriented programming contexts.\n",
      "reasoning: |\n",
      "  The correct answer must be the one that does not belong to any of the standard storage class specifiers in C. Option C, \"register,\" is incorrect because it does not define a storage class; rather, it suggests storing variables in CPU registers. Options A, B, and D are all valid storage class specifiers in C.\n",
      "conclusion: |\n",
      "  Answer C is correct because \"register\" is not a storage class specifier; instead, it pertains to optimizing variable storage in CPU registers.\n",
      "answer: C\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer:** C\n",
       "\n",
       "**Correct Answer:** D\n",
       "\n",
       "**Reasoning:**\n",
       "\n",
       "Understanding: The question asks which option is not a storage class specifier in C programming language. Storage classes determine how variables or functions are stored in memory and affect their scope and lifetime.\n",
       "\n",
       "\n",
       "The correct answer must be the one that does not belong to any of the standard storage class specifiers in C. Option C, \"register,\" is incorrect because it does not define a storage class; rather, it suggests storing variables in CPU registers. Options A, B, and D are all valid storage class specifiers in C.\n",
       "\n",
       "\n",
       "Conclusion: Answer C is correct because \"register\" is not a storage class specifier; instead, it pertains to optimizing variable storage in CPU registers.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Example 6\n",
       "\n",
       "**Question:** What does the 'const' keyword signify in C++?\n",
       "\n",
       "**Choices:**\n",
       "- **A.** The variable can be modified indirectly\n",
       "- **B.** The variable cannot be modified\n",
       "- **C.** The variable is stored in constant memory\n",
       "- **D.** The variable is initialized at compile time\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model Response (streaming):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "understanding: |\n",
      "  The question asks about the significance of the 'const' keyword in C++, specifically regarding its ability to prevent a variable from being modified.\n",
      "analysis: |\n",
      "  A. Incorrect. The 'const' keyword does not imply indirect modification; it restricts direct modification.\n",
      "  B. Correct. The 'const' keyword makes a variable immutable once assigned, preventing any further modification.\n",
      "  C. Incorrect. 'const' does not specify where a variable is stored; it only affects its modifiability.\n",
      "  D. Incorrect. While 'const' can help optimize code by storing variables in read-only memory, it does not inherently \"initialize\" variables at compile time.\n",
      "reasoning: |\n",
      "  The 'const' keyword in C++ is used to declare a variable that cannot be changed after initialization. This prevents accidental modifications, which is a key feature of making variables immutable. Option B accurately describes this behavior, while the other options misrepresent the primary function of 'const'.\n",
      "conclusion: |\n",
      "  Answer B is correct because the 'const' keyword ensures that a variable's value cannot be altered once set, thus preventing modification directly.\n",
      "answer: B\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer:** B\n",
       "\n",
       "**Correct Answer:** B\n",
       "\n",
       "**Reasoning:**\n",
       "\n",
       "Understanding: The question asks about the significance of the 'const' keyword in C++, specifically regarding its ability to prevent a variable from being modified.\n",
       "\n",
       "\n",
       "The 'const' keyword in C++ is used to declare a variable that cannot be changed after initialization. This prevents accidental modifications, which is a key feature of making variables immutable. Option B accurately describes this behavior, while the other options misrepresent the primary function of 'const'.\n",
       "\n",
       "\n",
       "Conclusion: Answer B is correct because the 'const' keyword ensures that a variable's value cannot be altered once set, thus preventing modification directly.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Example 7\n",
       "\n",
       "**Question:** Which C++ feature provides runtime polymorphism?\n",
       "\n",
       "**Choices:**\n",
       "- **A.** Virtual functions\n",
       "- **B.** Templates\n",
       "- **C.** Operator overloading\n",
       "- **D.** Friend functions\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model Response (streaming):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "understanding: |\n",
      "  The question asks which C++ feature supports runtime polymorphism, a key concept in object-oriented programming where different classes can be treated as instances of the same base class.\n",
      "analysis: |\n",
      "  A. Virtual functions allow for runtime polymorphism by enabling a method to be overridden in derived classes while still calling the correct implementation based on the pointer or reference type.\n",
      "  B. Templates enable compile-time polymorphism by allowing code reuse across different data types but do not support runtime polymorphism.\n",
      "  C. Operator overloading allows new operators to be defined for user-defined types but does not inherently support runtime polymorphism.\n",
      "  D. Friend functions allow one function to access the private and protected members of another class but do not support runtime polymorphism.\n",
      "reasoning: |\n",
      "  Virtual functions are specifically designed to achieve runtime polymorphism by enabling a base class pointer to call a derived class method. This is possible because the actual method called depends on the object's type at runtime, unlike static binding that occurs at compile time with templates or operator overloading.\n",
      "conclusion: |\n",
      "  Answer A is correct because virtual functions provide the necessary mechanism for runtime polymorphism by allowing derived class methods to be invoked dynamically based on the object's type.\n",
      "answer: A\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer:** A\n",
       "\n",
       "**Correct Answer:** A\n",
       "\n",
       "**Reasoning:**\n",
       "\n",
       "Understanding: The question asks which C++ feature supports runtime polymorphism, a key concept in object-oriented programming where different classes can be treated as instances of the same base class.\n",
       "\n",
       "\n",
       "Virtual functions are specifically designed to achieve runtime polymorphism by enabling a base class pointer to call a derived class method. This is possible because the actual method called depends on the object's type at runtime, unlike static binding that occurs at compile time with templates or operator overloading.\n",
       "\n",
       "\n",
       "Conclusion: Answer A is correct because virtual functions provide the necessary mechanism for runtime polymorphism by allowing derived class methods to be invoked dynamically based on the object's type.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Example 8\n",
       "\n",
       "**Question:** In C++, what is the purpose of the 'new' operator?\n",
       "\n",
       "**Choices:**\n",
       "- **A.** To create a new class\n",
       "- **B.** To allocate memory dynamically\n",
       "- **C.** To initialize a new variable\n",
       "- **D.** To create a new scope\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model Response (streaming):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "understanding: |\n",
      "  The question asks about the primary function of the 'new' operator in C++. Understanding its role involves recognizing how it interacts with memory allocation.\n",
      "analysis: |\n",
      "  A. Incorrect. Creating a new class is done using the 'class' keyword or struct, not 'new'.\n",
      "  B. Correct. The 'new' operator is used to request dynamic memory allocation from the system, allowing for the creation of objects that may not exist at compile time.\n",
      "  C. Incorrect. Initializing a variable is typically done using assignment operators like '=' or constructors, not 'new'.\n",
      "  D. Incorrect. A scope is managed by 'static', 'extern', or block-level declarations, not the 'new' operator.\n",
      "reasoning: |\n",
      "  The 'new' operator is specifically designed to allocate memory for an object during runtime. This allows developers to create objects whose size or location are determined at execution time, which is essential for implementing features like dynamic data structures and polymorphism.\n",
      "conclusion: |\n",
      "  Answer B is correct because the 'new' operator's primary purpose is to allocate memory dynamically, enabling flexible memory management in C++ programs.\n",
      "answer: B\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer:** B\n",
       "\n",
       "**Correct Answer:** B\n",
       "\n",
       "**Reasoning:**\n",
       "\n",
       "Understanding: The question asks about the primary function of the 'new' operator in C++. Understanding its role involves recognizing how it interacts with memory allocation.\n",
       "\n",
       "\n",
       "The 'new' operator is specifically designed to allocate memory for an object during runtime. This allows developers to create objects whose size or location are determined at execution time, which is essential for implementing features like dynamic data structures and polymorphism.\n",
       "\n",
       "\n",
       "Conclusion: Answer B is correct because the 'new' operator's primary purpose is to allocate memory dynamically, enabling flexible memory management in C++ programs.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Example 9\n",
       "\n",
       "**Question:** What is the correct way to declare a function pointer in C?\n",
       "\n",
       "**Choices:**\n",
       "- **A.** void (*func)(int);\n",
       "- **B.** void *func(int);\n",
       "- **C.** func->void(int);\n",
       "- **D.** pointer void func(int);\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model Response (streaming):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "understanding: |\n",
      "  The question asks for the correct syntax to declare a function pointer in C. Function pointers are used to store addresses of functions that can be called later.\n",
      "analysis: |\n",
      "  A. void (*func)(int); - Correct syntax for declaring a function pointer. It specifies the return type (void) and the argument list (int), followed by the parentheses indicating it's a function pointer.\n",
      "  B. void *func(int); - Incorrect because it lacks the parentheses around the parameter list, which is necessary for defining a function pointer.\n",
      "  C. func->void(int); - Incorrect due to incorrect use of arrow notation and missing parentheses, which do not define a function pointer.\n",
      "  D. pointer void func(int); - Incorrect because \"pointer\" is not a valid keyword in C for function pointers; the correct keyword is \"function\".\n",
      "reasoning: |\n",
      "  To declare a function pointer, you must specify both the return type and the argument list within parentheses. Option A correctly follows this pattern, while the other options fail to include these essential components.\n",
      "conclusion: |\n",
      "  Answer A is correct because it properly declares a function pointer using the required syntax: return type, parentheses enclosing the argument list, and the asterisk (*) to denote it as a pointer.\n",
      "answer: A\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer:** A\n",
       "\n",
       "**Correct Answer:** A\n",
       "\n",
       "**Reasoning:**\n",
       "\n",
       "Understanding: The question asks for the correct syntax to declare a function pointer in C. Function pointers are used to store addresses of functions that can be called later.\n",
       "\n",
       "\n",
       "To declare a function pointer, you must specify both the return type and the argument list within parentheses. Option A correctly follows this pattern, while the other options fail to include these essential components.\n",
       "\n",
       "\n",
       "Conclusion: Answer A is correct because it properly declares a function pointer using the required syntax: return type, parentheses enclosing the argument list, and the asterisk (*) to denote it as a pointer.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Example 10\n",
       "\n",
       "**Question:** Which of these is NOT a valid C++ smart pointer type?\n",
       "\n",
       "**Choices:**\n",
       "- **A.** std::unique_ptr\n",
       "- **B.** std::shared_ptr\n",
       "- **C.** std::weak_ptr\n",
       "- **D.** std::auto_ptr\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model Response (streaming):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "understanding: |\n",
      "  The question asks which option is not a valid C++ smart pointer type. Smart pointers are used to manage memory more efficiently and safely in C++. They help prevent memory leaks and ensure that resources are freed when they are no longer needed.\n",
      "analysis: |\n",
      "  A. std::unique_ptr - This is a unique pointer, which owns the object it points to and deletes it when no longer needed.\n",
      "  B. std::shared_ptr - This is a shared pointer, allowing multiple pointers to point to the same object and automatically managing resource ownership.\n",
      "  C. std::weak_ptr - This is a weak pointer, which holds a non-owning reference to an object managed by a shared pointer. It does not affect the lifetime of the object it points to.\n",
      "  D. std::auto_ptr - This was a legacy C++ smart pointer that owned the object it pointed to and deleted it upon destruction. However, it has been deprecated due to its inefficiencies and lack of safety features compared to modern smart pointers.\n",
      "reasoning: |\n",
      "  The key distinction lies in the behavior of std::auto_ptr, which is outdated and not recommended for use in new code. Other options (std::unique_ptr, std::shared_ptr, and std::weak_ptr) all serve different purposes in managing memory, making them valid choices. Therefore, std::auto_ptr is not a valid C++ smart pointer type.\n",
      "conclusion: |\n",
      "  Answer D is correct because std::auto_ptr is an outdated and unsafe smart pointer that should not be used in new C++ code. Options A, B, and C are valid types for smart pointers.\n",
      "answer: D\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer:** D\n",
       "\n",
       "**Correct Answer:** D\n",
       "\n",
       "**Reasoning:**\n",
       "\n",
       "Understanding: The question asks which option is not a valid C++ smart pointer type. Smart pointers are used to manage memory more efficiently and safely in C++. They help prevent memory leaks and ensure that resources are freed when they are no longer needed.\n",
       "\n",
       "\n",
       "The key distinction lies in the behavior of std::auto_ptr, which is outdated and not recommended for use in new code. Other options (std::unique_ptr, std::shared_ptr, and std::weak_ptr) all serve different purposes in managing memory, making them valid choices. Therefore, std::auto_ptr is not a valid C++ smart pointer type.\n",
       "\n",
       "\n",
       "Conclusion: Answer D is correct because std::auto_ptr is an outdated and unsafe smart pointer that should not be used in new C++ code. Options A, B, and C are valid types for smart pointers.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import yaml\n",
    "from IPython.display import Markdown, display\n",
    "import time\n",
    "\n",
    "# Define test examples with varied correct answers\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Which of the following is NOT a valid way to initialize a variable in C++?\",\n",
    "        \"choices\": [\"int x = 5;\", \"int x(5);\", \"int x{5};\", \"int x := 5;\"],\n",
    "        \"answer\": \"D\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"In C, what does the 'malloc' function do?\",\n",
    "        \"choices\": [\n",
    "            \"Frees allocated memory\",\n",
    "            \"Allocates memory dynamically\",\n",
    "            \"Manages automatic memory\",\n",
    "            \"Moves allocated memory\",\n",
    "        ],\n",
    "        \"answer\": \"B\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which C++ keyword is used to define a class template?\",\n",
    "        \"choices\": [\"class\", \"virtual\", \"template\", \"typename\"],\n",
    "        \"answer\": \"C\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the correct way to access a member of a structure through a pointer in C?\",\n",
    "        \"choices\": [\"pointer.member\", \"pointer->member\", \"pointer::member\", \"pointer@member\"],\n",
    "        \"answer\": \"B\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which of the following is NOT a storage class specifier in C?\",\n",
    "        \"choices\": [\"static\", \"extern\", \"register\", \"virtual\"],\n",
    "        \"answer\": \"D\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What does the 'const' keyword signify in C++?\",\n",
    "        \"choices\": [\n",
    "            \"The variable can be modified indirectly\",\n",
    "            \"The variable cannot be modified\",\n",
    "            \"The variable is stored in constant memory\",\n",
    "            \"The variable is initialized at compile time\",\n",
    "        ],\n",
    "        \"answer\": \"B\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which C++ feature provides runtime polymorphism?\",\n",
    "        \"choices\": [\"Virtual functions\", \"Templates\", \"Operator overloading\", \"Friend functions\"],\n",
    "        \"answer\": \"A\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"In C++, what is the purpose of the 'new' operator?\",\n",
    "        \"choices\": [\n",
    "            \"To create a new class\",\n",
    "            \"To allocate memory dynamically\",\n",
    "            \"To initialize a new variable\",\n",
    "            \"To create a new scope\",\n",
    "        ],\n",
    "        \"answer\": \"B\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the correct way to declare a function pointer in C?\",\n",
    "        \"choices\": [\n",
    "            \"void (*func)(int);\",\n",
    "            \"void *func(int);\",\n",
    "            \"func->void(int);\",\n",
    "            \"pointer void func(int);\",\n",
    "        ],\n",
    "        \"answer\": \"A\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which of these is NOT a valid C++ smart pointer type?\",\n",
    "        \"choices\": [\"std::unique_ptr\", \"std::shared_ptr\", \"std::weak_ptr\", \"std::auto_ptr\"],\n",
    "        \"answer\": \"D\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# Function to process and display examples with streaming markdown\n",
    "def process_example(example, index):\n",
    "    md_content = f\"## Example {index+1}\\n\\n\"\n",
    "    md_content += f\"**Question:** {example['question']}\\n\\n\"\n",
    "    md_content += \"**Choices:**\\n\"\n",
    "\n",
    "    for i, choice in enumerate(example[\"choices\"]):\n",
    "        md_content += f\"- **{chr(65+i)}.** {choice}\\n\"\n",
    "\n",
    "    display(Markdown(md_content))\n",
    "\n",
    "    # Convert to YAML format if needed (for examples 4-6 and 8-10)\n",
    "    if index >= 3:\n",
    "        example_dict = yaml.safe_load(yaml.safe_dump(example))\n",
    "    else:\n",
    "        example_dict = example\n",
    "\n",
    "    # Start streaming response\n",
    "    display(Markdown(\"**Model Response (streaming):**\"))\n",
    "\n",
    "    result = latest_tester_hub.infer_example(\n",
    "        example_dict, temperature=0.00001, stream=True, use_cache=False\n",
    "    )\n",
    "\n",
    "    # Display final result\n",
    "    result_md = f\"**Predicted Answer:** {result['predicted_answer']}\\n\\n\"\n",
    "    result_md += f\"**Correct Answer:** {example['answer']}\\n\\n\"\n",
    "    result_md += \"**Reasoning:**\\n\\n\"\n",
    "\n",
    "    try:\n",
    "        result_md += result[\"reasoning\"]\n",
    "    except Exception as e:\n",
    "        result_md += f\"Error: {e}\"\n",
    "\n",
    "    display(Markdown(result_md))\n",
    "    display(Markdown(\"---\"))\n",
    "\n",
    "\n",
    "# Process all examples\n",
    "for i, example in enumerate(examples):\n",
    "    process_example(example, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
